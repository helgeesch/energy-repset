{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"energy-repset","text":"<p>A unified, modular framework for representative subset selection in multi-variate time-series spaces.</p>"},{"location":"#why-this-package","title":"Why this package?","text":"<p>Energy system models, capacity expansion studies, and other time-series-heavy applications often need to reduce a full year (or longer) of hourly data to a small set of representative periods -- days, weeks, or months -- without losing what matters. The literature offers many methods (k-means, k-medoids, MILP-based selection, genetic algorithms, etc.), but the landscape is dense and tangled: each method bundles multiple decisions -- how to represent data, what to optimize, how to search -- into a single procedure, making it hard to see which choices matter, compare approaches on equal footing, or adapt a method to your specific problem.</p> <p><code>energy-repset</code> clears a path through the jungle in two ways:</p> <ol> <li> <p>A unified framework that decomposes any time-series aggregation method into five interchangeable components. Every established methodology is a specific instantiation of this structure. The framework provides a common language for describing, comparing, and assembling methods. The full theoretical treatment is available in the Unified Framework document.</p> </li> <li> <p>A modular Python package that implements this framework as a library of composable, protocol-based modules. You pick one implementation per component, wire them together, and run. Adding a new algorithm or score metric means implementing a single protocol -- everything else stays the same.</p> </li> </ol>"},{"location":"#the-five-components","title":"The Five Components","text":"Component Symbol Role Feature Space F How raw time-series are transformed into comparable representations Objective O How candidate selections are scored for quality Selection Space S What is being selected (historical subsets, synthetic archetypes, etc.) Representation Model R How selected periods represent the full dataset Search Algorithm A The engine that finds optimal selections"},{"location":"#navigating-the-project","title":"Navigating the project","text":"<p>Website: energy-repset.mesqual.io</p> <p>Documentation site (energy-repset-docs.mesqual.io):</p> Section What you'll find Unified Framework The theoretical paper: problem decomposition, component taxonomy, method comparison Workflow Types The three workflow patterns: generate-and-test, constructive, direct optimization Modules &amp; Components Inventory of all implemented modules and how they map to the five components Configuration Advisor Decision guide for choosing components based on your problem Getting Started End-to-end walkthrough from data to result Examples Worked examples showcasing different configurations API Reference Auto-generated class and method documentation <p>Package structure (<code>energy_repset/</code>):</p> Module Framework component <code>context</code>, <code>time_slicer</code> Problem definition and data container <code>feature_engineering/</code> F -- Feature engineers (statistical summaries, PCA, pipelines) <code>objectives</code>, <code>score_components/</code> O -- Objective sets and scoring metrics <code>combi_gens/</code> S -- Combination generators (exhaustive, group-quota, hierarchical) <code>representation/</code> R -- Representation models (uniform, cluster-based, blended) <code>search_algorithms/</code>, <code>selection_policies/</code> A -- Search algorithms and selection policies <code>workflow</code>, <code>problem</code>, <code>results</code> Orchestration: wire components, run, collect results <code>diagnostics/</code> Visualization and analysis of features, scores, and results"},{"location":"#installation","title":"Installation","text":"<p>Option 1 -- Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/mesqual/energy-repset.git\n</code></pre> <p>Option 2 -- Clone and install in editable mode:</p> <pre><code>git clone https://github.com/mesqual/energy-repset.git\ncd energy-repset\npip install -e .\n</code></pre> <p>Option 3 -- Add as a Git submodule (useful for monorepos):</p> <pre><code>git submodule add https://github.com/mesqual/energy-repset.git\npip install -e energy-repset\n</code></pre> <p>Alternatively, skip the install and mark the <code>energy-repset</code> directory as a source root in your IDE so that <code>import energy_repset</code> resolves directly.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import pandas as pd\nimport energy_repset as rep\n\n# Load hourly time-series data (columns = variables, index = datetime)\ndf_raw = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=True)\n\n# Define problem: slice the year into monthly candidate periods\nslicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw, slicer)\n\n# Feature engineering: statistical summaries per month\nfeature_engineer = rep.StandardStatsFeatureEngineer()\n\n# Objective: score each candidate selection on distribution fidelity\nobjective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n})\n\n# Search: evaluate all 4-of-12 monthly combinations\npolicy = rep.WeightedSumPolicy()\ncombi_gen = rep.ExhaustiveCombiGen(k=4)\nsearch = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\n\n# Representation: equal 1/k weights per selected month\nrepresentation = rep.UniformRepresentationModel()\n\n# Assemble and run\nworkflow = rep.Workflow(feature_engineer, search, representation)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n\nprint(result.selection)  # e.g., (Period('2019-01', 'M'), Period('2019-04', 'M'), ...)\nprint(result.weights)    # e.g., {Period('2019-01', 'M'): 3.0, ...}\nprint(result.scores)     # e.g., {'wasserstein': 0.023, 'correlation': 0.015}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Apache-2.0</p>"},{"location":"advisor/","title":"Configuration Advisor","text":"<p>This document serves a dual purpose:</p> <ol> <li>Human guide -- a structured decision tree for choosing energy-repset components.</li> <li>AI system prompt -- a self-contained reference an LLM can use to interactively guide users through configuration.</li> </ol> <p>For theory, see Unified Framework. For API details, see Modules &amp; Components.</p>"},{"location":"advisor/#component-catalog","title":"Component Catalog","text":""},{"location":"advisor/#f-feature-engineering","title":"F: Feature Engineering","text":"Class Import Description <code>StandardStatsFeatureEngineer</code> <code>energy_repset.feature_engineering</code> Statistical summaries per slice (mean, std, IQR, quantiles, ramp rates). Z-score normalized. <code>PCAFeatureEngineer</code> <code>energy_repset.feature_engineering</code> PCA dimensionality reduction. Supports variance-threshold or fixed component count. <code>FeaturePipeline</code> <code>energy_repset.feature_engineering</code> Chains multiple engineers sequentially. <p>Typical pipeline: <code>StandardStatsFeatureEngineer</code> -&gt; <code>PCAFeatureEngineer</code> (via <code>FeaturePipeline</code>).</p>"},{"location":"advisor/#o-score-components","title":"O: Score Components","text":"<p>All components implement the <code>ScoreComponent</code> protocol with <code>prepare(context)</code> and <code>score(combination)</code>.</p> Class Direction What it Measures Import <code>WassersteinFidelity</code> min Marginal distribution similarity (Wasserstein distance, IQR-normalized) <code>energy_repset.score_components</code> <code>CorrelationFidelity</code> min Cross-variable correlation preservation (Frobenius norm) <code>energy_repset.score_components</code> <code>DurationCurveFidelity</code> min Duration curve match (quantile-based NRMSE) <code>energy_repset.score_components</code> <code>NRMSEFidelity</code> min Duration curve match (full interpolation NRMSE) <code>energy_repset.score_components</code> <code>DiurnalFidelity</code> min Hour-of-day profile preservation (normalized MSE) <code>energy_repset.score_components</code> <code>DiurnalDTWFidelity</code> min Hour-of-day profile preservation (DTW distance) <code>energy_repset.score_components</code> <code>DTWFidelity</code> min Full series shape similarity (Dynamic Time Warping) <code>energy_repset.score_components</code> <code>DiversityReward</code> max Spread in feature space (avg pairwise distance) <code>energy_repset.score_components</code> <code>CentroidBalance</code> min Feature centroid deviation from global mean <code>energy_repset.score_components</code> <code>CoverageBalance</code> min Balanced coverage via RBF kernel soft assignment <code>energy_repset.score_components</code> <p>Components are bundled into an <code>ObjectiveSet</code> (<code>energy_repset.objectives</code>) with per-component weights.</p>"},{"location":"advisor/#s-combination-generators","title":"S: Combination Generators","text":"Class Import Description <code>ExhaustiveCombiGen</code> <code>energy_repset.combi_gens</code> All k-of-n combinations. <code>GroupQuotaCombiGen</code> <code>energy_repset.combi_gens</code> Exact quotas per group (e.g., 1 per season). <code>ExhaustiveHierarchicalCombiGen</code> <code>energy_repset.combi_gens</code> Selects parent groups, evaluates on child slices. <code>GroupQuotaHierarchicalCombiGen</code> <code>energy_repset.combi_gens</code> Hierarchical + group quotas. Has <code>from_slicers_with_seasons()</code> factory."},{"location":"advisor/#r-representation-models","title":"R: Representation Models","text":"Class Import Description <code>UniformRepresentationModel</code> <code>energy_repset.representation</code> Equal 1/k weights. Returns <code>dict</code>. <code>KMedoidsClustersizeRepresentation</code> <code>energy_repset.representation</code> Cluster-proportional weights via k-medoids. Returns <code>dict</code>. <code>BlendedRepresentationModel</code> <code>energy_repset.representation</code> Soft assignment (convex combination). Returns weight <code>DataFrame</code>."},{"location":"advisor/#a-search-algorithms","title":"A: Search Algorithms","text":"Class Workflow Import <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> Generate-and-Test <code>energy_repset.search_algorithms</code> <p>Not yet implemented: Constructive algorithms (Hull Clustering, K-Medoids PAM, Snippet Algorithm), Direct Optimization (MILP).</p>"},{"location":"advisor/#pi-selection-policies","title":"Pi: Selection Policies","text":"Class Import Description <code>WeightedSumPolicy</code> <code>energy_repset.selection_policies</code> Scalar aggregation. Supports <code>normalization='robust_minmax'</code>. <code>ParetoMaxMinStrategy</code> <code>energy_repset.selection_policies</code> Pareto-optimal solution maximizing worst objective. <code>ParetoUtopiaPolicy</code> <code>energy_repset.selection_policies</code> Pareto-optimal solution closest to utopia point."},{"location":"advisor/#decision-tree","title":"Decision Tree","text":""},{"location":"advisor/#step-1-understand-your-data","title":"Step 1: Understand your data","text":"<p>Ask yourself:</p> <ul> <li>Resolution: Hourly? 15-minute? Daily?</li> <li>Variables: How many time-series (load, wind, solar, prices, ...)?</li> <li>Horizon: One year? Multiple years?</li> <li>Candidate count: How many slices does your <code>TimeSlicer</code> produce?</li> <li>12 months -&gt; C(12,3) = 220 candidates for k=3</li> <li>52 weeks -&gt; C(52,8) = 752 million candidates for k=8</li> </ul> <p>This determines whether exhaustive search is feasible or you need constrained/hierarchical generation.</p>"},{"location":"advisor/#step-2-downstream-model-constraints","title":"Step 2: Downstream model constraints","text":"<p>Your energy system model may impose constraints on the representation:</p> Constraint Implication Model requires equal-length periods with scalar weights Use <code>UniformRepresentationModel</code> or <code>KMedoidsClustersizeRepresentation</code> Model can accept blended inputs (e.g., weighted hourly profiles) Use <code>BlendedRepresentationModel</code> Must cover all seasons Use <code>GroupQuotaCombiGen</code> or <code>GroupQuotaHierarchicalCombiGen</code> Must preserve temporal coupling within periods (e.g., multi-day storage) Prefer weekly/multi-day slicing over monthly"},{"location":"advisor/#step-3-computational-budget","title":"Step 3: Computational budget","text":"Candidate space size Recommended generator &lt; 10,000 <code>ExhaustiveCombiGen</code> -- evaluate all 10,000 -- 1,000,000 <code>GroupQuotaCombiGen</code> to constrain, or hierarchical generators &gt; 1,000,000 Hierarchical generators, or future genetic/constructive algorithms <p>Hierarchical trick: Select at the month level (small combinatorial space) but evaluate on day-level features (high resolution). Use <code>ExhaustiveHierarchicalCombiGen</code> or <code>GroupQuotaHierarchicalCombiGen</code>.</p>"},{"location":"advisor/#step-4-quality-goals","title":"Step 4: Quality goals","text":"<p>Choose score components based on what matters for your downstream model:</p> Goal Recommended Components Preserve marginal distributions (load duration curves) <code>WassersteinFidelity</code>, <code>DurationCurveFidelity</code>, <code>NRMSEFidelity</code> Preserve variable correlations (wind-solar complementarity) <code>CorrelationFidelity</code> Preserve diurnal patterns (solar noon peak, evening ramp) <code>DiurnalFidelity</code>, <code>DiurnalDTWFidelity</code> Preserve overall time-series shape <code>DTWFidelity</code> Ensure diverse representatives (avoid redundancy) <code>DiversityReward</code> Balanced coverage of the feature space <code>CentroidBalance</code>, <code>CoverageBalance</code> <p>Start simple: <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> covers most needs. Add more components only if you observe specific deficiencies in the results.</p>"},{"location":"advisor/#step-5-selection-policy","title":"Step 5: Selection policy","text":"Situation Recommended Policy Single objective or clear priority ranking <code>WeightedSumPolicy</code> (default) Multiple objectives, want balanced trade-off <code>WeightedSumPolicy(normalization='robust_minmax')</code> Multiple objectives, want to avoid worst-case failure <code>ParetoMaxMinStrategy</code> Multiple objectives, want closest to ideal <code>ParetoUtopiaPolicy</code>"},{"location":"advisor/#common-configurations","title":"Common Configurations","text":"<p>All examples below assume <code>import energy_repset as rep</code>.</p>"},{"location":"advisor/#minimal-single-objective-monthly-selection","title":"Minimal: single-objective monthly selection","text":"<pre><code>import energy_repset as rep\n\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n        rep.ObjectiveSet({'wass': (1.0, rep.WassersteinFidelity())}),\n        rep.WeightedSumPolicy(),\n        rep.ExhaustiveCombiGen(k=4),\n    ),\n    representation_model=rep.UniformRepresentationModel(),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#multi-objective-with-pca-features","title":"Multi-objective with PCA features","text":"<pre><code>feature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n\nobjective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n    'diversity':   (0.5, rep.DiversityReward()),\n})\n\nworkflow = rep.Workflow(\n    feature_engineer=feature_pipeline,\n    search_algorithm=rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n        objective_set, rep.ParetoMaxMinStrategy(), rep.ExhaustiveCombiGen(k=3),\n    ),\n    representation_model=rep.KMedoidsClustersizeRepresentation(),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#seasonal-constraints-with-hierarchical-search","title":"Seasonal constraints with hierarchical search","text":"<pre><code>child_slicer = rep.TimeSlicer(unit=\"day\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=child_slicer)\n\ncombi_gen = rep.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n    parent_k=4,\n    dt_index=df_raw.index,\n    child_slicer=child_slicer,\n    group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1},\n)\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n        objective_set, rep.WeightedSumPolicy(), combi_gen,\n    ),\n    representation_model=rep.KMedoidsClustersizeRepresentation(),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#blended-soft-representation","title":"Blended (soft) representation","text":"<pre><code>workflow = rep.Workflow(\n    feature_engineer=feature_pipeline,\n    search_algorithm=search_algorithm,\n    representation_model=rep.BlendedRepresentationModel(blend_type='convex'),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n# result.weights is a DataFrame (not a dict) for blended models\n</code></pre>"},{"location":"advisor/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Blended weight aggregation: <code>BlendedRepresentationModel.weigh()</code> returns a weight matrix. If you sum columns for visualization, normalize the result so weights sum to 1.0 (otherwise bars show raw sums that scale with N).</p> </li> <li> <p>Combinatorial explosion: C(52, 8) = 752 million. Always check <code>combi_gen.count(slices)</code> before running. Use hierarchical generators or group quotas to reduce the search space.</p> </li> <li> <p>PCA without stats: <code>PCAFeatureEngineer</code> operates on existing features. It must come after <code>StandardStatsFeatureEngineer</code> in a <code>FeaturePipeline</code>, not as a standalone.</p> </li> <li> <p>DTW components are slow: <code>DTWFidelity</code> and <code>DiurnalDTWFidelity</code> use dynamic time warping which is O(n^2) per pair. Suitable for small candidate sets; consider cheaper alternatives for large searches.</p> </li> <li> <p>Direction confusion: Most fidelity components use <code>direction=\"min\"</code> (lower is better). <code>DiversityReward</code> uses <code>direction=\"max\"</code>. The <code>ObjectiveSet</code> and selection policies handle direction automatically -- you do not need to negate scores.</p> </li> <li> <p>Single vs multi-objective: With a single score component, <code>WeightedSumPolicy</code> and <code>ParetoMaxMinStrategy</code> produce identical results. Pareto-based policies only add value with 2+ objectives.</p> </li> </ol>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide walks through a minimal end-to-end workflow, explaining each step and how it maps to the five-pillar framework (F, O, S, R, A).</p> <p>By the end, you will have selected 4 representative months from a year of hourly time-series data, scored them on distribution fidelity, and generated a simple diagnostic chart.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>Option 1 -- Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/mesqual/energy-repset.git\n</code></pre> <p>Option 2 -- Clone and install in editable mode:</p> <pre><code>git clone https://github.com/mesqual/energy-repset.git\ncd energy-repset\npip install -e .\n</code></pre> <p>Option 3 -- Add as a Git submodule (useful for monorepos):</p> <pre><code>git submodule add https://github.com/mesqual/energy-repset.git\npip install -e energy-repset\n</code></pre> <p>Alternatively, skip the install and mark the <code>energy-repset</code> directory as a source root in your IDE so that <code>import energy_repset</code> resolves directly.</p>"},{"location":"getting_started/#imports","title":"Imports","text":"<p>All framework classes are available from the top-level namespace. Diagnostics live one level down:</p> <pre><code>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\n</code></pre>"},{"location":"getting_started/#load-data","title":"Load Data","text":"<p>energy-repset works with any <code>pandas.DataFrame</code> where the index is a <code>DatetimeIndex</code> and each column is a variable (e.g., load, wind, solar):</p> <pre><code>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n</code></pre>"},{"location":"getting_started/#define-the-problem-context","title":"Define the Problem Context","text":"<p>The <code>ProblemContext</code> combines the raw data with a <code>TimeSlicer</code> that defines how the time axis is divided into candidate periods. Here, each calendar month becomes one candidate:</p> <pre><code>slicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\nprint(f\"Candidate slices: {context.get_unique_slices()}\")\n# -&gt; 12 monthly periods\n</code></pre>"},{"location":"getting_started/#pillar-f-feature-engineering","title":"Pillar F: Feature Engineering","text":"<p>Feature engineering transforms the raw time-series into a compact representation that can be compared across candidate periods. <code>StandardStatsFeatureEngineer</code> computes statistical summaries (mean, std, quantiles, ramp rates) per slice and variable:</p> <pre><code>feature_engineer = rep.StandardStatsFeatureEngineer()\n</code></pre> <p>For richer feature spaces, you can chain engineers with a <code>FeaturePipeline</code>:</p> <pre><code>feature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n</code></pre> <p>In this guide we keep it simple and use only the statistical features.</p>"},{"location":"getting_started/#pillar-o-objective","title":"Pillar O: Objective","text":"<p>The <code>ObjectiveSet</code> defines how candidate selections are scored. Each entry maps a name to a <code>(weight, ScoreComponent)</code> tuple. Here we use a single objective: Wasserstein distance between the marginal distributions of the selection and the full year.</p> <pre><code>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n})\n</code></pre> <p>Multiple objectives are easy to add:</p> <pre><code>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n})\n</code></pre>"},{"location":"getting_started/#pillar-s-selection-space","title":"Pillar S: Selection Space","text":"<p>A <code>CombinationGenerator</code> defines which subsets are considered. <code>ExhaustiveCombiGen</code> evaluates every possible k-of-n combination:</p> <pre><code>k = 4\ncombi_gen = rep.ExhaustiveCombiGen(k=k)\n# For 12 months, k=4 -&gt; C(12,4) = 495 candidates\n</code></pre>"},{"location":"getting_started/#pillar-a-search-algorithm","title":"Pillar A: Search Algorithm","text":"<p>The search algorithm orchestrates the evaluation loop. In the generate-and-test workflow, it generates candidates via the <code>CombinationGenerator</code>, scores each with the <code>ObjectiveSet</code>, and picks a winner using the <code>SelectionPolicy</code>:</p> <pre><code>policy = rep.WeightedSumPolicy()\nsearch_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    objective_set, policy, combi_gen\n)\n</code></pre>"},{"location":"getting_started/#pillar-r-representation-model","title":"Pillar R: Representation Model","text":"<p>The representation model determines how the selected periods represent the full year. <code>UniformRepresentationModel</code> assigns equal weight to each selected period:</p> <pre><code>representation_model = rep.UniformRepresentationModel()\n</code></pre>"},{"location":"getting_started/#run-the-workflow","title":"Run the Workflow","text":"<p>Assemble all components into a <code>Workflow</code>, wrap it in a <code>RepSetExperiment</code>, and run:</p> <pre><code>workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n</code></pre>"},{"location":"getting_started/#inspect-results","title":"Inspect Results","text":"<p>The <code>RepSetResult</code> contains the selected periods, their weights, and the objective scores:</p> <pre><code>print(f\"Selected months: {result.selection}\")\nprint(f\"Weights: {result.weights}\")\nprint(f\"Wasserstein score: {result.scores['wasserstein']:.4f}\")\n</code></pre>"},{"location":"getting_started/#diagnostic-responsibility-bars","title":"Diagnostic: Responsibility Bars","text":"<p>energy-repset includes interactive Plotly diagnostics. A responsibility bar chart shows how the total representation weight is distributed across selected periods:</p> <pre><code>fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True)\nfig.show()\n</code></pre>"},{"location":"getting_started/#full-script","title":"Full Script","text":"<p>The complete code is available at <code>examples/ex1_getting_started.py</code>.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Swap components: Try <code>rep.ParetoMaxMinStrategy</code> instead of <code>rep.WeightedSumPolicy</code>,   or <code>rep.KMedoidsClustersizeRepresentation</code> instead of uniform weights.   See the Modules &amp; Components page for all available implementations.</li> <li>Add objectives: Add <code>rep.CorrelationFidelity</code>, <code>rep.DurationCurveFidelity</code>, or   <code>rep.DiversityReward</code> to the <code>ObjectiveSet</code>.</li> <li>Browse examples: The Gallery shows more advanced   configurations with interactive visualizations.</li> </ul>"},{"location":"modules/","title":"Modules &amp; Components","text":"<p>energy-repset decomposes any representative period selection method into five interchangeable pillars. Each pillar has a protocol (interface) and one or more concrete implementations. Swapping a single component changes the behavior without affecting the rest of the pipeline.</p>"},{"location":"modules/#the-five-pillars","title":"The Five Pillars","text":"<pre><code>Raw DataFrame\n  -&gt; TimeSlicer (defines candidate periods)\n  -&gt; ProblemContext (holds data + metadata)\n  -&gt; [F] FeatureEngineer (creates feature vectors per slice)\n  -&gt; [A] SearchAlgorithm (finds optimal selection using [O] ObjectiveSet)\n  -&gt; [R] RepresentationModel (calculates weights)\n  -&gt; RepSetResult (selection, weights, scores)\n</code></pre>"},{"location":"modules/#f-feature-space","title":"F: Feature Space","text":"<p>Transforms raw time-series slices into comparable feature vectors.</p> Implementation Description <code>StandardStatsFeatureEngineer</code> Statistical summaries per slice (mean, std, IQR, quantiles, ramp rates). Z-score normalized. <code>PCAFeatureEngineer</code> PCA dimensionality reduction on existing features. Supports variance-threshold or fixed component count. <code>FeaturePipeline</code> Chains multiple engineers sequentially and concatenates their outputs. <pre><code>import energy_repset as rep\n\n# Single engineer\nfeature_engineer = rep.StandardStatsFeatureEngineer()\n\n# Chained pipeline: compute stats, then reduce with PCA\nfeature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n</code></pre>"},{"location":"modules/#o-objective","title":"O: Objective","text":"<p>An <code>ObjectiveSet</code> holds one or more weighted <code>ScoreComponent</code> instances. Each component evaluates how well a candidate selection represents the full dataset along a specific dimension.</p> Component Name Direction What it Measures <code>WassersteinFidelity</code> <code>wasserstein</code> min Marginal distribution similarity (Wasserstein distance, IQR-normalized) <code>CorrelationFidelity</code> <code>correlation</code> min Cross-variable correlation preservation (Frobenius norm) <code>DurationCurveFidelity</code> <code>nrmse_duration_curve</code> min Duration curve match (quantile-based NRMSE) <code>NRMSEFidelity</code> <code>nrmse</code> min Duration curve match (full interpolation NRMSE) <code>DiurnalFidelity</code> <code>diurnal</code> min Hour-of-day profile preservation (normalized MSE) <code>DiurnalDTWFidelity</code> <code>diurnal_dtw</code> min Hour-of-day profile preservation (DTW distance) <code>DTWFidelity</code> <code>dtw</code> min Full series shape similarity (Dynamic Time Warping) <code>DiversityReward</code> <code>diversity</code> max Spread of representatives in feature space (avg pairwise distance) <code>CentroidBalance</code> <code>centroid_balance</code> min Feature centroid deviation from global mean <code>CoverageBalance</code> <code>coverage_balance</code> min Balanced coverage via RBF kernel soft assignment <pre><code>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n    'diversity':   (0.5, rep.DiversityReward()),\n})\n</code></pre> <p>The weight (first element of each tuple) expresses relative importance. Components with <code>direction=\"min\"</code> are better when smaller; <code>direction=\"max\"</code> are better when larger.</p>"},{"location":"modules/#s-selection-space","title":"S: Selection Space","text":"<p>A <code>CombinationGenerator</code> defines which subsets the search algorithm considers.</p> Implementation Description <code>ExhaustiveCombiGen</code> All k-of-n combinations. Feasible for small n (e.g., 12 months, k=4 gives 495 candidates). <code>GroupQuotaCombiGen</code> Enforces exact quotas per group (e.g., 1 month per season). <code>ExhaustiveHierarchicalCombiGen</code> Selects parent groups (e.g., months) but evaluates on child slices (e.g., days). <code>GroupQuotaHierarchicalCombiGen</code> Combines hierarchical selection with group quotas. <pre><code># Simple: all 4-of-12 monthly combinations\ncombi_gen = rep.ExhaustiveCombiGen(k=4)\n\n# Hierarchical with seasonal constraints\ncombi_gen = rep.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n    parent_k=4,\n    dt_index=df_raw.index,\n    child_slicer=rep.TimeSlicer(unit=\"day\"),\n    group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1},\n)\n</code></pre>"},{"location":"modules/#r-representation-model","title":"R: Representation Model","text":"<p>Determines how selected periods represent the full dataset through responsibility weights.</p> Implementation Description <code>UniformRepresentationModel</code> Equal 1/k weights. Simplest option. <code>KMedoidsClustersizeRepresentation</code> Weights proportional to cluster sizes from k-medoids hard assignment. <code>BlendedRepresentationModel</code> Soft assignment: each original slice is a convex combination of representatives. Returns a weight matrix instead of a weight dict. <pre><code># Equal weights\nuniform = rep.UniformRepresentationModel()\n\n# Cluster-proportional weights\nkmedoids = rep.KMedoidsClustersizeRepresentation()\n\n# Soft blending (returns a DataFrame, not a dict)\nblended = rep.BlendedRepresentationModel(blend_type='convex')\n</code></pre>"},{"location":"modules/#a-search-algorithm","title":"A: Search Algorithm","text":"<p>The engine that finds the optimal selection.</p> Implementation Workflow Type Description <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> Generate-and-Test Evaluates all candidate combinations and selects the winner via a <code>SelectionPolicy</code>."},{"location":"modules/#selection-policies","title":"Selection Policies","text":"<p>The policy decides how to pick a winner from the scored candidates:</p> Policy Description <code>WeightedSumPolicy</code> Scalar aggregation of scores. Supports <code>normalization='robust_minmax'</code> for multi-objective balance. <code>ParetoMaxMinStrategy</code> Selects the Pareto-optimal solution that maximizes its worst-performing objective. <code>ParetoUtopiaPolicy</code> Selects the Pareto-optimal solution closest to the utopia point. <pre><code># Weighted sum (default)\npolicy = rep.WeightedSumPolicy(normalization='robust_minmax')\nsearch = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\n\n# Pareto max-min\npolicy = rep.ParetoMaxMinStrategy()\nsearch = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\n</code></pre>"},{"location":"modules/#diagnostics","title":"Diagnostics","text":"<p>Interactive Plotly visualizations for inspecting results, feature spaces, and score component behavior. See the Gallery for rendered examples.</p>"},{"location":"modules/#feature-space","title":"Feature Space","text":"Class Purpose <code>FeatureSpaceScatter2D</code> 2D scatter plot of feature space <code>FeatureSpaceScatter3D</code> 3D scatter plot of feature space <code>FeatureSpaceScatterMatrix</code> Pairwise scatter matrix <code>PCAVarianceExplained</code> Cumulative variance explained by PCA components <code>FeatureCorrelationHeatmap</code> Correlation heatmap between features <code>FeatureDistributions</code> Distribution histograms per feature"},{"location":"modules/#results","title":"Results","text":"Class Purpose <code>ResponsibilityBars</code> Weight distribution across selected representatives <code>ParetoScatter2D</code> 2D objective-space scatter with Pareto front <code>ParetoScatterMatrix</code> Pairwise objective-space scatter matrix <code>ParetoParallelCoordinates</code> Parallel coordinates of Pareto front <code>ScoreContributionBars</code> Per-component score breakdown"},{"location":"modules/#score-components","title":"Score Components","text":"Class Purpose <code>DistributionOverlayECDF</code> ECDF comparison of full vs selected data <code>DistributionOverlayHistogram</code> Histogram comparison of full vs selected data <code>CorrelationDifferenceHeatmap</code> Correlation matrix difference heatmap <code>DiurnalProfileOverlay</code> Diurnal profile comparison"},{"location":"modules/#putting-it-together","title":"Putting It Together","text":"<pre><code>workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n\n# result.selection -&gt; tuple of selected slice identifiers\n# result.weights   -&gt; dict mapping each selected slice to its weight\n# result.scores    -&gt; dict mapping each objective name to its score\n</code></pre> <p>For a complete walkthrough, see the Getting Started guide. For the theoretical foundations, see the Unified Framework.</p>"},{"location":"unified_framework/","title":"A Unified Framework for Representative Subset Selection in Energy Time Series","text":""},{"location":"unified_framework/#abstract","title":"Abstract","text":"<p>Time series aggregation (TSA) \u2014 selecting or constructing a small set of representative periods from a larger dataset \u2014 is essential for making computationally expensive energy system models tractable. The field has produced a rich but fragmented array of methods: clustering, mathematical programming, autoencoders, greedy algorithms, and more. Each method is typically presented as a monolithic procedure, making it difficult to compare methods, understand their implicit assumptions, or assemble custom pipelines.</p> <p>This paper proposes a unified framework that decomposes any TSA method into five fundamental components: the Feature Space (how periods are represented), the Objective (what quality means), the Selection Space (what form the output takes), the Representation Model (how the selection approximates the whole), and the Search Algorithm (how the solution is found). We show that every established methodology is a specific instantiation of this five-component structure, and that the framework enables systematic comparison, exposes trade-offs, and provides an architectural blueprint for modular software.</p>"},{"location":"unified_framework/#1-introduction","title":"1. Introduction","text":"<p>Energy system models (ESMs) are central analytical tools for energy system planning and research. The integration of variable renewable energy sources (VRES), storage technologies, and cross-sectoral coupling demands high temporal resolution \u2014 typically hourly or sub-hourly \u2014 across a full year or longer. For many ESMs, particularly those used in capacity expansion planning, investment optimization, or market studies, this produces optimization problems that are computationally intractable.</p> <p>Time series aggregation addresses this by selecting a small number \\(k\\) of representative periods (days, weeks, months, or even years) that preserve the essential characteristics of the full dataset. A well-chosen selection can reduce computation by orders of magnitude while maintaining the fidelity of model results.</p> <p>The challenge is that \"representativeness\" is not a single, well-defined concept. At a fundamental level, the modeler must decide what to represent: the statistical properties of the input data (e.g., weather patterns, demand profiles) or the outcomes of the downstream model (e.g., socio-economic welfare, system cost, capacity investments). Within these categories, the practical meaning of \"representative\" varies further depending on the modeling question:</p> <ul> <li>Aggregate fidelity: the selected periods, when weighted appropriately, reproduce the overall statistics (means, distributions, correlations) of the full dataset \u2014 for instance, so that annualized system cost is captured accurately.</li> <li>State-space coverage: the selected periods span the diversity of conditions that occur \u2014 high wind with high demand, low wind with high solar, peak events, etc. \u2014 so that the model encounters all operationally distinct system states.</li> <li>A combination: cover the breadth of system states while also matching aggregate statistics \u2014 often under the constraint that all periods carry equal weight.</li> </ul> <p>Different TSA methods make different implicit choices about what to preserve, how to search, and what form the output takes. These choices are rarely made explicit, which makes comparison difficult.</p> <p>Inspired by unifying efforts in other fields \u2014 notably Powell's unified framework for sequential decision problems under uncertainty \u2014 we propose a decomposition of the TSA problem into five modular, interchangeable components. Any concrete method is a specific instantiation of this structure. The framework does not prescribe a single best method; rather, it provides a common language for describing, comparing, and assembling methods.</p> <p>The remainder of this paper is organized as follows. Section 2 presents the five-component framework. Section 3 demonstrates how established methods decompose into the framework. Section 4 discusses practical implications and open questions.</p>"},{"location":"unified_framework/#2-the-unified-framework","title":"2. The Unified Framework","text":""},{"location":"unified_framework/#21-overview","title":"2.1 Overview","text":"<p>Consider a dataset \\(D = \\{d_1, \\ldots, d_N\\}\\) of \\(N\\) time periods, where each \\(d_i\\) is a multivariate time series for period \\(i\\). The variables may include load, wind capacity factors, solar irradiance, temperature, and others, potentially across multiple regions \u2014 each region-variable pair is simply an additional dimension of the time series vector. The temporal granularity \u2014 days, weeks, months, years \u2014 is a problem parameter that depends on the application: one study may select representative days from a year, another representative weeks, and yet another a subset of representative years from a multi-decadal climate dataset.</p> <p>The goal is to find a selection \\(x\\) of \\(k \\ll N\\) representative periods (or constructs derived from them) such that some quality measure is optimized. We formalize this as:</p> \\[ x^* \\;=\\; \\underset{x \\,\\in\\, \\mathcal{S}}{\\arg\\min}\\; \\mathcal{O}\\!\\bigl(\\mathcal{R}(x,\\, D)\\bigr) \\] <p>where:</p> Symbol Component Role \\(\\mathcal{F}\\) Feature Space How periods are represented mathematically \\(\\mathcal{O}\\) Objective What quality measure is optimized \\(\\mathcal{S}\\) Selection Space What structural form the output takes \\(\\mathcal{R}\\) Representation Model How the selection approximates the full dataset \\(\\mathcal{A}\\) Search Algorithm How the optimal selection is found <p>Any concrete TSA method is defined by a specific 5-tuple \\((\\mathcal{F},\\, \\mathcal{O},\\, \\mathcal{S},\\, \\mathcal{R},\\, \\mathcal{A})\\).</p> <p>The five components are conceptually independent: each addresses a distinct design decision. In practice, certain combinations are more natural than others, and some methods couple components tightly. Making these couplings explicit is one of the framework's main contributions.</p> <p>A note on the role of \\(\\mathcal{R}\\) during search. The formulation above presents the ideal: the objective evaluates the quality of the representation, not of the raw selection. In practice, however, most combinatorial search methods (\\(\\mathcal{A}_\\text{comb}\\)) evaluate candidates by comparing the raw data of the selected periods against the full dataset \u2014 effectively bypassing \\(\\mathcal{R}\\) during the search and applying it only after the best selection has been identified. This is a pragmatic simplification: computing \\(\\mathcal{R}\\) for every candidate in an exhaustive search can be expensive, and for simple representation models like \\(\\mathcal{R}_\\text{equal}\\) the difference is negligible. Methods that jointly optimize selection and representation \u2014 such as \\(\\mathcal{A}_\\text{optim}\\) (MILP) or \\(\\mathcal{A}_\\text{construct}\\) (clustering, where the assignment is the representation) \u2014 adhere more closely to the full formulation.</p> <p>The following subsections define each component, its variants, and their trade-offs.</p>"},{"location":"unified_framework/#22-component-1-feature-space-mathcalf-how-we-see-the-data","title":"2.2 Component 1: Feature Space (\\(\\mathcal{F}\\)) \u2014 How We See the Data","text":"<p>Before periods can be compared, grouped, or evaluated, they must be represented as mathematical objects. The feature space \\(\\mathcal{F}\\) defines this representation:</p> \\[\\mathcal{F}: D \\;\\to\\; \\{z_1, \\ldots, z_N\\}, \\quad z_i \\in \\mathbb{R}^p\\] <p>where \\(z_i\\) is the feature vector for period \\(i\\), and \\(p\\) is the dimensionality of the feature space.</p> <p>The choice of \\(\\mathcal{F}\\) is consequential: it defines what \"similar\" means. Two periods that are close in one feature space may be distant in another. The feature space shapes how the objective is computed and how the search algorithm operates.</p> <p>Variants:</p> Variant Description \\(\\mathcal{F}_\\text{direct}\\) Raw time-series vectors. For a period of \\(H\\) time steps and \\(V\\) variables (where \\(V\\) may include multiple regions): \\(z_i \\in \\mathbb{R}^{V \\times H}\\). Complete but high-dimensional. \\(\\mathcal{F}_\\text{stat}\\) Hand-crafted statistical summaries: means, standard deviations, quantiles, ramp rates, correlations, etc. Lower-dimensional and interpretable, but depends on the modeler's choice of features. \\(\\mathcal{F}_\\text{latent}\\) Learned low-dimensional representations via PCA, autoencoders, or other dimensionality reduction. Captures complex patterns automatically, including nonlinear structure (autoencoders). \\(\\mathcal{F}_\\text{model}\\) Features derived from running a simplified model (e.g., a dispatch model) for each period. Model outputs \u2014 generation mix, storage dispatch, marginal prices \u2014 are used as features, potentially combined with input data. This makes the feature space \"problem-aware\": periods are grouped by their operational impact, not just their statistical appearance. <p>\\(\\mathcal{F}_\\text{direct}\\) is the default when no explicit feature engineering is performed (e.g., standard k-means on raw hourly data). \\(\\mathcal{F}_\\text{stat}\\) and \\(\\mathcal{F}_\\text{latent}\\) trade information for computational efficiency and noise reduction. \\(\\mathcal{F}_\\text{model}\\) is the most advanced variant, requiring preliminary model runs but offering the strongest link between input representation and downstream model fidelity.</p> <p>These variants can be composed: for instance, \\(\\mathcal{F}_\\text{model}\\) features may be passed through an autoencoder to produce a \\(\\mathcal{F}_\\text{latent}\\) representation that encodes both input patterns and model responses.</p>"},{"location":"unified_framework/#23-component-2-objective-mathcalo-what-we-want-to-preserve","title":"2.3 Component 2: Objective (\\(\\mathcal{O}\\)) \u2014 What We Want to Preserve","text":"<p>The objective defines the quality measure that the selection should optimize. This is the most consequential choice in the framework, as it determines what \"representative\" means for a given application.</p> \\[\\mathcal{O}: \\mathcal{R}(x, D) \\;\\to\\; \\mathbb{R} \\quad (\\text{or } \\mathbb{R}^m \\text{ in the multi-objective case})\\] <p>At the highest level, objectives fall into two categories:</p> Variant Description \\(\\mathcal{O}_\\text{stat}\\) Statistical fidelity. Preserve statistical properties of the input data. This is a proxy for the true goal. \\(\\mathcal{O}_\\text{model}\\) Model outcome fidelity. Preserve the results of the downstream optimization model (total cost, capacity mix, emissions). This is the true goal, but typically requires running the model during the selection process. <p>Most practical methods use \\(\\mathcal{O}_\\text{stat}\\) because evaluating \\(\\mathcal{O}_\\text{model}\\) during the selection process is computationally expensive. The fundamental challenge of TSA is that the relationship between statistical fidelity and model outcome fidelity is complex and nonlinear: low statistical error does not guarantee accurate model results.</p>"},{"location":"unified_framework/#statistical-objectives-in-practice","title":"Statistical objectives in practice","text":"<p>In practice, the modeler faces a fundamental choice about what kind of statistical fidelity matters:</p> <p>Aggregate fidelity. The selection should reproduce the overall statistics of the full dataset \u2014 annual means, marginal distributions, correlation structures, duration curves. This is the right goal when the model question concerns aggregate outcomes (e.g., annualized system cost, total generation mix). Metrics include:</p> <ul> <li>Distributional distance (Wasserstein, duration curve NRMSE): how well does the weighted selection reproduce the marginal distributions?</li> <li>Correlation fidelity (Frobenius norm of correlation matrix difference): does the selection preserve the dependencies between variables?</li> <li>Diurnal pattern fidelity (MSE of mean hourly profiles): does the selection reproduce typical intra-day shapes?</li> </ul> <p>State-space coverage. The selection should span the diversity of conditions that occur in the full dataset \u2014 capturing distinct system states such as \"high wind + low demand,\" \"low VRES + peak demand,\" \"shoulder season with storage cycling,\" etc. This is the right goal when the model question concerns system adequacy, resilience, or the identification of binding constraints. Metrics include:</p> <ul> <li>Diversity (mean pairwise distance in feature space): are the selected periods sufficiently different from each other?</li> <li>Coverage balance (uniformity of representation responsibilities): does each selected period \"cover\" a roughly equal portion of the full dataset?</li> <li>Centroid balance (distance from selection centroid to global center): does the selection avoid systematic bias toward extreme conditions?</li> </ul> <p>Note that diversity and coverage metrics evaluate properties of the selection itself in feature space, rather than how well the representation matches the full dataset. They complement fidelity metrics by ensuring the selection is well-spread and balanced, which is especially important when combined objectives or equal-weight constraints are in play.</p> <p>Combined objectives. In many applications, both aggregate fidelity and state-space coverage matter simultaneously. A common practical scenario is the requirement that the selected periods carry equal weight (see \\(\\mathcal{R}_\\text{equal}\\) in Section 2.5), which means the selection must be intrinsically representative \u2014 it cannot rely on non-uniform weights to correct for bias. In this setting, the selection must:</p> <ol> <li>land close to the center of the data distribution (aggregate fidelity), and</li> <li>span a broad range of system states (coverage/diversity).</li> </ol> <p>These two goals are in natural tension: optimizing for aggregate fidelity tends to select \"average\" periods, while optimizing for coverage tends to select \"extreme\" or \"boundary\" periods. This tension is resolved through multi-objective optimization, where the trade-off frontier (Pareto front) between the objectives is computed explicitly, and the modeler chooses a preferred balance. Multi-objective formulations thus provide a systematic alternative to multi-stage approaches (see Section 2.6, \\(\\mathcal{A}_\\text{hybrid}\\)).</p>"},{"location":"unified_framework/#24-component-3-selection-space-mathcals-what-we-are-picking","title":"2.4 Component 3: Selection Space (\\(\\mathcal{S}\\)) \u2014 What We Are Picking","text":"<p>The selection space defines the structural form of the output \u2014 what kind of object \\(x\\) is.</p> Variant Description \\(\\mathcal{S}_\\text{subset}\\) Historical subset. \\(x \\subset \\{1, \\ldots, N\\}\\) with \\(\\lvert x \\rvert = k\\). The output is a set of \\(k\\) actual periods from the original data. \\(\\mathcal{S}_\\text{synthetic}\\) Synthetic archetypes. \\(x = \\{p_1, \\ldots, p_k\\}\\) where each \\(p_j \\in \\mathbb{R}^{V \\times H}\\) is an artificial period (e.g., a cluster centroid). These may not correspond to any historical period. \\(\\mathcal{S}_\\text{chrono}\\) Chronological segments. \\(x = \\{(t_1, l_1), \\ldots, (t_k, l_k)\\}\\) where \\((t_j, l_j)\\) defines a contiguous segment of variable length \\(l_j\\) starting at time \\(t_j\\). Segments collectively cover the full timeline. <p>\\(\\mathcal{S}_\\text{subset}\\) is the most common choice because it guarantees that each representative period is a physically realistic, historical pattern. It is the natural output of k-medoids clustering, combinatorial search, and greedy selection methods.</p> <p>\\(\\mathcal{S}_\\text{synthetic}\\) arises from methods that construct artificial representatives, such as k-means clustering, where centroids are computed as averages over cluster members. When clustering is performed in \\(\\mathcal{F}_\\text{direct}\\) (raw time-series space), centroids are themselves time series and can be used directly as synthetic periods \u2014 though they may produce physically unrealistic profiles (e.g., smoothed-out peaks). When clustering is performed in a reduced feature space (\\(\\mathcal{F}_\\text{stat}\\) or \\(\\mathcal{F}_\\text{latent}\\)), the centroid exists in feature space, not in time-series space. Recovering a synthetic time series then requires an inverse mapping \u2014 for instance, a decoder in the case of \\(\\mathcal{F}_\\text{latent}\\) (autoencoders). For \\(\\mathcal{F}_\\text{stat}\\), no natural inverse exists, which is one reason k-medoids (\\(\\mathcal{S}_\\text{subset}\\)) is generally preferred over k-means (\\(\\mathcal{S}_\\text{synthetic}\\)) when working with engineered features.</p> <p>\\(\\mathcal{S}_\\text{chrono}\\) preserves the chronological ordering of the original data, which is critical for models with long-duration storage or seasonal dynamics. It is the output of Chronological Time-Period Clustering (CTPC) and related methods.</p> <p>The choice of \\(\\mathcal{S}\\) interacts with other components: \\(\\mathcal{S}_\\text{subset}\\) pairs naturally with \\(\\mathcal{A}_\\text{comb}\\) (combinatorial search) and \\(\\mathcal{A}_\\text{construct}\\) (clustering with medoid selection), while \\(\\mathcal{S}_\\text{chrono}\\) requires specialized constructive algorithms that enforce contiguity.</p>"},{"location":"unified_framework/#25-component-4-representation-model-mathcalr-how-the-selection-represents-the-whole","title":"2.5 Component 4: Representation Model (\\(\\mathcal{R}\\)) \u2014 How the Selection Represents the Whole","text":"<p>Given a selection \\(x\\) of \\(k\\) representatives, the representation model defines how the full dataset \\(D\\) is approximated for use in the downstream model. The output of \\(\\mathcal{R}\\) is the reduced input to the downstream model: a set of representative periods and their associated weights or reconstruction rules.</p> Variant Description \\(\\mathcal{R}_\\text{equal}\\) Equal weighting. Each selected period receives weight \\(1/k\\). No assignment of original periods to representatives is performed. \\(\\mathcal{R}_\\text{hard}\\) Hard assignment. Each of the \\(N\\) original periods is assigned to a single closest representative. The weight of representative \\(j\\) is proportional to the number of periods assigned to it. \\(\\mathcal{R}_\\text{soft}\\) Blended representation. Each original period \\(i\\) is approximated as a weighted combination of all \\(k\\) representatives: \\(d_i \\approx \\sum_{j=1}^k w_{ij} \\cdot x_j\\)."},{"location":"unified_framework/#mathcalr_textequal-equal-weighting","title":"\\(\\mathcal{R}_\\text{equal}\\): Equal weighting","text":"<p>The simplest representation model. Each of the \\(k\\) selected periods is treated identically in the downstream model, with weight \\(1/k\\).</p> <p>This choice is common in practice when the downstream model cannot accommodate non-uniform period weights \u2014 for instance, when the model is formulated to run each representative period once, and the results are simply averaged. It places the strongest requirements on the selection itself: since weights cannot compensate for a biased selection, the \\(k\\) periods must be intrinsically representative. The aggregate statistics of the equally-weighted selection must match those of the full dataset, while simultaneously covering the relevant state space.</p>"},{"location":"unified_framework/#mathcalr_texthard-hard-assignment","title":"\\(\\mathcal{R}_\\text{hard}\\): Hard assignment","text":"<p>Each original period is mapped to its single closest representative. The weight of representative \\(j\\) reflects the number of original periods it represents:</p> \\[w_j \\;=\\; \\frac{|\\{i : r(i) = j\\}|}{N}, \\quad \\text{where } r(i) = \\underset{j \\in x}{\\arg\\min}\\; \\|z_i - z_j\\|\\] <p>This is the standard output of clustering methods. The downstream model runs \\(k\\) periods, each weighted by \\(w_j\\), so that the weighted aggregate approximates the full-year result.</p> <p>The assignment function \\(r(\\cdot)\\) and distance metric can vary: Euclidean distance on features, DTW distance on raw series, RBF kernel similarity, or PCA-based k-medoids assignment. The choice of assignment method is a sub-decision within \\(\\mathcal{R}_\\text{hard}\\).</p> <p>Variants also exist where the weights are not simply cluster sizes but are optimized (e.g., via MILP) to minimize some reconstruction error, decoupling the weight computation from the cluster assignment.</p>"},{"location":"unified_framework/#mathcalr_textsoft-blended-representation","title":"\\(\\mathcal{R}_\\text{soft}\\): Blended representation","text":"<p>Each original period \\(i\\) is approximated as a weighted combination of all \\(k\\) representatives:</p> \\[d_i \\;\\approx\\; \\sum_{j=1}^k w_{ij} \\cdot x_j\\] <p>with constraints on the weight vectors \\(w_i = (w_{i1}, \\ldots, w_{ik})\\), typically:</p> <ul> <li>\\(w_{ij} \\geq 0\\) (non-negativity), and optionally</li> <li>\\(\\sum_j w_{ij} = 1\\) (convex combination) or \\(\\sum_j w_{ij}\\) unconstrained (conic combination).</li> </ul> <p>The blended representation provides a much richer approximation: rather than collapsing each original period to a single representative, it reconstructs each one from the full basis of representatives. This is the key idea in hull clustering, where the selected periods form the vertices of a polytope that spans the data.</p> <p>However, \\(\\mathcal{R}_\\text{soft}\\) requires the downstream model to handle blended inputs \u2014 the time-series parameters (load, VRES profiles) for each modeled period are themselves weighted sums of the representative profiles. This requires a different ESM formulation than the standard weighted-period approach.</p>"},{"location":"unified_framework/#duration-scaling","title":"Duration scaling","text":"<p>When periods have unequal durations \u2014 for instance, calendar months ranging from 28 to 31 days \u2014 the raw responsibility weights produced by any \\(\\mathcal{R}\\) variant should be adjusted to reflect the actual time span each representative covers. Without this adjustment, a representative month of 28 days and one of 31 days would receive the same weight despite covering different fractions of the year.</p> <p>Duration scaling is not a separate representation model but a practical post-processing refinement applicable to \\(\\mathcal{R}_\\text{equal}\\), \\(\\mathcal{R}_\\text{hard}\\), and \\(\\mathcal{R}_\\text{soft}\\) alike. The adjustment is straightforward: each weight \\(w_j\\) is multiplied by the duration \\(l_j\\) of period \\(j\\), and the result is renormalized so that the weights sum to the total time horizon. For \\(\\mathcal{R}_\\text{equal}\\) with monthly periods, this means a 31-day month receives slightly more weight than a 28-day month, ensuring that the weighted reconstruction accounts for the correct number of hours per period.</p>"},{"location":"unified_framework/#26-component-5-search-algorithm-mathcala-how-we-find-the-solution","title":"2.6 Component 5: Search Algorithm (\\(\\mathcal{A}\\)) \u2014 How We Find the Solution","text":"<p>The search algorithm is the computational procedure that finds \\(x^*\\) (or an approximation of it). Different algorithms impose different requirements on the other components and exhibit different computational trade-offs.</p> Variant Description \\(\\mathcal{A}_\\text{comb}\\) Combinatorial search. Enumerate or sample candidate selections from \\(\\mathcal{S}\\), evaluate each via \\(\\mathcal{O}\\), select the best. \\(\\mathcal{A}_\\text{construct}\\) Constructive algorithms. Build the solution incrementally: clustering (k-means, k-medoids, hierarchical) or greedy selection (forward selection, hull vertex identification). \\(\\mathcal{A}_\\text{optim}\\) Mathematical programming. Formulate the selection as an optimization problem (typically MILP) and solve it with a general-purpose solver. \\(\\mathcal{A}_\\text{hybrid}\\) Multi-stage or composite. Combine multiple algorithms or objectives \u2014 e.g., first select \"typical\" periods via clustering, then add \"extreme\" periods via optimization."},{"location":"unified_framework/#mathcala_textcomb-combinatorial-search","title":"\\(\\mathcal{A}_\\text{comb}\\): Combinatorial search","text":"<p>The most flexible approach. Candidate selections are generated (exhaustively or via metaheuristics such as genetic algorithms, simulated annealing, or random sampling) and evaluated against \\(\\mathcal{O}\\). This decouples the search from the objective: any \\(\\mathcal{O}\\) can be used, including multi-objective formulations.</p> <p>The main limitation is scalability. The number of possible \\(k\\)-subsets from \\(N\\) periods is \\(\\binom{N}{k}\\), which grows combinatorially. Exhaustive enumeration is feasible only for small problems (e.g., \\(\\binom{52}{8} \\approx 7.5 \\times 10^8\\) is already impractical without further constraints). Metaheuristics can handle larger problems but provide no optimality guarantees.</p> <p>A key advantage of \\(\\mathcal{A}_\\text{comb}\\) is its compatibility with multi-objective optimization: by evaluating each candidate on multiple objectives, it naturally produces Pareto fronts, enabling the modeler to inspect trade-offs explicitly.</p> <p>Selection policies. When the objective is multi-dimensional (\\(\\mathcal{O} \\to \\mathbb{R}^m\\)), the combinatorial search produces a table of \\(m\\) scores per candidate. A selection policy resolves this into a single winner. Common strategies include weighted-sum aggregation (simple but requires choosing weights a priori), utopia-distance methods (select the Pareto-optimal point closest to the ideal), and max-min fairness (select the Pareto-optimal point that maximizes the worst-performing objective). The choice of policy is a sub-decision within \\(\\mathcal{A}_\\text{comb}\\) that can significantly affect which selection is returned, even when the Pareto front is identical.</p> <p>Structured candidate generation. The scalability of \\(\\mathcal{A}_\\text{comb}\\) can be improved by constraining the search space at the generation stage. Beyond simple group quotas (e.g., \"one period per season\"), hierarchical generation enables evaluation at a finer granularity than the selection unit. For example, features may be computed at daily resolution while the selection operates at the monthly level: each candidate is a set of complete months, but the objective evaluates the daily data within those months. This hierarchical approach \u2014 combining group quotas with multi-resolution evaluation \u2014 dramatically reduces the combinatorial space while preserving the flexibility of the generate-and-test paradigm.</p>"},{"location":"unified_framework/#mathcala_textconstruct-constructive-algorithms","title":"\\(\\mathcal{A}_\\text{construct}\\): Constructive algorithms","text":"<p>These algorithms build the solution incrementally rather than evaluating complete candidates.</p> <p>Clustering algorithms (k-means, k-medoids, hierarchical agglomerative) iteratively refine an assignment of periods to clusters. The output is a set of cluster representatives (centroids for k-means, medoids for k-medoids) and an implicit hard assignment. The objective is built into the algorithm: k-means minimizes within-cluster sum of squares; k-medoids minimizes within-cluster sum of distances.</p> <p>Note: k-medoids selects actual data points as representatives (\\(\\mathcal{S}_\\text{subset}\\)), while k-means produces centroids (\\(\\mathcal{S}_\\text{synthetic}\\)). Hierarchical methods can produce either, depending on how representatives are extracted from the dendrogram.</p> <p>Greedy algorithms build the selection one element at a time. At each step, the period that provides the greatest improvement to the objective is added. Hull clustering uses a greedy strategy to identify extreme points (hull vertices) that define the boundary of the data distribution.</p> <p>Constructive algorithms are typically fast and scalable, but they couple the search algorithm tightly with the objective \u2014 you cannot easily swap in a different \\(\\mathcal{O}\\) without changing the algorithm itself.</p>"},{"location":"unified_framework/#mathcala_textoptim-mathematical-programming","title":"\\(\\mathcal{A}_\\text{optim}\\): Mathematical programming","text":"<p>The selection problem is formulated as a mathematical optimization problem, typically a Mixed-Integer Linear Program (MILP), with binary decision variables \\(y_i \\in \\{0, 1\\}\\) indicating whether period \\(i\\) is selected:</p> \\[\\min \\;\\mathcal{O}(\\ldots) \\quad \\text{s.t.} \\quad \\sum_{i=1}^N y_i = k, \\quad \\text{and problem-specific constraints}\\] <p>This approach can provide global optimality guarantees (within solver tolerances) and naturally handles complex constraints. It is the standard approach for duration-curve-based selection and interregional optimization.</p> <p>Note that structural constraints such as \"select at least one period from each season\" do not necessarily require mathematical programming. They can also be enforced within \\(\\mathcal{A}_\\text{comb}\\) by constraining the candidate generation itself \u2014 for example, by enumerating only combinations that satisfy group quotas.</p> <p>The limitation is that the objective must be expressible in a form compatible with the solver (linear or quadratic for LP/MILP). Complex, nonlinear objectives or multi-objective formulations may be difficult to encode.</p>"},{"location":"unified_framework/#mathcala_texthybrid-multi-stage-and-composite-approaches","title":"\\(\\mathcal{A}_\\text{hybrid}\\): Multi-stage and composite approaches","text":"<p>Hybrid approaches combine multiple algorithms or objectives in stages. A common pattern is:</p> <ol> <li>Select \\(k_1\\) \"typical\" periods using a clustering or combinatorial method.</li> <li>Select \\(k_2 = k - k_1\\) \"extreme\" or \"critical stress\" periods using optimization-based identification (e.g., identifying periods with the highest system stress via slack variables in a preliminary model run).</li> </ol> <p>This is pragmatic and widely used. Alternatively, the same goal \u2014 balancing aggregate fidelity with extreme-event coverage \u2014 can be pursued through multi-objective optimization within a single \\(\\mathcal{A}_\\text{comb}\\) framework, where both fidelity and coverage are explicit objectives. One advantage of the multi-objective approach is that trade-offs are made transparent and the modeler retains full control over the balance, rather than committing to a fixed \\(k_1\\)/\\(k_2\\) split a priori.</p>"},{"location":"unified_framework/#3-methods-as-framework-instances","title":"3. Methods as Framework Instances","text":"<p>The framework's utility lies in its ability to decompose any TSA method into a specific \\((\\mathcal{F}, \\mathcal{O}, \\mathcal{S}, \\mathcal{R}, \\mathcal{A})\\) tuple, making implicit choices explicit and enabling direct comparison.</p>"},{"location":"unified_framework/#31-decomposition-table","title":"3.1 Decomposition Table","text":"<p>The table below classifies established methodologies. Each row is a specific instantiation of the five components.</p> Methodology \\(\\mathcal{F}\\) \\(\\mathcal{O}\\) \\(\\mathcal{S}\\) \\(\\mathcal{R}\\) \\(\\mathcal{A}\\) k-means \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): min. intra-cluster variance \\(\\mathcal{S}_\\text{synthetic}\\) (centroids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) k-medoids (PAM) \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): min. intra-cluster distance \\(\\mathcal{S}_\\text{subset}\\) (medoids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Hierarchical clustering \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): linkage criterion \\(\\mathcal{S}_\\text{subset}\\) or \\(\\mathcal{S}_\\text{synthetic}\\) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Duration curve MILP \\(\\mathcal{F}_\\text{direct}\\) \\(\\mathcal{O}_\\text{stat}\\): min. duration curve NRMSE \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) (optimized weights) \\(\\mathcal{A}_\\text{optim}\\) (MILP) Interregional MILP (NREL) \\(\\mathcal{F}_\\text{direct}\\) \\(\\mathcal{O}_\\text{stat}\\): min. regional errors \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) (optimized weights) \\(\\mathcal{A}_\\text{optim}\\) (MILP) Autoencoder (inputs only) \\(\\mathcal{F}_\\text{latent}\\) \\(\\mathcal{O}_\\text{stat}\\): min. latent distance \\(\\mathcal{S}_\\text{subset}\\) (medoids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Autoencoder (inputs + outputs) \\(\\mathcal{F}_\\text{model}\\) + \\(\\mathcal{F}_\\text{latent}\\) \\(\\mathcal{O}_\\text{stat}\\): min. latent distance \\(\\mathcal{S}_\\text{subset}\\) (medoids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) DTW-based clustering \\(\\mathcal{F}_\\text{direct}\\) (DTW metric) \\(\\mathcal{O}_\\text{stat}\\): min. DTW intra-cluster \\(\\mathcal{S}_\\text{subset}\\) or \\(\\mathcal{S}_\\text{synthetic}\\) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Snippet algorithm \\(\\mathcal{F}_\\text{direct}\\) (subsequences) \\(\\mathcal{O}_\\text{stat}\\): min. subsequence distance \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{construct}\\) (greedy) CTPC \\(\\mathcal{F}_\\text{direct}\\) \\(\\mathcal{O}_\\text{stat}\\): min. intra-cluster distance \\(\\mathcal{S}_\\text{chrono}\\) \\(\\mathcal{R}_\\text{hard}\\) (implicit) \\(\\mathcal{A}_\\text{construct}\\) (hierarchical + contiguity) Hull clustering (blended) \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): min. projection error \\(\\mathcal{S}_\\text{subset}\\) (hull vertices) \\(\\mathcal{R}_\\text{soft}\\) (blended) \\(\\mathcal{A}_\\text{construct}\\) (greedy) Multi-objective Pareto \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): multi-objective (Pareto) \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{equal}\\) or \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{comb}\\) (exhaustive / GA) Extreme event (slack vars) \\(\\mathcal{F}_\\text{model}\\) \\(\\mathcal{O}_\\text{model}\\): max. system stress \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{hybrid}\\) (model-in-loop) Hybrid (typical + extreme) mixed \\(\\mathcal{O}_\\text{stat}\\) + \\(\\mathcal{O}_\\text{model}\\) \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{hybrid}\\)"},{"location":"unified_framework/#32-observations","title":"3.2 Observations","text":"<p>Several insights emerge from the decomposition:</p> <ol> <li> <p>Most methods differ in only one or two components. Moving from k-medoids to autoencoder-based selection changes only \\(\\mathcal{F}\\) (from statistical to latent/model-informed). Moving from k-medoids to hull clustering changes \\(\\mathcal{R}\\) (from hard to soft) and \\(\\mathcal{A}\\) (from centroid-based to greedy). The other components remain the same. This makes trade-offs explicit and isolable.</p> </li> <li> <p>\\(\\mathcal{R}_\\text{soft}\\) is uncommon in the current literature. Almost all established methods use hard assignment or equal weighting. Blended representation (hull clustering) is a recent innovation that requires changes to the downstream model formulation.</p> </li> <li> <p>\\(\\mathcal{O}_\\text{model}\\) is the frontier. Most methods operate entirely on statistical objectives. Direct optimization for model outcome fidelity requires model-in-the-loop methods (extreme event identification via slack variables, autoencoder with model outputs), which are computationally expensive but represent the state of the art.</p> </li> <li> <p>Multi-objective optimization offers an alternative to hybrid approaches. The hybrid approach (typical + extreme) is pragmatic and widely used for balancing aggregate fidelity with extreme-event coverage. Multi-objective formulations provide an alternative: they compute the full trade-off frontier and let the modeler choose, rather than committing to a fixed split.</p> </li> <li> <p>The choice of \\(\\mathcal{A}\\) often dominates the method's identity. Methods are typically named after their search algorithm (k-means, MILP, genetic algorithm), but the other components \u2014 particularly \\(\\mathcal{F}\\) and \\(\\mathcal{O}\\) \u2014 often have a greater impact on the quality of the result. The framework redirects attention from how the search is conducted to what is being optimized and how quality is measured.</p> </li> </ol>"},{"location":"unified_framework/#4-discussion","title":"4. Discussion","text":""},{"location":"unified_framework/#41-component-interactions-and-coupling","title":"4.1 Component Interactions and Coupling","text":"<p>While the five components are conceptually independent, certain combinations are tightly coupled in practice:</p> <ul> <li>\\(\\mathcal{A}_\\text{construct}\\) couples \\(\\mathcal{A}\\) with \\(\\mathcal{O}\\): clustering algorithms have their objective function built in (e.g., k-means minimizes within-cluster variance). You cannot freely swap \\(\\mathcal{O}\\) without changing \\(\\mathcal{A}\\).</li> <li>\\(\\mathcal{R}_\\text{soft}\\) couples \\(\\mathcal{R}\\) with the downstream model: the ESM must be formulated to accept blended inputs, which is a non-trivial modeling change.</li> <li>\\(\\mathcal{F}_\\text{model}\\) couples \\(\\mathcal{F}\\) with a preliminary model run: model-informed features require access to a simplified ESM, creating a dependency between the feature engineering pipeline and the modeling workflow.</li> </ul> <p>The framework makes these couplings visible, helping practitioners understand which components they can modify independently and which require coordinated changes.</p>"},{"location":"unified_framework/#42-practical-guidance","title":"4.2 Practical Guidance","text":"<p>The choice of components should be guided by the modeling question:</p> <ul> <li>If aggregate cost accuracy is the priority and non-uniform period weights are acceptable: \\(\\mathcal{O}_\\text{stat}\\) with distributional metrics, \\(\\mathcal{R}_\\text{hard}\\) with optimized weights, \\(\\mathcal{A}_\\text{optim}\\) (duration curve MILP).</li> <li>If equal-weight periods are required: \\(\\mathcal{R}_\\text{equal}\\), combined with a multi-objective \\(\\mathcal{O}\\) that balances aggregate fidelity with coverage/diversity, and \\(\\mathcal{A}_\\text{comb}\\) for explicit trade-off analysis.</li> <li>If the model has significant storage or temporal coupling: \\(\\mathcal{F}\\) should capture temporal dynamics (DTW, ramp features), or \\(\\mathcal{S}_\\text{chrono}\\) should be used to preserve chronology.</li> <li>If the problem space is very large (e.g., selecting 10 periods from 365 days): \\(\\mathcal{A}_\\text{construct}\\) (clustering) or \\(\\mathcal{A}_\\text{optim}\\) (MILP) for scalability; exhaustive \\(\\mathcal{A}_\\text{comb}\\) is infeasible.</li> <li>If model outcome fidelity is critical and computational budget allows: \\(\\mathcal{F}_\\text{model}\\) with autoencoder, or \\(\\mathcal{A}_\\text{hybrid}\\) with slack-variable-based extreme identification.</li> </ul>"},{"location":"unified_framework/#43-open-questions","title":"4.3 Open Questions","text":"<p>Several aspects of the framework invite further investigation:</p> <ol> <li>Systematic benchmarking. The framework enables, and calls for, controlled experiments comparing different component combinations on the same datasets and downstream models.</li> <li>Better proxies for \\(\\mathcal{O}_\\text{model}\\). Developing statistical objectives that are stronger predictors of model outcome fidelity \u2014 without requiring model-in-the-loop evaluation \u2014 remains a major research gap.</li> <li>Automated component selection. Can the best \\((\\mathcal{F}, \\mathcal{O}, \\mathcal{S}, \\mathcal{R}, \\mathcal{A})\\) tuple be selected automatically based on problem characteristics?</li> <li>\\(\\mathcal{R}_\\text{soft}\\) adoption. Blended representations promise higher fidelity but require ESM reformulation. Quantifying the fidelity gain is important for motivating this effort.</li> </ol>"},{"location":"unified_framework/#5-conclusion","title":"5. Conclusion","text":"<p>The field of representative period selection for energy time series has produced a rich and growing body of methods, each designed with care for specific use cases. However, these methods are typically presented as monolithic procedures, which obscures their shared structure and makes systematic comparison difficult.</p> <p>The five-component framework proposed in this paper \u2014 Feature Space, Objective, Selection Space, Representation Model, and Search Algorithm \u2014 provides a common structure for understanding any TSA method. By decomposing methods into their fundamental choices, the framework:</p> <ul> <li>makes implicit assumptions explicit,</li> <li>enables direct, component-level comparison between methods,</li> <li>guides practitioners in assembling custom pipelines suited to their specific modeling questions, and</li> <li>provides an architectural blueprint for modular software design.</li> </ul> <p>The framework does not claim that one component combination is universally superior. Rather, it provides the vocabulary and structure needed to make informed, transparent choices \u2014 moving the field from ad-hoc method selection toward systematic, principled design of representative period selection pipelines.</p>"},{"location":"unified_framework/#references","title":"References","text":"<p>[To be populated with full citations from the literature. Key references include:]</p> <ol> <li>Hoffmann et al. (2020). \"A Review on Time Series Aggregation Methods for Energy System Models.\" Energies, 13(3), 641.</li> <li>Kotzur et al. (2022). \"Time-series aggregation for the optimization of energy systems: Goals, challenges, approaches, and opportunities.\" RSER, 157.</li> <li>Nahmmacher et al. (2016). \"Selecting Representative Days for Capturing the Implications of Integrating Intermittent Renewables in Generation Expansion Planning.\"</li> <li>Scott et al. (2022). \"Representative period selection for power system planning using autoencoder-based dimensionality reduction.\" arXiv:2204.13608.</li> <li>Sun et al. (2025). \"An Interregional Optimization Approach for Time Series Aggregation in Continent-Scale Electricity System Models.\" NREL/TP-6A20-90183.</li> <li>Teichgraeber &amp; Brandt (2024). \"On the Selection of Intermediate Length Representative Periods for Capacity Expansion.\" arXiv:2401.02888.</li> <li>Hull Clustering with Blended Representative Periods (2025). arXiv:2508.21641.</li> <li>Kotzur et al. (2018). \"Impact of different time series aggregation methods on optimal energy system design.\"</li> <li>Bahl et al. (2018). \"Typical periods for two-stage synthesis by time-series aggregation with bounded error in objective function.\"</li> <li>Poncelet et al. (2017). \"Selecting Representative Days for Investment Planning Models.\"</li> </ol>"},{"location":"workflow/","title":"Workflow Types","text":""},{"location":"workflow/#the-three-generalized-workflows","title":"The Three Generalized Workflows","text":"<p>Here are the three primary types of workflows that energy-repset supports. Each uses the five core modules in a slightly different sequence and with different emphasis.</p>"},{"location":"workflow/#workflow-1-generate-and-test","title":"Workflow 1: Generate-and-Test","text":"<p>This is the classic combinatorial search approach. Its philosophy is to create many candidate solutions, evaluate each one thoroughly, and then use a clear policy to select the best.</p> <ul> <li>Status: Fully implemented in the current software.</li> <li>Examples: Exhaustive (brute-force) search, Genetic Algorithms.</li> <li>How the Modules are Used:</li> <li>SearchAlgorithm (A): Its main role is to generate candidate subsets. This is often delegated to a CombinationGenerator (which can handle constraints like \"one per season\").</li> <li>ObjectiveSet (O): Used intensively. It is called to evaluate every single candidate subset generated by the search algorithm.</li> <li>SelectionPolicy (Pi): Used at the very end. After the search is complete, it receives a full table of all candidates and their scores, and applies its rule (e.g., WeightedSum) to pick the winner.</li> <li>Summary: There is a clear, linear sequence: the SearchAlgorithm generates, the ObjectiveSet evaluates, and the Policy decides.</li> </ul> <p>Key Modules:</p> Role Implementation Import Search <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <code>energy_repset.search_algorithms</code> Candidates <code>ExhaustiveCombiGen</code>, <code>GroupQuotaCombiGen</code>, hierarchical variants <code>energy_repset.combi_gens</code> Evaluation <code>ObjectiveSet</code> with <code>ScoreComponent</code> instances <code>energy_repset.objectives</code> Decision <code>WeightedSumPolicy</code>, <code>ParetoMaxMinStrategy</code>, <code>ParetoUtopiaPolicy</code> <code>energy_repset.selection_policies</code> <p>Examples: Ex1: Getting Started, Ex2: Feature Space, Ex3: Hierarchical Selection, Ex4: Representation Models, Ex5: Multi-Objective</p>"},{"location":"workflow/#workflow-2-constructive","title":"Workflow 2: Constructive","text":"<p>This approach builds a solution directly and iteratively, rather than testing pre-made combinations. It's generally much faster than the Generate-and-Test workflow.</p> <ul> <li>Status: Not yet implemented in the current software.</li> <li>Examples: K-Medoids, K-Means, Hierarchical Clustering, Hull Clustering.</li> <li>How the Modules are Used:</li> <li>SearchAlgorithm (A): This is the star of the show. It contains the entire complex logic of the method (e.g., the k-medoids algorithm). It has its own internal objective (e.g., minimize intra-cluster distance) that guides its construction process.</li> <li>ObjectiveSet (O): Not used during the search. Its role shifts to post-hoc evaluation. After the SearchAlgorithm has constructed a final solution, the ObjectiveSet is used to give it a standardized score. This is crucial for comparing its result against the results from other workflow types.</li> <li>SelectionPolicy (Pi): Bypassed entirely. The constructive nature of the algorithm is its own policy; the final result is the direct output of the construction process, with no separate decision step needed.</li> <li>Summary: The SearchAlgorithm does all the heavy lifting to directly build a solution. The ObjectiveSet is only used for final validation.</li> </ul> <p>Key Modules:</p> Role Implementation Status Search Constructive <code>SearchAlgorithm</code> implementations Planned Protocol <code>SearchAlgorithm</code> (structural typing) <code>energy_repset.search_algorithms</code> Evaluation <code>ObjectiveSet</code> (post-hoc only) <code>energy_repset.objectives</code> <p>Planned algorithms: Hull Clustering (greedy projection-error minimization), K-Medoids (PAM), Snippet Algorithm (MPdist-based multi-day selection).</p>"},{"location":"workflow/#workflow-3-direct-optimization","title":"Workflow 3: Direct Optimization","text":"<p>This is the most sophisticated workflow. It formulates the entire selection problem as a single, large-scale mathematical optimization problem and uses a dedicated solver to find the globally optimal solution.</p> <ul> <li>Status: Not yet implemented in the current software.</li> <li>Examples: Mixed-Integer Linear Programming (MILP) to select periods that best reconstruct an annual duration curve.</li> <li>How the Modules are Used:</li> <li>SearchAlgorithm (A): Its main job is to act as a bridge to a mathematical programming solver (e.g., Gurobi, HiGHS). It translates the user's problem into the strict mathematical language of the solver.</li> <li>ObjectiveSet (O): Its concepts (e.g., \"minimize NRMSE\") are translated into the formal mathematical objective function of the optimization model. It is deeply embedded within the SearchAlgorithm's formulation.</li> <li>SelectionPolicy (Pi): Bypassed entirely. The solver's goal is to find the single optimal solution that minimizes the mathematical objective. The decision is inherent in the optimization process.</li> <li>Summary: The problem is handed off to a powerful external solver. The main work is in the formulation of the problem within the SearchAlgorithm module.</li> </ul> <p>Key Modules:</p> Role Implementation Status Search MILP-based <code>SearchAlgorithm</code> Planned Protocol <code>SearchAlgorithm</code> (structural typing) <code>energy_repset.search_algorithms</code> Solver External (Gurobi, HiGHS, etc.) N/A <p>Planned algorithms: MILP-based duration curve reconstruction, MILP with temporal coupling constraints.</p>"},{"location":"workflow/#the-workflow-dataclass","title":"The Workflow Dataclass","text":"<p>All three workflow types share the same assembly pattern. The <code>Workflow</code> dataclass bundles the three runtime components -- a <code>FeatureEngineer</code>, a <code>SearchAlgorithm</code>, and a <code>RepresentationModel</code> -- into a single object that <code>RepSetExperiment</code> can execute:</p> <pre><code>import energy_repset as rep\n\nworkflow = rep.Workflow(\n    feature_engineer=feature_engineer,   # F: transforms raw data into features\n    search_algorithm=search_algorithm,   # A: finds optimal selection (uses O internally)\n    representation_model=representation_model,  # R: calculates weights\n)\n\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n</code></pre> <p>The <code>Workflow</code> is intentionally thin: it holds references to components, not logic. The orchestration logic lives in <code>RepSetExperiment.run()</code>, which calls the components in sequence. This keeps each component independently testable and swappable.</p> <p>For the full component catalog, see Modules &amp; Components.</p>"},{"location":"api/","title":"API Reference","text":"Module Description Context &amp; Slicing <code>ProblemContext</code> data container and <code>TimeSlicer</code> Workflow &amp; Experiment <code>Workflow</code>, <code>RepSetExperiment</code>, and <code>RepSetResult</code> Feature Engineering Feature engineer protocol and implementations Objectives <code>ObjectiveSet</code>, <code>ObjectiveSpec</code>, and <code>ScoreComponent</code> base Score Components All concrete score component implementations Combination Generators <code>CombinationGenerator</code> protocol and implementations Selection Policies <code>SelectionPolicy</code> protocol and implementations Search Algorithms <code>SearchAlgorithm</code> protocol and implementations Representation <code>RepresentationModel</code> protocol and implementations Diagnostics Visualization classes for feature space, scores, and results"},{"location":"api/combi_gens/","title":"Combination Generators","text":""},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator","title":"CombinationGenerator","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for generating and counting combinations of candidate slices.</p> <p>This protocol defines the interface for combination generators used in Generate-and-Test workflows. Implementations determine which k-element subsets of candidate slices should be evaluated.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>Number of elements in each combination to generate.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate k-combinations from the candidate slices.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples of length k representing candidate selections.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count the total number of combinations that will be generated.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of k-combinations.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if a combination is valid according to generator constraints.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the combination satisfies the generator's constraints.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen","title":"ExhaustiveCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate all k-combinations of the candidate slices.</p> <p>This generator produces every possible k-element subset using itertools.combinations. It is suitable for small problem sizes where the total number of combinations (n choose k) is computationally feasible.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of elements in each combination.</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of elements per combination.</p> Note <p>The count is computed via binomial coefficient (n choose k) and matches the number of yielded combinations exactly.</p> <p>Examples:</p> <p>Generate all 3-month combinations from a year:</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; months = [pd.Period('2024-01', 'M'), pd.Period('2024-02', 'M'),\n...           pd.Period('2024-03', 'M'), pd.Period('2024-04', 'M')]\n&gt;&gt;&gt; generator = ExhaustiveCombiGen(k=3)\n&gt;&gt;&gt; generator.count(months)  # 4 choose 3\n    4\n&gt;&gt;&gt; list(generator.generate(months))\n    [(Period('2024-01', 'M'), Period('2024-02', 'M'), Period('2024-03', 'M')),\n     (Period('2024-01', 'M'), Period('2024-02', 'M'), Period('2024-04', 'M')),\n     (Period('2024-01', 'M'), Period('2024-03', 'M'), Period('2024-04', 'M')),\n     (Period('2024-02', 'M'), Period('2024-03', 'M'), Period('2024-04', 'M'))]\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(k: int) -&gt; None\n</code></pre> <p>Initialize exhaustive generator with target combination size.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of elements in each combination.</p> required"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate all k-combinations using itertools.combinations.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>All possible k-element tuples from unique_slices.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total combinations using binomial coefficient.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>n choose k, where n is the number of unique slices.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination has exactly k elements from unique_slices.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination has k elements all in unique_slices.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen","title":"GroupQuotaCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate combinations that respect exact quotas per group.</p> <p>This generator enforces that selections contain a specific number of elements from each group. It is useful for ensuring balanced representation across categories (e.g., seasons, must-have periods, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Total number of elements in each combination. Must equal sum of group quotas.</p> required <code>slice_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from each candidate slice to its group label.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Mapping from group label to the required count in the selection.</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of elements per combination.</p> <code>group_of</code> <p>Mapping from slice to group.</p> <code>group_quota</code> <p>Required count per group.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal k.</p> Note <p>Use this to enforce constraints like \"exactly one month per season\" or \"2 must-have periods plus 2 optional periods\".</p> <p>Examples:</p> <p>Example 1 - Seasonal constraints (one month per season):</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define months and their seasons\n&gt;&gt;&gt; months = [pd.Period(f'2024-{i:02d}', 'M') for i in range(1, 13)]\n&gt;&gt;&gt; season_map = {}\n&gt;&gt;&gt; for month in months:\n...     if month.month in [12, 1, 2]: season_map[month] = 'winter'\n...     elif month.month in [3, 4, 5]: season_map[month] = 'spring'\n...     elif month.month in [6, 7, 8]: season_map[month] = 'summer'\n...     else: season_map[month] = 'fall'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Select 4 months, one per season\n&gt;&gt;&gt; generator = GroupQuotaCombiGen(\n...     k=4,\n...     slice_to_group_mapping=season_map,\n...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n... )\n&gt;&gt;&gt; generator.count(months)  # 3 * 3 * 3 * 3 = 81 combinations\n    81\n</code></pre> <p>Example 2 - Optional and must-have categories:</p> <pre><code>&gt;&gt;&gt; # Force specific periods to be included\n&gt;&gt;&gt; months = [pd.Period(f'2024-{i:02d}', 'M') for i in range(1, 13)]\n&gt;&gt;&gt; group_mapping = {p: 'optional' for p in months}\n&gt;&gt;&gt; group_mapping[pd.Period('2024-01', 'M')] = 'must'\n&gt;&gt;&gt; group_mapping[pd.Period('2024-12', 'M')] = 'must'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Select 4 total: 2 must-have + 2 optional\n&gt;&gt;&gt; generator = GroupQuotaCombiGen(\n...     k=4,\n...     slice_to_group_mapping=group_mapping,\n...     group_quota={'optional': 2, 'must': 2}\n... )\n&gt;&gt;&gt; # All combinations will include Jan and Dec plus 2 from the other 10\n&gt;&gt;&gt; generator.count(months)  # 1 * 45 = 45 combinations\n45\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(k: int, slice_to_group_mapping: dict[Hashable, Hashable], group_quota: dict[Hashable, int]) -&gt; None\n</code></pre> <p>Initialize generator with group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Total number of elements in each combination.</p> required <code>slice_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from slice to its group label.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count per group.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal k.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate combinations respecting group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples of length k where each group contributes exactly its quota.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total combinations respecting group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Product of binomial coefficients across all groups. For each group</p> <code>int</code> <p>with n members and quota q, contributes C(n, q) to the product.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination satisfies group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination has exactly k elements with each group contributing</p> <code>bool</code> <p>its required quota.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen","title":"ExhaustiveHierarchicalCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate combinations where child slices are selected in complete parent groups.</p> <p>This generator enforces hierarchical selection: child slices (e.g., days) can only be selected as complete parent groups (e.g., months). It enables high-resolution features (e.g. per-day) while enforcing structural constraints at the parent level (e.g. months).</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Number of parent groups to select.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from each child slice to its parent group. Example: {Period('2024-01-01', 'D'): Period('2024-01', 'M'), ...}</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of parent groups per combination (same as parent_k for protocol compliance).</p> <code>parent_k</code> <p>Number of parent groups per combination.</p> <code>slice_to_parent</code> <p>Child to parent mapping.</p> Note <p>The <code>generate()</code> method yields flattened tuples of child slices, but internally enforces parent-level constraints. Use the factory method <code>from_slicers()</code> for automatic parent grouping based on TimeSlicer objects.</p> <p>Examples:</p> <p>Manual construction with custom grouping:</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define child-to-parent mapping\n&gt;&gt;&gt; slice_to_parent = {\n...     pd.Period('2024-01-01', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-01-02', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-02-01', 'D'): pd.Period('2024-02', 'M'),\n...     pd.Period('2024-02-02', 'D'): pd.Period('2024-02', 'M'),\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Select 2 months, but combinations contain days\n&gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen(\n...     parent_k=2,\n...     slice_to_parent_mapping=slice_to_parent\n... )\n&gt;&gt;&gt; gen.count(list(slice_to_parent.keys()))  # C(2, 2) = 1\n    1\n</code></pre> <p>Using factory method with TimeSlicer:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n...     parent_k=3,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     parent_slicer=parent_slicer\n... )\n&gt;&gt;&gt; unique_days = child_slicer.unique_slices(dates)\n&gt;&gt;&gt; gen.count(unique_days)  # C(12, 3) = 220 combinations of months\n    220\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(parent_k: int, slice_to_parent_mapping: dict[Hashable, Hashable]) -&gt; None\n</code></pre> <p>Initialize hierarchical generator with child-to-parent mapping.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Number of parent groups to select.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Dict mapping each child slice to its parent.</p> required"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.from_slicers","title":"from_slicers  <code>classmethod</code>","text":"<pre><code>from_slicers(parent_k: int, dt_index: DatetimeIndex, child_slicer: TimeSlicer, parent_slicer: TimeSlicer) -&gt; ExhaustiveHierarchicalCombiGen\n</code></pre> <p>Factory method to create generator from child and parent TimeSlicer objects.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Number of parent groups to select.</p> required <code>dt_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex of the time series data.</p> required <code>child_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining child slice granularity (e.g., daily).</p> required <code>parent_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining parent slice granularity (e.g., monthly).</p> required <p>Returns:</p> Type Description <code>ExhaustiveHierarchicalCombiGen</code> <p>ExhaustiveHierarchicalCombinationGenerator with auto-constructed mappings.</p> <p>Examples:</p> <p>Select 4 months from a year of daily data:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n...     parent_k=4,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     parent_slicer=parent_slicer\n... )\n&gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # C(12, 4) = 495\n495\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate combinations of k parent groups, yielding flattened child slices.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples containing all child slices from k selected parent groups.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total number of parent-level combinations.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>C(n_parents, parent_k) where n_parents is the number of unique parent groups.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination represents exactly parent_k complete parent groups.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of child slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of all valid child slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination contains all children from exactly parent_k parent groups.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen","title":"GroupQuotaHierarchicalCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate combinations respecting quotas per parent-level group.</p> <p>This generator combines hierarchical selection (child slices selected in complete parent groups) with group quotas (e.g., exactly 1 month per season). It enables high-resolution features (e.g. per-day) while enforcing structural constraints at the parent level (e.g. months).</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select. Must equal sum of group quotas.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from each child slice to its parent group.</p> required <code>parent_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from parent ID to its group label (e.g., season).</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count of parents per group.</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of parent groups per combination (same as parent_k for protocol compliance).</p> <code>slice_to_parent</code> <p>Child to parent mapping.</p> <code>parent_to_group</code> <p>Parent to group label mapping.</p> <code>group_quota</code> <p>Required count per group.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal parent_k.</p> Note <p>Use factory methods <code>from_slicers()</code> for automatic parent mapping and <code>from_slicers_with_seasons()</code> for automatic seasonal grouping.</p> <p>Examples:</p> <p>Manual construction for seasonal month selection:</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; slice_to_parent = {\n...     pd.Period('2024-01-01', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-01-02', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-07-01', 'D'): pd.Period('2024-07', 'M'),\n...     pd.Period('2024-07-02', 'D'): pd.Period('2024-07', 'M'),\n... }\n&gt;&gt;&gt; parent_to_group = {\n...     pd.Period('2024-01', 'M'): 'winter',\n...     pd.Period('2024-07', 'M'): 'summer',\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen(\n...     parent_k=2,\n...     slice_to_parent_mapping=slice_to_parent,\n...     parent_to_group_mapping=parent_to_group,\n...     group_quota={'winter': 1, 'summer': 1}\n... )\n&gt;&gt;&gt; gen.count(list(slice_to_parent.keys()))  # 1 * 1 = 1\n    1\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(parent_k: int, slice_to_parent_mapping: dict[Hashable, Hashable], parent_to_group_mapping: dict[Hashable, Hashable], group_quota: dict[Hashable, int]) -&gt; None\n</code></pre> <p>Initialize hierarchical quota generator.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Dict mapping each child slice to its parent.</p> required <code>parent_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from parent ID to group label.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count per group.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal parent_k, or if any parent in slice_to_parent_mapping is missing from parent_to_group_mapping.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.from_slicers","title":"from_slicers  <code>classmethod</code>","text":"<pre><code>from_slicers(parent_k: int, dt_index: DatetimeIndex, child_slicer: TimeSlicer, parent_slicer: TimeSlicer, parent_to_group_mapping: dict[Hashable, Hashable], group_quota: dict[Hashable, int]) -&gt; GroupQuotaHierarchicalCombiGen\n</code></pre> <p>Factory method to create generator from slicers with custom group mapping.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select.</p> required <code>dt_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex of the time series data.</p> required <code>child_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining child slice granularity (e.g., daily).</p> required <code>parent_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining parent slice granularity (e.g., monthly).</p> required <code>parent_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Dict mapping parent IDs to group labels.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count per group.</p> required <p>Returns:</p> Type Description <code>GroupQuotaHierarchicalCombiGen</code> <p>GroupQuotaHierarchicalCombinationGenerator with auto-constructed child-to-parent mapping.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If quotas invalid.</p> <p>Examples:</p> <p>Custom grouping of months into seasons:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt;\n&gt;&gt;&gt; parent_to_group = {\n...     pd.Period('2024-01', 'M'): 'winter',\n...     pd.Period('2024-02', 'M'): 'winter',\n...     # ... define for all 12 months\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers(\n...     parent_k=4,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     parent_slicer=parent_slicer,\n...     parent_to_group_mapping=parent_to_group,\n...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n... )\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons","title":"from_slicers_with_seasons  <code>classmethod</code>","text":"<pre><code>from_slicers_with_seasons(parent_k: int, dt_index: DatetimeIndex, child_slicer: TimeSlicer, group_quota: dict[Literal['winter', 'spring', 'summer', 'fall'], int]) -&gt; GroupQuotaHierarchicalCombiGen\n</code></pre> <p>Factory method with automatic seasonal grouping of months.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select (must equal sum of quotas).</p> required <code>dt_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex of the time series data.</p> required <code>child_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining child slice granularity (e.g., daily).</p> required <code>group_quota</code> <code>dict[Literal['winter', 'spring', 'summer', 'fall'], int]</code> <p>Required count per season. Keys must be subset of {'winter', 'spring', 'summer', 'fall'}.</p> required <p>Returns:</p> Type Description <code>GroupQuotaHierarchicalCombiGen</code> <p>GroupQuotaHierarchicalCombinationGenerator with seasonal parent grouping.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If quotas invalid.</p> Note <p>This factory method uses monthly parents regardless of child slicer. Seasons are assigned as: winter (Dec/Jan/Feb), spring (Mar/Apr/May), summer (Jun/Jul/Aug), fall (Sep/Oct/Nov).</p> <p>Examples:</p> <p>Select 4 months (1 per season) from daily data:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n...     parent_k=4,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n... )\n&gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # 3 * 3 * 3 * 3 = 81\n    81\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate combinations respecting group quotas, yielding flattened child slices.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples containing all child slices from parent_k parent groups satisfying quotas.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total combinations respecting group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Product of C(n_parents_in_group, quota) across all groups.</p>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination satisfies group quotas and completeness.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of child slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of all valid child slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination contains complete parent groups satisfying quotas.</p>"},{"location":"api/context/","title":"Context &amp; Slicing","text":""},{"location":"api/context/#energy_repset.context.ProblemContext","title":"ProblemContext","text":"<p>A data container passed through the entire workflow.</p> <p>This class holds all data and metadata needed for representative subset selection. It is the central object passed between workflow stages (feature engineering, search algorithms, representation models).</p> <p>Parameters:</p> Name Type Description Default <code>df_raw</code> <code>DataFrame</code> <p>Raw time-series data with datetime index and variable columns.</p> required <code>slicer</code> <code>'TimeSlicer'</code> <p>TimeSlicer defining how the time index is divided into candidate periods.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dict for storing arbitrary user data (e.g., default weights, experiment configuration, notes, etc.). Not used by the framework itself, but available for user convenience and custom component implementations.</p> <code>None</code> <p>Examples:</p> <p>Create a context with monthly slicing:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.context import ProblemContext\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'demand': np.random.rand(8760),\n...     'solar': np.random.rand(8760)\n... }, index=dates)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create context with metadata\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; context = ProblemContext(\n...     df_raw=df,\n...     slicer=slicer,\n...     metadata={\n...         'experiment_name': 'test_run_1',\n...         'default_weights': {'demand': 1.5, 'solar': 1.0},\n...         'notes': 'Testing seasonal selection'\n...     }\n... )\n&gt;&gt;&gt; len(context.get_unique_slices())  # 12 months\n    12\n&gt;&gt;&gt; context.metadata['experiment_name']  # 'test_run_1'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create context without metadata\n&gt;&gt;&gt; context2 = ProblemContext(df_raw=df, slicer=slicer)\n&gt;&gt;&gt; context2.metadata  # {}\n</code></pre>"},{"location":"api/context/#energy_repset.context.ProblemContext.df_features","title":"df_features  <code>property</code> <code>writable</code>","text":"<pre><code>df_features: DataFrame\n</code></pre> <p>Get the computed feature matrix.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with slice labels as index and engineered features as columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If features have not been computed yet. Use a FeatureEngineer to populate this field first.</p>"},{"location":"api/context/#energy_repset.context.ProblemContext.__init__","title":"__init__","text":"<pre><code>__init__(df_raw: DataFrame, slicer: 'TimeSlicer', metadata: dict[str, Any] | None = None)\n</code></pre> <p>Initialize a ProblemContext.</p> <p>Parameters:</p> Name Type Description Default <code>df_raw</code> <code>DataFrame</code> <p>Raw time-series data with datetime index and variable columns.</p> required <code>slicer</code> <code>'TimeSlicer'</code> <p>TimeSlicer defining how the time index is divided into candidate periods.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dict for storing arbitrary user data. Not used by the framework itself.</p> <code>None</code>"},{"location":"api/context/#energy_repset.context.ProblemContext.copy","title":"copy","text":"<pre><code>copy() -&gt; 'ProblemContext'\n</code></pre> <p>Create a deep copy of this ProblemContext instance.</p> <p>Returns:</p> Type Description <code>'ProblemContext'</code> <p>A new, independent instance of the context with all data copied.</p>"},{"location":"api/context/#energy_repset.context.ProblemContext.get_sliced_data","title":"get_sliced_data","text":"<pre><code>get_sliced_data() -&gt; dict[Hashable, DataFrame]\n</code></pre> <p>Generate sliced raw data on demand.</p> <p>Returns:</p> Type Description <code>dict[Hashable, DataFrame]</code> <p>Dictionary mapping slice labels to their corresponding DataFrame chunks.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not yet implemented.</p>"},{"location":"api/context/#energy_repset.context.ProblemContext.get_unique_slices","title":"get_unique_slices","text":"<pre><code>get_unique_slices() -&gt; list[Hashable]\n</code></pre> <p>Get list of all unique slice labels from the time index.</p> <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>List of slice labels (e.g., Period objects for monthly slicing).</p>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer","title":"TimeSlicer","text":"<p>Convert a DatetimeIndex into labeled time slices.</p> <p>This class defines how the time index is divided into candidate periods for representative subset selection. It converts timestamps into Period objects or floored timestamps based on the specified temporal granularity.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>SliceUnit</code> <p>Temporal granularity of the slices. One of \"year\", \"month\", \"week\", \"day\", or \"hour\".</p> required <p>Attributes:</p> Name Type Description <code>unit</code> <p>The temporal granularity used for slicing.</p> Note <p>The labels are hashable and suitable for set membership and grouping. Period objects are used for year, month, week, and day. Naive timestamps (floored to hour) are used for hourly slicing.</p> <p>Examples:</p> <p>Create a slicer for monthly periods:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; labels = slicer.labels_for_index(dates)\n&gt;&gt;&gt; unique_months = slicer.unique_slices(dates)\n&gt;&gt;&gt; len(unique_months)  # 12 months in a year\n    12\n&gt;&gt;&gt; unique_months[0]  # First month\n    Period('2024-01', 'M')\n</code></pre> <p>Weekly slicing:</p> <pre><code>&gt;&gt;&gt; slicer = TimeSlicer(unit='week')\n&gt;&gt;&gt; unique_weeks = slicer.unique_slices(dates)\n&gt;&gt;&gt; len(unique_weeks)  # ~52 weeks in a year\n    53\n</code></pre>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.__init__","title":"__init__","text":"<pre><code>__init__(unit: SliceUnit) -&gt; None\n</code></pre> <p>Initialize TimeSlicer with specified temporal granularity.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>SliceUnit</code> <p>One of \"year\", \"month\", \"week\", \"day\", or \"hour\".</p> required"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.labels_for_index","title":"labels_for_index","text":"<pre><code>labels_for_index(index: DatetimeIndex) -&gt; Index\n</code></pre> <p>Return a vector of slice labels aligned to the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>DatetimeIndex</code> <p>DatetimeIndex for the input data.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>Index of slice labels matching the input index length. Each timestamp</p> <code>Index</code> <p>is mapped to its corresponding period or floored hour.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unit is not one of the supported values.</p>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.unique_slices","title":"unique_slices","text":"<pre><code>unique_slices(index: DatetimeIndex) -&gt; list[Hashable]\n</code></pre> <p>Return the sorted list of unique slice labels present in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>DatetimeIndex</code> <p>DatetimeIndex for the input data.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>Sorted list of unique slice labels. The sort order follows the natural</p> <code>list[Hashable]</code> <p>ordering of Period objects or timestamps.</p>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.get_indices_for_slice_combi","title":"get_indices_for_slice_combi","text":"<pre><code>get_indices_for_slice_combi(index: DatetimeIndex, selection: Hashable | SliceCombination) -&gt; Index\n</code></pre> <p>Return the index positions for timestamps belonging to the given slice(s).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>DatetimeIndex</code> <p>DatetimeIndex for the input data.</p> required <code>selection</code> <code>Hashable | SliceCombination</code> <p>Either a single slice label or a tuple of slice labels (SliceCombination) to extract indices for.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>Index of timestamps that belong to the specified slice(s). If selection</p> <code>Index</code> <p>is a tuple, returns the union of all timestamps from all slices.</p> <p>Examples:</p> <p>Get indices for a single month:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; jan_slice = slicer.unique_slices(dates)[0]  # Period('2024-01', 'M')\n&gt;&gt;&gt; jan_indices = slicer.get_indices_for_slice_combi(dates, jan_slice)\n&gt;&gt;&gt; len(jan_indices)  # 744 hours in January 2024\n    744\n</code></pre> <p>Get indices for multiple months (selection):</p> <pre><code>&gt;&gt;&gt; selection = (Period('2024-01', 'M'), Period('2024-06', 'M'))\n&gt;&gt;&gt; selected_indices = slicer.get_indices_for_slice_combi(dates, selection)\n&gt;&gt;&gt; len(selected_indices)  # Jan (744) + Jun (720) = 1464\n    1464\n</code></pre>"},{"location":"api/diagnostics/","title":"Diagnostics","text":""},{"location":"api/diagnostics/#feature-space","title":"Feature Space","text":""},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter2D","title":"FeatureSpaceScatter2D","text":"<p>2D scatter plot for visualizing feature space.</p> <p>Creates an interactive scatter plot of any two features from df_features. Can highlight a specific selection of slices. Works with any feature columns including PCA components ('pc_0', 'pc_1'), statistical features ('mean__wind'), or mixed features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize PCA space\n&gt;&gt;&gt; scatter = FeatureSpaceScatter2D()\n&gt;&gt;&gt; fig = scatter.plot(context.df_features, x='pc_0', y='pc_1')\n&gt;&gt;&gt; fig.update_layout(title='PCA Feature Space')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Visualize with selection highlighted\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='mean__demand',\n...     y='pc_0',\n...     selection=('2024-01', '2024-04', '2024-07')\n... )\n\n&gt;&gt;&gt; # Color by another feature\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     color='std__wind'\n... )\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter2D.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the scatter plot diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter2D.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, x: str, y: str, selection: SliceCombination = None, color: str = None) -&gt; Figure\n</code></pre> <p>Create a 2D scatter plot of feature space.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>x</code> <code>str</code> <p>Column name for x-axis.</p> required <code>y</code> <code>str</code> <p>Column name for y-axis.</p> required <code>selection</code> <code>SliceCombination</code> <p>Optional tuple of slice identifiers to highlight.</p> <code>None</code> <code>color</code> <code>str</code> <p>Optional column name to use for color mapping.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If x, y, or color columns are not in df_features.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter3D","title":"FeatureSpaceScatter3D","text":"<p>3D scatter plot for visualizing feature space.</p> <p>Creates an interactive 3D scatter plot of any three features from df_features. Can highlight a specific selection of slices. Works with any feature columns including PCA components or statistical features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize 3D PCA space\n&gt;&gt;&gt; scatter = FeatureSpaceScatter3D()\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     z='pc_2'\n... )\n&gt;&gt;&gt; fig.update_layout(title='3D PCA Space')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Highlight selection\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     z='pc_2',\n...     selection=('2024-01', '2024-04')\n... )\n\n&gt;&gt;&gt; # Color by feature value\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     z='pc_2',\n...     color='mean__demand'\n... )\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter3D.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the 3D scatter plot diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter3D.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, x: str, y: str, z: str, selection: SliceCombination = None, color: str = None) -&gt; Figure\n</code></pre> <p>Create a 3D scatter plot of feature space.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>x</code> <code>str</code> <p>Column name for x-axis.</p> required <code>y</code> <code>str</code> <p>Column name for y-axis.</p> required <code>z</code> <code>str</code> <p>Column name for z-axis.</p> required <code>selection</code> <code>SliceCombination</code> <p>Optional tuple of slice identifiers to highlight.</p> <code>None</code> <code>color</code> <code>str</code> <p>Optional column name to use for color mapping.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If x, y, z, or color columns are not in df_features.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatterMatrix","title":"FeatureSpaceScatterMatrix","text":"<p>Scatter matrix (SPLOM) for visualizing relationships between multiple features.</p> <p>Creates an interactive scatter plot matrix showing pairwise relationships between all specified features. Can highlight a specific selection of slices. Useful for exploring multi-dimensional feature spaces and identifying feature correlations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize PCA components\n&gt;&gt;&gt; scatter_matrix = FeatureSpaceScatterMatrix()\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     context.df_features,\n...     dimensions=['pc_0', 'pc_1', 'pc_2']\n... )\n&gt;&gt;&gt; fig.update_layout(title='PCA Component Relationships')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Visualize statistical features with selection\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     context.df_features,\n...     dimensions=['mean__demand', 'std__demand', 'max__wind'],\n...     selection=('2024-01', '2024-04', '2024-07')\n... )\n\n&gt;&gt;&gt; # Color by a feature value\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     context.df_features,\n...     dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3'],\n...     color='mean__demand'\n... )\n\n&gt;&gt;&gt; # All features\n&gt;&gt;&gt; fig = scatter_matrix.plot(context.df_features)\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatterMatrix.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the scatter matrix diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatterMatrix.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, dimensions: list[str] = None, selection: SliceCombination = None, color: str = None) -&gt; Figure\n</code></pre> <p>Create a scatter plot matrix of feature space.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>dimensions</code> <code>list[str]</code> <p>List of column names to include in the matrix. If None, uses all columns (may be slow for many features).</p> <code>None</code> <code>selection</code> <code>SliceCombination</code> <p>Optional tuple of slice identifiers to highlight.</p> <code>None</code> <code>color</code> <code>str</code> <p>Optional column name to use for color mapping. If None and selection is provided, colors by selection status.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If any dimension or color column is not in df_features.</p> <code>ValueError</code> <p>If dimensions list is empty.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.PCAVarianceExplained","title":"PCAVarianceExplained","text":"<p>Visualize explained variance ratio for PCA components.</p> <p>Creates a bar chart showing the proportion of variance explained by each principal component, along with cumulative variance. Helps determine how many components are needed to capture most of the data's variance.</p> <p>This diagnostic requires the fitted PCAFeatureEngineer instance to access the explained_variance_ratio_ attribute.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get PCA engineer from pipeline\n&gt;&gt;&gt; pca_engineer = pipeline.engineers['pca']\n&gt;&gt;&gt; variance_plot = PCAVarianceExplained(pca_engineer)\n&gt;&gt;&gt; fig = variance_plot.plot()\n&gt;&gt;&gt; fig.update_layout(title='PCA Variance Explained')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # With custom number of components shown\n&gt;&gt;&gt; fig = variance_plot.plot(n_components=10)\n\n&gt;&gt;&gt; # After running workflow\n&gt;&gt;&gt; context_with_features = workflow.feature_engineer.run(context)\n&gt;&gt;&gt; pca_eng = workflow.feature_engineer.engineers['pca']\n&gt;&gt;&gt; variance_plot = PCAVarianceExplained(pca_eng)\n&gt;&gt;&gt; fig = variance_plot.plot()\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.PCAVarianceExplained.__init__","title":"__init__","text":"<pre><code>__init__(pca_engineer: PCAFeatureEngineer)\n</code></pre> <p>Initialize the PCA variance explained diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>pca_engineer</code> <code>PCAFeatureEngineer</code> <p>A fitted PCAFeatureEngineer instance. Must have been fitted on data (i.e., calc_and_get_features_df has been called).</p> required"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.PCAVarianceExplained.plot","title":"plot","text":"<pre><code>plot(n_components: int = None, show_cumulative: bool = True) -&gt; Figure\n</code></pre> <p>Create a bar chart of explained variance ratios.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of components to show. If None, shows all components.</p> <code>None</code> <code>show_cumulative</code> <code>bool</code> <p>If True, adds a line showing cumulative variance explained.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the PCA engineer has not been fitted yet.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureCorrelationHeatmap","title":"FeatureCorrelationHeatmap","text":"<p>Visualize correlation matrix of features.</p> <p>Creates an interactive heatmap showing Pearson correlations between all features in the feature matrix. Helps identify redundant features and understand feature relationships. Can optionally show only the lower triangle to avoid redundancy.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize all feature correlations\n&gt;&gt;&gt; heatmap = FeatureCorrelationHeatmap()\n&gt;&gt;&gt; fig = heatmap.plot(context.df_features)\n&gt;&gt;&gt; fig.update_layout(title='Feature Correlation Matrix')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Show only lower triangle\n&gt;&gt;&gt; fig = heatmap.plot(context.df_features, show_lower_only=True)\n\n&gt;&gt;&gt; # Subset of features\n&gt;&gt;&gt; selected_features = context.df_features[['pc_0', 'pc_1', 'mean__demand']]\n&gt;&gt;&gt; fig = heatmap.plot(selected_features)\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureCorrelationHeatmap.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the feature correlation heatmap diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureCorrelationHeatmap.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, method: str = 'pearson', show_lower_only: bool = False) -&gt; Figure\n</code></pre> <p>Create a heatmap of feature correlations.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>method</code> <code>str</code> <p>Correlation method ('pearson', 'spearman', or 'kendall'). Default is 'pearson'.</p> <code>'pearson'</code> <code>show_lower_only</code> <code>bool</code> <p>If True, shows only the lower triangle of the correlation matrix (removes redundant upper triangle and diagonal).</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is not one of the supported correlation methods.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureDistributions","title":"FeatureDistributions","text":"<p>Visualize distributions of all features as histograms.</p> <p>Creates a grid of histograms showing the distribution of each feature across all slices. Helps identify feature scales, skewness, and potential outliers. Useful for understanding the feature space before selection.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize all feature distributions\n&gt;&gt;&gt; dist_plot = FeatureDistributions()\n&gt;&gt;&gt; fig = dist_plot.plot(context.df_features)\n&gt;&gt;&gt; fig.update_layout(title='Feature Distributions')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Subset of features\n&gt;&gt;&gt; selected_features = context.df_features[['pc_0', 'pc_1', 'mean__demand']]\n&gt;&gt;&gt; fig = dist_plot.plot(selected_features)\n\n&gt;&gt;&gt; # With custom bin count\n&gt;&gt;&gt; fig = dist_plot.plot(context.df_features, nbins=30)\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureDistributions.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the feature distributions diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureDistributions.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, nbins: int = 20, cols: int = 3) -&gt; Figure\n</code></pre> <p>Create a grid of histograms for all features.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>nbins</code> <code>int</code> <p>Number of bins for each histogram. Default is 20.</p> <code>20</code> <code>cols</code> <code>int</code> <p>Number of columns in the subplot grid. Default is 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If df_features is empty or nbins/cols are invalid.</p>"},{"location":"api/diagnostics/#score-components","title":"Score Components","text":""},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayECDF","title":"DistributionOverlayECDF","text":"<p>Overlay empirical cumulative distribution functions (ECDF) to compare distributions.</p> <p>Creates a plot showing the ECDF of a variable for both the full dataset and a selection. This helps visualize how well the selection represents the full distribution, which is what WassersteinFidelity measures.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare demand distribution\n&gt;&gt;&gt; ecdf_plot = DistributionOverlayECDF()\n&gt;&gt;&gt; full_data = context.df_raw['demand']\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, 'demand']\n&gt;&gt;&gt; fig = ecdf_plot.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Demand Distribution: Full vs Selected')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Alternative: using iloc\n&gt;&gt;&gt; selection_mask = context.df_raw.index.isin(selected_indices)\n&gt;&gt;&gt; fig = ecdf_plot.plot(\n...     context.df_raw['wind'],\n...     context.df_raw.loc[selection_mask, 'wind']\n... )\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayECDF.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the ECDF overlay diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayECDF.plot","title":"plot","text":"<pre><code>plot(df_full: Series, df_selection: Series, full_label: str = 'Full', selection_label: str = 'Selection') -&gt; Figure\n</code></pre> <p>Create an ECDF overlay plot.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>Series</code> <p>Series containing values for the full dataset.</p> required <code>df_selection</code> <code>Series</code> <p>Series containing values for the selection.</p> required <code>full_label</code> <code>str</code> <p>Label for the full dataset in the legend. Default 'Full'.</p> <code>'Full'</code> <code>selection_label</code> <code>str</code> <p>Label for the selection in the legend. Default 'Selection'.</p> <code>'Selection'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayHistogram","title":"DistributionOverlayHistogram","text":"<p>Overlay histograms to compare distributions.</p> <p>Creates a plot showing normalized histograms of a variable for both the full dataset and a selection. Alternative to ECDF that may be more intuitive for some users. Shows probability density rather than cumulative probability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare demand distribution\n&gt;&gt;&gt; hist_plot = DistributionOverlayHistogram()\n&gt;&gt;&gt; full_data = context.df_raw['demand']\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, 'demand']\n&gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Demand Distribution: Full vs Selected')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # With custom bin count\n&gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data, nbins=50)\n\n&gt;&gt;&gt; # Using density mode\n&gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data, histnorm='probability density')\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayHistogram.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the histogram overlay diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayHistogram.plot","title":"plot","text":"<pre><code>plot(df_full: Series, df_selection: Series, nbins: int = 30, histnorm: str = 'probability', full_label: str = 'Full', selection_label: str = 'Selection') -&gt; Figure\n</code></pre> <p>Create a histogram overlay plot.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>Series</code> <p>Series containing values for the full dataset.</p> required <code>df_selection</code> <code>Series</code> <p>Series containing values for the selection.</p> required <code>nbins</code> <code>int</code> <p>Number of bins for the histogram. Default is 30.</p> <code>30</code> <code>histnorm</code> <code>str</code> <p>Histogram normalization mode. Options: 'probability', 'probability density', 'percent'. Default is 'probability'.</p> <code>'probability'</code> <code>full_label</code> <code>str</code> <p>Label for the full dataset in the legend. Default 'Full'.</p> <code>'Full'</code> <code>selection_label</code> <code>str</code> <p>Label for the selection in the legend. Default 'Selection'.</p> <code>'Selection'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If histnorm is not a valid option.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.CorrelationDifferenceHeatmap","title":"CorrelationDifferenceHeatmap","text":"<p>Visualize the difference between correlation matrices.</p> <p>Creates a heatmap showing the difference between the correlation matrix of the full dataset and the selection. This helps identify which variable relationships are well-preserved or poorly-preserved by the selection. Related to CorrelationFidelity score component.</p> <p>Positive values (red) indicate the selection has stronger correlation than the full dataset. Negative values (blue) indicate weaker correlation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare correlation structure\n&gt;&gt;&gt; corr_diff = CorrelationDifferenceHeatmap()\n&gt;&gt;&gt; full_data = context.df_raw[['demand', 'wind', 'solar']]\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, ['demand', 'wind', 'solar']]\n&gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Correlation Difference: Selection - Full')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # With Spearman correlation\n&gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data, method='spearman')\n\n&gt;&gt;&gt; # Show only lower triangle\n&gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data, show_lower_only=True)\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.CorrelationDifferenceHeatmap.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the correlation difference heatmap diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.CorrelationDifferenceHeatmap.plot","title":"plot","text":"<pre><code>plot(df_full: DataFrame, df_selection: DataFrame, method: str = 'pearson', show_lower_only: bool = False) -&gt; Figure\n</code></pre> <p>Create a heatmap of correlation differences.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>DataFrame</code> <p>DataFrame containing variables for the full dataset.</p> required <code>df_selection</code> <code>DataFrame</code> <p>DataFrame containing variables for the selection. Must have the same columns as df_full.</p> required <code>method</code> <code>str</code> <p>Correlation method ('pearson', 'spearman', or 'kendall'). Default is 'pearson'.</p> <code>'pearson'</code> <code>show_lower_only</code> <code>bool</code> <p>If True, shows only the lower triangle of the difference matrix (removes redundant upper triangle and diagonal).</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is invalid or columns don't match.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DiurnalProfileOverlay","title":"DiurnalProfileOverlay","text":"<p>Overlay mean diurnal (hour-of-day) profiles for full vs selected data.</p> <p>Creates a plot showing the average value by hour of day for each variable, comparing the full dataset to the selection. This helps visualize how well the selection preserves daily patterns, which is related to DiurnalFidelity score component.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare diurnal patterns\n&gt;&gt;&gt; diurnal_plot = DiurnalProfileOverlay()\n&gt;&gt;&gt; full_data = context.df_raw[['demand', 'wind', 'solar']]\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, ['demand', 'wind', 'solar']]\n&gt;&gt;&gt; fig = diurnal_plot.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Diurnal Profiles: Full vs Selected')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Single variable\n&gt;&gt;&gt; fig = diurnal_plot.plot(\n...     full_data[['demand']],\n...     selected_data[['demand']]\n... )\n\n&gt;&gt;&gt; # Subset of variables\n&gt;&gt;&gt; fig = diurnal_plot.plot(\n...     full_data,\n...     selected_data,\n...     variables=['demand', 'wind']\n... )\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DiurnalProfileOverlay.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the diurnal profile overlay diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DiurnalProfileOverlay.plot","title":"plot","text":"<pre><code>plot(df_full: DataFrame, df_selection: DataFrame, variables: list[str] = None, full_label: str = 'Full', selection_label: str = 'Selection') -&gt; Figure\n</code></pre> <p>Create a diurnal profile overlay plot.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex and variable columns for full dataset.</p> required <code>df_selection</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex and variable columns for selection. Must have the same columns as df_full.</p> required <code>variables</code> <code>list[str]</code> <p>List of variable names to include. If None, uses all columns.</p> <code>None</code> <code>full_label</code> <code>str</code> <p>Label suffix for full dataset traces. Default 'Full'.</p> <code>'Full'</code> <code>selection_label</code> <code>str</code> <p>Label suffix for selection traces. Default 'Selection'.</p> <code>'Selection'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DataFrames don't have DatetimeIndex or columns don't match.</p>"},{"location":"api/diagnostics/#results","title":"Results","text":""},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ResponsibilityBars","title":"ResponsibilityBars","text":"<p>Bar chart showing responsibility weights for selected representatives.</p> <p>Visualizes the weight distribution across selected periods as computed by a RepresentationModel. Each bar shows how much each representative contributes to the full dataset representation.</p> <p>Optionally displays a reference line showing uniform weights (1/k) for comparison with non-uniform weighting schemes like cluster-size based weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ResponsibilityBars\n&gt;&gt;&gt;\n&gt;&gt;&gt; # After running workflow with result containing weights\n&gt;&gt;&gt; weights = result.weights  # e.g., {Period('2024-01'): 0.35, ...}\n&gt;&gt;&gt; bars = ResponsibilityBars()\n&gt;&gt;&gt; fig = bars.plot(weights, show_uniform_reference=True)\n&gt;&gt;&gt; fig.update_layout(title='Responsibility Weights')\n&gt;&gt;&gt; fig.show()\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ResponsibilityBars.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize ResponsibilityBars diagnostic.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ResponsibilityBars.plot","title":"plot","text":"<pre><code>plot(weights: dict[Hashable, float], show_uniform_reference: bool = True) -&gt; Figure\n</code></pre> <p>Create bar chart of responsibility weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>dict[Hashable, float]</code> <p>Dictionary mapping slice identifiers to their weights. Weights should sum to 1.0 for meaningful comparison with the uniform reference line.</p> required <code>show_uniform_reference</code> <code>bool</code> <p>If True, adds horizontal dashed line showing uniform weight (1/k) for comparison.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with bar chart. X-axis shows slice labels, Y-axis</p> <code>Figure</code> <p>shows weight values. Text labels show weights to 3 decimal places.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If weights dictionary is empty.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatter2D","title":"ParetoScatter2D","text":"<p>2D scatter plot of all evaluated combinations with Pareto front highlighted.</p> <p>Visualizes the objective space for two objectives, showing: - All evaluated combinations as scatter points - Pareto-optimal solutions highlighted - Selected combination (if provided) marked distinctly - Feasible vs infeasible solutions (if constraints exist)</p> <p>Parameters:</p> Name Type Description Default <code>objective_x</code> <code>str</code> <p>Name of objective for x-axis.</p> required <code>objective_y</code> <code>str</code> <p>Name of objective for y-axis.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoScatter2D\n&gt;&gt;&gt; scatter = ParetoScatter2D(objective_x='wasserstein', objective_y='correlation')\n&gt;&gt;&gt; fig = scatter.plot(\n...     search_algorithm=workflow.search_algorithm,\n...     selected_combination=result.selection\n... )\n&gt;&gt;&gt; fig.update_layout(title='Pareto Front: Wasserstein vs Correlation')\n&gt;&gt;&gt; fig.show()\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatter2D.__init__","title":"__init__","text":"<pre><code>__init__(objective_x: str, objective_y: str)\n</code></pre> <p>Initialize Pareto scatter diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>objective_x</code> <code>str</code> <p>Name of objective for x-axis.</p> required <code>objective_y</code> <code>str</code> <p>Name of objective for y-axis.</p> required"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatter2D.plot","title":"plot","text":"<pre><code>plot(search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm, selected_combination: SliceCombination | None = None) -&gt; Figure\n</code></pre> <p>Create 2D scatter plot of Pareto front.</p> <p>Parameters:</p> Name Type Description Default <code>search_algorithm</code> <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <p>Search algorithm after find_selection() has been called.</p> required <code>selected_combination</code> <code>SliceCombination | None</code> <p>Optional combination to highlight (e.g., result.selection).</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with scatter plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() hasn't been called or objectives not found.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatterMatrix","title":"ParetoScatterMatrix","text":"<p>Scatter matrix of all objectives showing Pareto front.</p> <p>Creates a scatter plot matrix (SPLOM) showing pairwise relationships between all objectives. Each subplot shows two objectives with Pareto front highlighted.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all objectives).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoScatterMatrix\n&gt;&gt;&gt; scatter_matrix = ParetoScatterMatrix(\n...     objectives=['wasserstein', 'correlation', 'diurnal']\n... )\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     search_algorithm=workflow.search_algorithm,\n...     selected_combination=result.selection\n... )\n&gt;&gt;&gt; fig.update_layout(title='Pareto Front: All Objectives')\n&gt;&gt;&gt; fig.show()\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatterMatrix.__init__","title":"__init__","text":"<pre><code>__init__(objectives: list[str] | None = None)\n</code></pre> <p>Initialize Pareto scatter matrix diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all).</p> <code>None</code>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatterMatrix.plot","title":"plot","text":"<pre><code>plot(search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm, selected_combination: SliceCombination | None = None) -&gt; Figure\n</code></pre> <p>Create scatter matrix of Pareto front.</p> <p>Parameters:</p> Name Type Description Default <code>search_algorithm</code> <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <p>Search algorithm after find_selection() has been called.</p> required <code>selected_combination</code> <code>SliceCombination | None</code> <p>Optional combination to highlight.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with scatter matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() hasn't been called.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoParallelCoordinates","title":"ParetoParallelCoordinates","text":"<p>Parallel coordinates plot of Pareto front.</p> <p>Visualizes multi-objective trade-offs using parallel coordinates where each vertical axis represents one objective. Lines connecting axes show individual solutions, with Pareto-optimal solutions highlighted.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all objectives).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoParallelCoordinates\n&gt;&gt;&gt; parallel = ParetoParallelCoordinates()\n&gt;&gt;&gt; fig = parallel.plot(search_algorithm=workflow.search_algorithm)\n&gt;&gt;&gt; fig.update_layout(title='Pareto Front: Parallel Coordinates')\n&gt;&gt;&gt; fig.show()\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoParallelCoordinates.__init__","title":"__init__","text":"<pre><code>__init__(objectives: list[str] | None = None)\n</code></pre> <p>Initialize parallel coordinates diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all).</p> <code>None</code>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoParallelCoordinates.plot","title":"plot","text":"<pre><code>plot(search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm) -&gt; Figure\n</code></pre> <p>Create parallel coordinates plot of Pareto front.</p> <p>Parameters:</p> Name Type Description Default <code>search_algorithm</code> <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <p>Search algorithm after find_selection() has been called.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with parallel coordinates plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() hasn't been called.</p>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ScoreContributionBars","title":"ScoreContributionBars","text":"<p>Bar chart showing final scores from each objective component.</p> <p>Visualizes the contribution of each score component to understand which objectives were most influential in the final selection. Can display absolute scores or normalized as fractions of total.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ScoreContributionBars\n&gt;&gt;&gt; contrib = ScoreContributionBars()\n&gt;&gt;&gt; fig = contrib.plot(result.scores, normalize=True)\n&gt;&gt;&gt; fig.update_layout(title='Score Component Contributions')\n&gt;&gt;&gt; fig.show()\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ScoreContributionBars.plot","title":"plot","text":"<pre><code>plot(scores: dict[str, float], normalize: bool = False) -&gt; Figure\n</code></pre> <p>Create bar chart of score component contributions.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>dict[str, float]</code> <p>Dictionary of scores from each component (from result.scores).</p> required <code>normalize</code> <code>bool</code> <p>If True, show as fractions of total score.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with bar chart.</p>"},{"location":"api/feature_engineering/","title":"Feature Engineering","text":""},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeatureEngineer","title":"FeatureEngineer","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for feature engineering transformations (Pillar F).</p> <p>Transforms raw sliced time-series data into a feature matrix that can be used for comparing and selecting representative periods. Implementations define how raw data is converted into a comparable feature space.</p> <p>The run() method creates a new ProblemContext with df_features populated, while subclasses implement _calc_and_get_features_df() to define the specific feature engineering logic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class SimpleStatsFeatureEngineer(FeatureEngineer):\n...     def _calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n...         features = []\n...         for slice_id in context.slicer.slices:\n...             slice_data = context.df_raw.loc[slice_id]\n...             features.append({\n...                 'mean': slice_data.mean().mean(),\n...                 'std': slice_data.std().mean(),\n...                 'max': slice_data.max().max()\n...             })\n...         return pd.DataFrame(features, index=context.slicer.slices)\n...\n&gt;&gt;&gt; engineer = SimpleStatsFeatureEngineer()\n&gt;&gt;&gt; context_with_features = engineer.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.head())\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeatureEngineer.run","title":"run","text":"<pre><code>run(context: ProblemContext) -&gt; ProblemContext\n</code></pre> <p>Calculate features and return a new context with df_features populated.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context containing raw time-series data and slicing information.</p> required <p>Returns:</p> Type Description <code>ProblemContext</code> <p>A new ProblemContext instance with df_features set to the computed</p> <code>ProblemContext</code> <p>feature matrix. The original context is not modified.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df  <code>abstractmethod</code>","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Calculate and return the feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context containing raw data and slicing information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where each row represents one slice (candidate period) and</p> <code>DataFrame</code> <p>each column represents a feature. The index should match the slice</p> <code>DataFrame</code> <p>identifiers from context.slicer.slices.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeaturePipeline","title":"FeaturePipeline","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Chains multiple feature engineers to create a combined feature space.</p> <p>Runs multiple feature engineering transformations sequentially and concatenates their outputs into a single feature matrix. Useful for combining different feature types (e.g., statistical summaries + PCA components).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.feature_engineering import StandardStatsFeatureEngineer, PCAFeatureEngineer\n&gt;&gt;&gt; stats_engineer = StandardStatsFeatureEngineer()\n&gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=3)\n&gt;&gt;&gt; pipeline = FeaturePipeline({'stats': stats_engineer, 'pca': pca_engineer})\n&gt;&gt;&gt; context_with_features = pipeline.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.columns)\n    # Shows columns from both engineers: ['mean', 'std', 'max', 'min', 'pc1', 'pc2', 'pc3']\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeaturePipeline.__init__","title":"__init__","text":"<pre><code>__init__(engineers: dict[str, FeatureEngineer])\n</code></pre> <p>Initialize the feature pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>engineers</code> <code>dict[str, FeatureEngineer]</code> <p>Dict of FeatureEngineer instances to run sequentially. Features from all engineers will be concatenated column-wise.</p> required"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeaturePipeline.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Calculate features from all engineers sequentially, accumulating results.</p> <p>Each engineer in the pipeline sees the accumulated features from all previous engineers via context.df_features. New features from each stage are concatenated to the existing feature set. This allows: - Early engineers to create base features (e.g., StandardStatsFeatureEngineer) - Later engineers to transform or add to those features (e.g., PCAFeatureEngineer)</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context containing raw data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns from all engineers concatenated horizontally.</p> <code>DataFrame</code> <p>Each engineer's features are added to the cumulative feature set.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer","title":"StandardStatsFeatureEngineer","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Extracts statistical features from time-series slices with robust scaling.</p> <p>For each original variable and slice, computes: - Central tendency: mean, median (q50) - Dispersion: std, IQR (q90 - q10), q10, q90 - Distribution shape: neg_share (proportion of negative values) - Temporal dynamics: ramp_std (std of first differences)</p> <p>Optionally includes cross-variable correlations within each slice (upper triangle only, Fisher-z transformed). Features are z-score normalized across slices to ensure comparability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; engineer = StandardStatsFeatureEngineer()\n&gt;&gt;&gt; context_with_features = engineer.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.columns)\n# ['mean__demand', 'mean__solar', 'std__demand', 'std__solar', ...]\n</code></pre> <pre><code>&gt;&gt;&gt; engineer_no_corr = StandardStatsFeatureEngineer(\n...     include_correlations=False,\n...     scale='zscore'\n... )\n&gt;&gt;&gt; context_with_features = engineer_no_corr.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.shape)\n# (12, 16) for 12 months, 2 variables, 8 stats each\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer.__init__","title":"__init__","text":"<pre><code>__init__(include_correlations: bool = True, scale: Literal['zscore', 'none'] = 'zscore', min_rows_for_corr: int = 8)\n</code></pre> <p>Initialize the statistical feature engineer.</p> <p>Parameters:</p> Name Type Description Default <code>include_correlations</code> <code>bool</code> <p>If True, include cross-variable correlations per slice (Fisher-z transformed).</p> <code>True</code> <code>scale</code> <code>Literal['zscore', 'none']</code> <p>Scaling method. Currently only \"zscore\" is fully supported.</p> <code>'zscore'</code> <code>min_rows_for_corr</code> <code>int</code> <p>Minimum number of rows per slice required to compute correlations. Slices with fewer rows get correlation features set to 0.</p> <code>8</code>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: 'ProblemContext') -&gt; DataFrame\n</code></pre> <p>Calculate statistical features and return scaled feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'ProblemContext'</code> <p>Problem context with raw time-series data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame where each row is a slice and columns are scaled statistical</p> <code>DataFrame</code> <p>features. Column names follow pattern '{stat}__{variable}'.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer.feature_names","title":"feature_names","text":"<pre><code>feature_names() -&gt; list[str]\n</code></pre> <p>Get list of feature column names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of feature names in the format '{stat}__{variable}' or</p> <code>list[str]</code> <p>'corr__{var1}__{var2}' for correlations.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer","title":"PCAFeatureEngineer","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Performs PCA dimensionality reduction on existing features.</p> <p>Reduces the feature space using Principal Component Analysis, typically applied after statistical feature engineering. This is useful for: - Reducing dimensionality when you have many correlated features - Creating orthogonal feature representations - Focusing on the main axes of variation</p> <p>Commonly used in a FeaturePipeline after StandardStatsFeatureEngineer to compress statistical features into a smaller number of principal components.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int | float | None</code> <p>Number of principal components to retain. Can be: - int: Exact number of components - float (0.0-1.0): Retain enough components to explain this   fraction of variance - None: Retain all components (no reduction)</p> <code>None</code> <code>whiten</code> <code>bool</code> <p>If True, scale components to unit variance. This can improve results when PCA features are used with distance-based algorithms.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.feature_engineering import PCAFeatureEngineer\n&gt;&gt;&gt; # Use PCA alone (requires context to already have df_features)\n&gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=5)\n&gt;&gt;&gt; context_with_pca = pca_engineer.run(context_with_features)\n&gt;&gt;&gt; print(context_with_pca.df_features.columns)\n    ['pc_0', 'pc_1', 'pc_2', 'pc_3', 'pc_4']\n\n&gt;&gt;&gt; # More common: chain with StandardStats in a pipeline\n&gt;&gt;&gt; from energy_repset.feature_engineering import (\n...     StandardStatsFeatureEngineer,\n...     FeaturePipeline\n... )\n&gt;&gt;&gt; pipeline = FeaturePipeline([\n...     StandardStatsFeatureEngineer(),\n...     PCAFeatureEngineer(n_components=0.95)  # Keep 95% variance\n... ])\n&gt;&gt;&gt; context_with_both = pipeline.run(context)\n\n&gt;&gt;&gt; # Check explained variance\n&gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=10)\n&gt;&gt;&gt; context_out = pca_engineer.run(context_with_features)\n&gt;&gt;&gt; print(pca_engineer.explained_variance_ratio_)\n    [0.45, 0.22, 0.11, ...]\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.explained_variance_ratio_","title":"explained_variance_ratio_  <code>property</code>","text":"<pre><code>explained_variance_ratio_: ndarray | None\n</code></pre> <p>Get the proportion of variance explained by each component.</p> <p>Returns:</p> Type Description <code>ndarray | None</code> <p>Array of explained variance ratios, or None if PCA not fitted yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pca_eng = PCAFeatureEngineer(n_components=5)\n&gt;&gt;&gt; context_out = pca_eng.run(context_with_features)\n&gt;&gt;&gt; print(pca_eng.explained_variance_ratio_)\n# [0.45, 0.22, 0.15, 0.09, 0.05]\n&gt;&gt;&gt; print(f\"Total variance explained: {pca_eng.explained_variance_ratio_.sum():.2%}\")\n# Total variance explained: 96%\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.components_","title":"components_  <code>property</code>","text":"<pre><code>components_: ndarray | None\n</code></pre> <p>Get the principal component loadings.</p> <p>Returns:</p> Type Description <code>ndarray | None</code> <p>Array of shape (n_components, n_features) containing the</p> <code>ndarray | None</code> <p>principal axes in feature space, or None if PCA not fitted yet.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.__init__","title":"__init__","text":"<pre><code>__init__(n_components: int | float | None = None, whiten: bool = False) -&gt; None\n</code></pre> <p>Initialize PCA feature engineer.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int | float | None</code> <p>Number of components to keep, or fraction of variance to preserve (if float). None keeps all components.</p> <code>None</code> <code>whiten</code> <code>bool</code> <p>Whether to whiten (scale) the principal components.</p> <code>False</code>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Apply PCA to existing features in context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with df_features already populated (typically by StandardStatsFeatureEngineer or similar).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with principal component features. Columns are named</p> <code>DataFrame</code> <p>'pc_0', 'pc_1', etc.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If context.df_features is None or empty.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.feature_names","title":"feature_names","text":"<pre><code>feature_names() -&gt; list[str]\n</code></pre> <p>Get list of principal component feature names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of feature names: ['pc_0', 'pc_1', ...].</p>"},{"location":"api/objectives/","title":"Objectives","text":""},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSpec","title":"ObjectiveSpec  <code>dataclass</code>","text":"<p>Specification for a single score component with its preference weight.</p> <p>Attributes:</p> Name Type Description <code>component</code> <code>ScoreComponent</code> <p>The ScoreComponent that computes the metric.</p> <code>weight</code> <code>float</code> <p>Non-negative weight indicating the component's importance (&gt;= 0).</p>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet","title":"ObjectiveSet","text":"<p>Pillar O: A collection of weighted score components for evaluating selections.</p> <p>This class holds multiple ScoreComponents, each with a weight indicating its importance. Components define their optimization direction (min/max), while weights specify preference magnitude. The ObjectiveSet prepares all components with context data and evaluates candidate selections.</p> <p>Attributes:</p> Name Type Description <code>weighted_score_components</code> <code>dict[str, ObjectiveSpec]</code> <p>Dictionary mapping component names to ObjectiveSpec instances containing the component and its weight.</p> <p>Examples:</p> <p>Create an objective set with multiple fidelity metrics:</p> <pre><code>&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt; from energy_repset.score_components import (\n...     WassersteinFidelity, CorrelationFidelity\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; objective_set = ObjectiveSet({\n...     'wasserstein': (0.5, WassersteinFidelity()),\n...     'correlation': (0.5, CorrelationFidelity()),\n... })\n</code></pre> <p>With variable-specific weights:</p> <pre><code>&gt;&gt;&gt; wass = WassersteinFidelity(variable_weights={'demand': 2.0, 'solar': 1.0})\n&gt;&gt;&gt; objective_set = ObjectiveSet({\n...     'wasserstein': (1.0, wass),\n... })\n</code></pre>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.__init__","title":"__init__","text":"<pre><code>__init__(weighted_score_components: dict[str, tuple[float, ScoreComponent]]) -&gt; None\n</code></pre> <p>Initialize ObjectiveSet with weighted score components.</p> <p>Parameters:</p> Name Type Description Default <code>weighted_score_components</code> <code>dict[str, tuple[float, ScoreComponent]]</code> <p>Dictionary mapping component names to tuples of (weight, ScoreComponent). Weights must be non-negative.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any weight is negative or if any component lacks a 'direction' attribute set to 'min' or 'max'.</p>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Prepare all score components with context data.</p> <p>This method calls prepare() on each component to allow pre-computation of reference statistics, duration curves, etc.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>ProblemContext containing raw data and features.</p> required"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.evaluate","title":"evaluate","text":"<pre><code>evaluate(combination: SliceCombination, context: ProblemContext) -&gt; dict[str, float]\n</code></pre> <p>Evaluate a candidate selection across all score components.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels forming the candidate selection.</p> required <code>context</code> <code>ProblemContext</code> <p>ProblemContext for accessing data.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary mapping component names to their unweighted scores.</p> Note <p>Returns raw scores from components. Weights are applied by SelectionPolicy during the selection process.</p>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.component_meta","title":"component_meta","text":"<pre><code>component_meta() -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Get metadata for all components.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping component names to their metadata containing: - 'direction': Optimization direction ('min' or 'max') - 'pref': Preference weight (&gt;= 0)</p>"},{"location":"api/objectives/#energy_repset.score_components.base_score_component.ScoreComponent","title":"ScoreComponent","text":"<p>               Bases: <code>ABC</code></p> <p>Protocol for a single metric used in evaluating candidate selections.</p> <p>ScoreComponents are the building blocks of the ObjectiveSet. Each component computes a scalar score measuring how well a candidate selection performs on a specific criterion (e.g., distribution fidelity, diversity, balance).</p> Implementations must define <ul> <li>A unique name identifying the component</li> <li>An optimization direction ('min' or 'max')</li> <li>A prepare() method to precompute reference data from the full context</li> <li>A score() method to evaluate a candidate selection</li> </ul> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for this component.</p> <code>direction</code> <code>ScoreComponentDirection</code> <p>Optimization direction, either \"min\" or \"max\".</p> <p>Examples:</p> <p>Implementing a simple score component:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components.base_score_component import ScoreComponent\n&gt;&gt;&gt; from energy_repset.context import ProblemContext\n&gt;&gt;&gt; from energy_repset.types import SliceCombination\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; class SimpleMeanDeviation(ScoreComponent):\n...     def __init__(self):\n...         self.name = \"mean_deviation\"\n...         self.direction = \"min\"\n...         self.full_mean = None\n...\n...     def prepare(self, context: ProblemContext) -&gt; None:\n...         '''Compute reference mean from full dataset.'''\n...         self.full_mean = context.df_raw.mean().mean()\n...\n...     def score(self, combination: SliceCombination) -&gt; float:\n...         '''Measure deviation from reference mean.'''\n...         # Get data for selection and compute deviation\n...         # (implementation details omitted)\n...         return abs(selection_mean - self.full_mean)\n</code></pre>"},{"location":"api/objectives/#energy_repset.score_components.base_score_component.ScoreComponent.prepare","title":"prepare  <code>abstractmethod</code>","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute state needed before scoring selections.</p> <p>This method is called once before evaluating any combinations. Use it to compute reference statistics, duration curves, or other data derived from the full dataset that will be compared against selections.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>ProblemContext containing raw data, features, and metadata.</p> required Note <p>This method should store computed state as instance attributes for use in score().</p>"},{"location":"api/objectives/#energy_repset.score_components.base_score_component.ScoreComponent.score","title":"score  <code>abstractmethod</code>","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute the component score for a candidate selection.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels forming the candidate selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar score. Lower is better for direction='min', higher is better</p> <code>float</code> <p>for direction='max'.</p> Note <p>This method is called many times during search. Precompute expensive operations in prepare() to avoid redundant calculations.</p>"},{"location":"api/representation/","title":"Representation Models","text":""},{"location":"api/representation/#energy_repset.representation.RepresentationModel","title":"RepresentationModel","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for representation models (Pillar R).</p> <p>Defines how selected representative periods represent the full dataset by calculating responsibility weights. The model is first fitted to learn about the entire dataset, then the weigh() method calculates weights for specific selections.</p> <p>Different models implement different weighting strategies: - Uniform: Equal weights (e.g., 365/k for yearly data) - Cluster-based: Weights proportional to cluster sizes - Blended: Soft assignment where each period is a weighted mix of representatives</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class UniformWeights(RepresentationModel):\n...     def fit(self, context: ProblemContext):\n...         self.n_total = len(context.slicer.slices)\n...\n...     def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n...         weight = self.n_total / len(combination)\n...         return {slice_id: weight for slice_id in combination}\n...\n&gt;&gt;&gt; model = UniformWeights()\n&gt;&gt;&gt; model.fit(context)\n&gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n&gt;&gt;&gt; print(weights)  # {0: 91.25, 3: 91.25, 6: 91.25, 9: 91.25} for 365 days, k=4\n</code></pre> <pre><code>&gt;&gt;&gt; class ClusterSizeWeights(RepresentationModel):\n...     def fit(self, context: ProblemContext):\n...         from sklearn.cluster import KMeans\n...         self.kmeans = KMeans(n_clusters=4)\n...         self.kmeans.fit(context.df_features)\n...\n...     def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n...         labels = self.kmeans.labels_\n...         weights = {}\n...         for i, slice_id in enumerate(combination):\n...             cluster_size = (labels == i).sum()\n...             weights[slice_id] = cluster_size\n...         return weights\n...\n&gt;&gt;&gt; model = ClusterSizeWeights()\n&gt;&gt;&gt; model.fit(context)\n&gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n&gt;&gt;&gt; print(weights)  # Weights proportional to cluster membership\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.RepresentationModel.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(context: 'ProblemContext')\n</code></pre> <p>Fit the representation model to the full dataset.</p> <p>This method performs any necessary pre-computation based on the full set of candidate slices (e.g., storing the feature matrix, fitting clustering models, computing distance matrices).</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'ProblemContext'</code> <p>The problem context with df_features populated. Feature engineering must be run before calling this method.</p> required"},{"location":"api/representation/#energy_repset.representation.RepresentationModel.weigh","title":"weigh  <code>abstractmethod</code>","text":"<pre><code>weigh(combination: SliceCombination) -&gt; dict[Hashable, float] | DataFrame\n</code></pre> <p>Calculate representation weights for a given selection.</p> <p>This method should only be called after the model has been fitted.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of selected slice identifiers for which to calculate representation weights.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, float] | DataFrame</code> <p>The calculated weights, either as a dictionary mapping each selected</p> <code>dict[Hashable, float] | DataFrame</code> <p>slice to its weight, or as a DataFrame for more complex weight</p> <code>dict[Hashable, float] | DataFrame</code> <p>structures (e.g., blended models where each original period has</p> <code>dict[Hashable, float] | DataFrame</code> <p>weights across multiple representatives).</p>"},{"location":"api/representation/#energy_repset.representation.UniformRepresentationModel","title":"UniformRepresentationModel","text":"<p>               Bases: <code>RepresentationModel</code></p> <p>Assigns equal weights to all selected representatives.</p> <p>The simplest representation model where each selected period gets weight 1/k. This is appropriate when you want each representative to contribute equally to downstream modeling, regardless of how many original periods it represents.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = UniformRepresentationModel()\n&gt;&gt;&gt; model.fit(context)\n&gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n&gt;&gt;&gt; print(weights)\n    {0: 0.25, 3: 0.25, 6: 0.25, 9: 0.25}\n\n&gt;&gt;&gt; # For yearly data with k=4 months, each month represents ~91 days\n&gt;&gt;&gt; # Weights sum to 1.0 for normalized analysis\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.UniformRepresentationModel.fit","title":"fit","text":"<pre><code>fit(context: ProblemContext)\n</code></pre> <p>No fitting required for uniform weighting.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context (unused but required by protocol).</p> required"},{"location":"api/representation/#energy_repset.representation.UniformRepresentationModel.weigh","title":"weigh","text":"<pre><code>weigh(combination: SliceCombination) -&gt; dict[Hashable, float]\n</code></pre> <p>Calculate uniform weights (1/k for each selected period).</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of selected slice identifiers.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, float]</code> <p>Dictionary mapping each slice ID to its weight (1/k).</p>"},{"location":"api/representation/#energy_repset.representation.KMedoidsClustersizeRepresentation","title":"KMedoidsClustersizeRepresentation","text":"<p>               Bases: <code>RepresentationModel</code></p> <p>Assigns weights based on k-medoids cluster sizes (hard assignment).</p> <p>This representation model performs virtual k-medoids clustering where the selected periods are enforced as medoids (cluster centers). Each candidate period is assigned to its nearest medoid, and weights are calculated as the proportion of periods assigned to each medoid.</p> <p>The weights reflect how many original periods each representative is responsible for, making this appropriate when representatives should be weighted by their \"sphere of influence\" in feature space.</p> <p>Attributes:</p> Name Type Description <code>all_features_</code> <p>Feature matrix for all candidate periods (set during fit).</p> <code>all_slice_labels_</code> <p>Labels for all candidate periods (set during fit).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = KMedoidsClustersizeRepresentation()\n&gt;&gt;&gt; model.fit(context)  # context has 12 monthly candidates\n&gt;&gt;&gt; weights = model.weigh((Period('2024-01', 'M'), Period('2024-06', 'M')))\n&gt;&gt;&gt; print(weights)\n    {Period('2024-01', 'M'): 0.583, Period('2024-06', 'M'): 0.417}\n&gt;&gt;&gt; # Jan represents 7 months, Jun represents 5 months\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.KMedoidsClustersizeRepresentation.fit","title":"fit","text":"<pre><code>fit(context: ProblemContext)\n</code></pre> <p>Store the full feature matrix for later clustering.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing df_features and candidates.</p> required"},{"location":"api/representation/#energy_repset.representation.KMedoidsClustersizeRepresentation.weigh","title":"weigh","text":"<pre><code>weigh(combination: SliceCombination) -&gt; dict[Hashable, float]\n</code></pre> <p>Calculate weights based on cluster sizes from hard assignment.</p> <p>Performs virtual k-medoids clustering where: 1. Selected periods are enforced as medoids 2. Each candidate is assigned to its nearest medoid (Euclidean distance) 3. Weight = (cluster size) / (total candidates)</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of selected slice identifiers.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, float]</code> <p>Dictionary mapping each slice ID to its weight (proportion of</p> <code>dict[Hashable, float]</code> <p>candidates assigned to it).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If combination contains slices not in the feature matrix.</p>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel","title":"BlendedRepresentationModel","text":"<p>               Bases: <code>RepresentationModel</code></p> <p>Assigns weights using a blended representation (R_soft).</p> <p>Each original slice in the full dataset is represented as a unique weighted combination of all the selected representatives. This is found by solving a small optimization problem for each original slice.</p> <p>The output is a DataFrame where rows are the original slice labels, columns are the selected representative labels, and values are the weights.</p>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel.__init__","title":"__init__","text":"<pre><code>__init__(blend_type: str = 'convex')\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel.__init__--parameters","title":"Parameters","text":"<p>blend_type : str, optional     The type of blend to perform. 'convex' is the most common,     ensuring weights are non-negative and sum to 1.     (default is 'convex')</p>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel.fit","title":"fit","text":"<pre><code>fit(context: 'ProblemContext')\n</code></pre> <p>Stores the full feature matrix for later use.</p>"},{"location":"api/score_components/","title":"Score Components","text":""},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity","title":"WassersteinFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Measures distribution similarity using 1D Wasserstein distance per variable.</p> <p>Computes the Earth Mover's Distance between the full dataset's distribution and the selected subset's distribution for each variable. Distances are normalized by the interquartile range (IQR) to make them scale-invariant and comparable across variables.</p> <p>Lower scores indicate better distribution matching. This component is particularly effective for preserving statistical properties of the data.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights for prioritizing certain variables in the score. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Equal weights (default)\n&gt;&gt;&gt; component = WassersteinFidelity()\n&gt;&gt;&gt; component.prepare(context)\n&gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n&gt;&gt;&gt; print(f\"Wasserstein distance: {score:.3f}\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With variable-specific weights\n&gt;&gt;&gt; component = WassersteinFidelity(\n...     variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n... )\n&gt;&gt;&gt; component.prepare(context)\n&gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n&gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity.__init__","title":"__init__","text":"<pre><code>__init__(variable_weights: Dict[str, float] | None = None) -&gt; None\n</code></pre> <p>Initialize Wasserstein fidelity component.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code>"},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute reference distributions and normalization factors.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute normalized Wasserstein distance between full and selection.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sum of per-variable Wasserstein distances, each normalized by IQR</p> <code>float</code> <p>and weighted according to variable_weights. Lower is better.</p>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity","title":"CorrelationFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Preserves cross-variable correlation structure using Frobenius norm.</p> <p>Measures how well the selection preserves the correlation structure between variables by comparing the full dataset's correlation matrix with the selection's correlation matrix. Uses relative Frobenius norm of the difference matrix.</p> <p>Lower scores indicate better preservation of variable relationships. This component is important for downstream modeling tasks that depend on realistic co-occurrence patterns (e.g., solar and wind generation).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; component = CorrelationFidelity()\n&gt;&gt;&gt; component.prepare(context)\n&gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n&gt;&gt;&gt; print(f\"Correlation mismatch: {score:.3f}\")\n# 0.0 would be perfect preservation, 1.0+ indicates poor preservation\n</code></pre> <pre><code>&gt;&gt;&gt; # Combine with Wasserstein in an ObjectiveSet\n&gt;&gt;&gt; from energy_repset import ObjectiveSet, ObjectiveSpec\n&gt;&gt;&gt; objectives = ObjectiveSet([\n...     ObjectiveSpec('wasserstein', WassersteinFidelity(), weight=1.0),\n...     ObjectiveSpec('correlation', CorrelationFidelity(), weight=1.0)\n... ])\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize correlation fidelity component.</p>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute full dataset's correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute relative Frobenius norm of correlation matrix difference.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Relative Frobenius norm ||C_full - C_sel||_F / ||C_full||_F where</p> <code>float</code> <p>C denotes correlation matrices. Lower is better (0 = perfect match).</p>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity","title":"DiurnalFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Measures how well the selection preserves hourly (diurnal) patterns.</p> <p>Compares the mean hourly profiles between the full dataset and the selected subset. This is useful for applications where intraday patterns matter (e.g., electricity demand profiles, solar generation curves).</p> <p>The score is the normalized mean squared error between the full and selected hour-of-day profiles, averaged across all variables and hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DiurnalFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add diurnal fidelity to your objective set\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'diurnal': (1.0, DiurnalFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # For hourly data, this ensures selected periods\n&gt;&gt;&gt; # preserve the typical daily load shape\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize diurnal fidelity component.</p>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute the full dataset's mean hourly profile.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute normalized MSE between full and selection diurnal profiles.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Normalized mean squared error across all variables and hours.</p> <code>float</code> <p>Lower values indicate better preservation of diurnal patterns.</p>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity","title":"DurationCurveFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Matches duration curves using quantile approximation and IQR normalization.</p> <p>Measures how well the selection preserves the statistical distribution of each variable by comparing quantiles of the full and selected data. This is more computationally efficient than NRMSEFidelity for large datasets since it compares a fixed number of quantiles rather than full sorted arrays.</p> <p>Uses IQR (interquartile range) normalization instead of mean normalization, making it more robust to outliers.</p> <p>Parameters:</p> Name Type Description Default <code>n_quantiles</code> <code>int</code> <p>Number of quantiles to compute for duration curve approximation. Default is 101 (0%, 1%, ..., 100%).</p> <code>101</code> <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights for prioritizing certain variables in the score. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DurationCurveFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: 101 quantiles (0%, 1%, ..., 100%)\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'duration': (1.0, DurationCurveFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Coarser approximation for faster computation\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'duration': (1.0, DurationCurveFidelity(n_quantiles=21))\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With variable weights for prioritizing specific variables\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'duration': (1.0, DurationCurveFidelity(\n...         n_quantiles=101,\n...         variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n...     ))\n... })\n&gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity.__init__","title":"__init__","text":"<pre><code>__init__(n_quantiles: int = 101, variable_weights: Dict[str, float] | None = None) -&gt; None\n</code></pre> <p>Initialize duration curve fidelity component.</p> <p>Parameters:</p> Name Type Description Default <code>n_quantiles</code> <code>int</code> <p>Number of quantiles for duration curve approximation.</p> <code>101</code> <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute quantiles and normalization factors for full dataset.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute sum of per-variable NRMSE for quantile-based duration curves.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Weighted sum of per-variable NRMSE values using IQR normalization.</p> <code>float</code> <p>Returns infinity if the selection is empty.</p>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity","title":"NRMSEFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Matches duration curves using interpolation and NRMSE.</p> <p>Measures how well the selection preserves the statistical distribution of each variable by comparing full and selected duration curves (sorted value profiles). The selection's duration curve is interpolated to match the full curve's length, then NRMSE is computed.</p> <p>This approach uses the full sorted arrays and is accurate but can be computationally expensive for very large datasets. For efficiency with large data, consider DurationCurveFidelity which uses quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights for prioritizing certain variables in the score. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import NRMSEFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage with equal variable weights\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'nrmse': (1.0, NRMSEFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prioritize specific variables\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'nrmse': (1.0, NRMSEFidelity(\n...         variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n...     ))\n... })\n&gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity.__init__","title":"__init__","text":"<pre><code>__init__(variable_weights: Dict[str, float] | None = None) -&gt; None\n</code></pre> <p>Initialize NRMSE fidelity component.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute full duration curves and normalization factors.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute sum of per-variable NRMSE for duration curves.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Weighted sum of per-variable NRMSE values. Returns infinity</p> <code>float</code> <p>if the selection is empty.</p>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity","title":"DTWFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Measures representation quality using Dynamic Time Warping distance.</p> <p>Computes the average DTW distance from each unselected slice to its nearest representative in the selection. This is analogous to inertia in k-medoids clustering but uses DTW instead of Euclidean distance.</p> <p>DTW allows temporal alignment, making it suitable for time-series where similar patterns may be shifted in time (e.g., seasonal load profiles with varying peak times).</p> <p>Requires the <code>tslearn</code> package for DTW computation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DTWFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use DTW for time-series with temporal shifts\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'dtw': (1.0, DTWFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Good for multi-day periods with similar but shifted patterns\n&gt;&gt;&gt; # e.g., weeks with similar load but peak occurring at different times\n</code></pre> Note <p>This requires <code>tslearn</code> to be installed:     pip install tslearn</p>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize DTW fidelity component.</p>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute per-slice time-series data.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute average DTW distance from unselected to selected slices.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average DTW distance from each unselected slice to its nearest</p> <code>float</code> <p>representative. Returns 0.0 if all slices are selected or</p> <code>float</code> <p>none are selected.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If tslearn is not installed.</p>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity","title":"DiurnalDTWFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Preserves hourly patterns using Dynamic Time Warping on diurnal profiles.</p> <p>Combines the concepts of DiurnalFidelity and DTWFidelity: compares hour-of-day aggregated profiles between full and selected data, but uses DTW distance instead of MSE to allow for temporal flexibility.</p> <p>This is useful when you want to preserve the general shape of hourly patterns but allow for some temporal shifting (e.g., load profiles with similar shapes but shifted peak hours).</p> <p>Uses a custom DTW implementation (no external dependencies), normalized by the standard deviation of the full diurnal profile.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DiurnalDTWFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Preserve diurnal patterns with temporal flexibility\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'diurnal_dtw': (1.0, DiurnalDTWFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Useful when hourly patterns are important but exact timing\n&gt;&gt;&gt; # alignment is not critical (e.g., shifted daily load curves)\n</code></pre> Note <p>Unlike DTWFidelity, this does not require tslearn since it uses a custom DTW implementation on aggregated hourly profiles.</p>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize diurnal DTW fidelity component.</p>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute full dataset's diurnal profile and normalization factors.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute sum of per-variable normalized DTW distances for diurnal profiles.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sum of per-variable DTW distances between full and selected</p> <code>float</code> <p>diurnal profiles, normalized by standard deviation. Returns</p> <code>float</code> <p>infinity if the selection is empty.</p>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward","title":"DiversityReward","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Rewards selections with diverse, mutually distant representative periods.</p> <p>Computes the average pairwise Euclidean distance between selected slice features in feature space. Higher diversity can help ensure the selection covers a wider range of conditions, avoiding redundant representatives.</p> <p>This is particularly useful when combined with fidelity objectives to balance accuracy with coverage.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DiversityReward\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Encourage diverse representatives\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'diversity': (0.3, DiversityReward())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Combine with fidelity for balanced selection\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'fidelity': (1.0, WassersteinFidelity()),\n...     'diversity': (0.2, DiversityReward())\n... })\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize diversity reward component.</p>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Store the feature matrix for pairwise distance computation.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with computed features.</p> required"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute mean pairwise Euclidean distance among selected features.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average pairwise distance in feature space. Returns 0.0 if</p> <code>float</code> <p>fewer than two slices are selected.</p>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance","title":"CentroidBalance","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Penalizes selections whose centroid deviates from the global center.</p> <p>Computes the Euclidean distance between the centroid of selected slice features and the origin (global center in standardized feature space).</p> <p>This objective ensures the selection doesn't systematically bias toward extreme conditions, maintaining balance around typical conditions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import CentroidBalance\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Penalize selections biased toward extreme periods\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'balance': (0.5, CentroidBalance())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Used in examples/ex2_feature_space.py to maintain balanced selections\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'fidelity': (1.0, WassersteinFidelity()),\n...     'balance': (0.3, CentroidBalance())\n... })\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize centroid balance component.</p>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Store the feature matrix for centroid computation.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with computed features (should be standardized for meaningful centroid distances).</p> required"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute distance from selection centroid to global center.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Euclidean distance from the selection's feature centroid to</p> <code>float</code> <p>the origin. Lower values indicate more balanced selections.</p>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance","title":"CoverageBalance","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Promotes balanced coverage by encouraging uniform responsibility.</p> <p>Uses RBF (Radial Basis Function) kernel-based soft assignment to compute how much \"responsibility\" each selected representative has for covering all candidate slices. Penalizes selections where some representatives cover many slices while others cover few.</p> <p>This is conceptually similar to cluster balance in k-medoids, ensuring no representative is over- or under-utilized.</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>RBF kernel sharpness parameter (higher = sharper assignments). Default is 1.0.</p> <code>1.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import CoverageBalance\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Ensure balanced coverage with default sharpness\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'coverage': (0.5, CoverageBalance())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sharper assignments (more cluster-like behavior)\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'coverage': (0.5, CoverageBalance(gamma=2.0))\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Softer assignments (smoother transitions)\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'coverage': (0.5, CoverageBalance(gamma=0.5))\n... })\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance.__init__","title":"__init__","text":"<pre><code>__init__(gamma: float = 1.0) -&gt; None\n</code></pre> <p>Initialize coverage balance component.</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>RBF kernel sharpness. Higher values create sharper cluster-like assignments.</p> <code>1.0</code>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Store feature matrix for responsibility computation.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with computed features.</p> required"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute L2 deviation of responsibilities from uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>L2 norm of (responsibilities - uniform). Zero indicates perfectly</p> <code>float</code> <p>balanced coverage; higher values indicate imbalance.</p>"},{"location":"api/search_algorithms/","title":"Search Algorithms","text":""},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SearchAlgorithm","title":"SearchAlgorithm","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all selection search algorithms (Pillar A).</p> <p>Defines the interface for algorithms that find optimal representative subsets. The algorithm's sole responsibility is to take a problem context and find the best selection of k items based on its internal logic and objective function.</p> <p>Different workflow types implement this protocol differently: - Generate-and-Test: Generates candidates, evaluates with ObjectiveSet, selects best - Constructive: Builds solution iteratively (e.g., k-means clustering) - Direct Optimization: Formulates and solves as single optimization problem (e.g., MILP)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class SimpleExhaustiveSearch(SearchAlgorithm):\n...     def __init__(self, objective_set: ObjectiveSet, selection_policy: SelectionPolicy, k: int):\n...         self.objective_set = objective_set\n...         self.selection_policy = selection_policy\n...         self.k = k\n...\n...     def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n...         # Generate all k-combinations\n...         from itertools import combinations\n...         all_combis = list(combinations(context.slicer.slices, self.k))\n...\n...         # Score each combination\n...         scored_combis = []\n...         for combi in all_combis:\n...             scores = self.objective_set.evaluate(context, combi)\n...             scored_combis.append((combi, scores))\n...\n...         # Select best according to policy\n...         best_combi, best_scores = self.selection_policy.select(scored_combis)\n...\n...         return RepSetResult(\n...             selection=best_combi,\n...             weights={s: 1/self.k for s in best_combi},\n...             scores=best_scores\n...         )\n...\n&gt;&gt;&gt; algorithm = SimpleExhaustiveSearch(objective_set, policy, k=4)\n&gt;&gt;&gt; result = algorithm.find_selection(context)\n&gt;&gt;&gt; print(result.selection)  # e.g., (0, 3, 6, 9) - selected slice IDs\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SearchAlgorithm.find_selection","title":"find_selection  <code>abstractmethod</code>","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Find the best subset of k representative periods.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context with df_features populated (feature engineering must be run before calling this method).</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>A RepSetResult containing the selected slice identifiers, their</p> <code>RepSetResult</code> <p>representation weights, and objective scores.</p>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenSearchAlgorithm","title":"ObjectiveDrivenSearchAlgorithm","text":"<p>               Bases: <code>SearchAlgorithm</code>, <code>ABC</code></p> <p>Base class for search algorithms guided by external objective functions.</p> <p>Provides a common structure for algorithms that rely on a user-defined ObjectiveSet to score candidates and a SelectionPolicy to choose the best. This pattern separates the search strategy from the objective function, enabling flexible algorithm design.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet, ObjectiveSpec\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; from energy_repset.selection_policies import WeightedSumPolicy\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'wasserstein': (1.0, WassersteinFidelity()),\n... })\n&gt;&gt;&gt; policy = WeightedSumPolicy()\n&gt;&gt;&gt; # See ObjectiveDrivenCombinatorialSearchAlgorithm for concrete usage\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenSearchAlgorithm.__init__","title":"__init__","text":"<pre><code>__init__(objective_set: ObjectiveSet, selection_policy: SelectionPolicy)\n</code></pre> <p>Initialize objective-driven search algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>objective_set</code> <code>ObjectiveSet</code> <p>Collection of score components defining quality metrics.</p> required <code>selection_policy</code> <code>SelectionPolicy</code> <p>Strategy for selecting best combination from scored candidates (e.g., weighted sum, Pareto dominance).</p> required"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm","title":"ObjectiveDrivenCombinatorialSearchAlgorithm","text":"<p>               Bases: <code>ObjectiveDrivenSearchAlgorithm</code></p> <p>Generate-and-test search using a combination generator (Workflow Type 1).</p> <p>Generates candidate combinations using a CombinationGenerator, scores each with the ObjectiveSet, and selects the best according to the SelectionPolicy. This is the canonical implementation of the Generate-and-Test workflow.</p> <p>Supports exhaustive search (all k-combinations) and constrained generation (e.g., seasonal quotas). Displays progress with tqdm and stores all evaluations in diagnostics for analysis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet, ObjectiveSpec,\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen,\n&gt;&gt;&gt; from energy_repset.selection_policies import WeightedSumPolicy\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity, CorrelationFidelity\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'wasserstein': (1.0, WassersteinFidelity()),\n...     'correlation': (0.5, CorrelationFidelity())\n... })\n&gt;&gt;&gt; policy = WeightedSumPolicy()\n&gt;&gt;&gt; generator = ExhaustiveCombiGen(k=4)\n&gt;&gt;&gt; algorithm = ObjectiveDrivenCombinatorialSearchAlgorithm(\n...     objective_set=objectives,\n...     selection_policy=policy,\n...     combination_generator=generator\n... )\n&gt;&gt;&gt; result = algorithm.find_selection(context, k=4)\n&gt;&gt;&gt; print(result.selection)  # Best 4-month selection\n&gt;&gt;&gt; print(result.diagnostics['evaluations_df'])  # All scored combinations\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm.__init__","title":"__init__","text":"<pre><code>__init__(objective_set: ObjectiveSet, selection_policy: SelectionPolicy, combination_generator: CombinationGenerator)\n</code></pre> <p>Initialize combinatorial search algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>objective_set</code> <code>ObjectiveSet</code> <p>Collection of score components defining quality metrics.</p> required <code>selection_policy</code> <code>SelectionPolicy</code> <p>Strategy for selecting the best combination.</p> required <code>combination_generator</code> <code>CombinationGenerator</code> <p>Defines which combinations to evaluate (e.g., all combinations, seasonal constraints).</p> required"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm.find_selection","title":"find_selection","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Find optimal selection by exhaustively scoring generated combinations.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with df_features populated.</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>RepSetResult with the winning selection, scores, representatives,</p> <code>RepSetResult</code> <p>and diagnostics containing evaluations_df with all scored combinations.</p>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm.get_all_scores","title":"get_all_scores","text":"<pre><code>get_all_scores() -&gt; DataFrame\n</code></pre> <p>Return DataFrame of all evaluated combinations with scores.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: slices, label, score_comp_1, score_comp_2, ...</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() has not been called yet.</p>"},{"location":"api/selection_policies/","title":"Selection Policies","text":""},{"location":"api/selection_policies/#energy_repset.selection_policies.SelectionPolicy","title":"SelectionPolicy","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for selection policies that choose the best combination.</p> <p>Selection policies define the strategy for choosing the winning combination from a set of scored candidates. Different policies implement different trade-offs between competing objectives (e.g., weighted sum vs. Pareto).</p> <p>This is a key component of the Generate-and-Test workflow where the SearchAlgorithm generates candidates, the ObjectiveSet scores them, and the SelectionPolicy picks the winner.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # See WeightedSumPolicy and ParetoUtopiaPolicy for concrete examples\n&gt;&gt;&gt; class SimpleMinPolicy(SelectionPolicy):\n...     def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet):\n...         # Just pick the row with minimum of first objective\n...         first_obj = list(objective_set.component_meta().keys())[0]\n...         best_row = evaluations_df.loc[evaluations_df[first_obj].idxmin()]\n...         return tuple(best_row['slices'])\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.SelectionPolicy.select_best","title":"select_best  <code>abstractmethod</code>","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select the best combination from scored candidates.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations_df</code> <code>DataFrame</code> <p>DataFrame where each row is a candidate combination with columns 'slices' (the combination tuple) and score columns for each objective component.</p> required <code>objective_set</code> <code>ObjectiveSet</code> <p>Provides metadata about score components (direction, weights, etc.) needed for selection logic.</p> required <p>Returns:</p> Type Description <code>tuple[Hashable, ...]</code> <p>Tuple of slice identifiers representing the winning combination.</p>"},{"location":"api/selection_policies/#energy_repset.selection_policies.PolicyOutcome","title":"PolicyOutcome  <code>dataclass</code>","text":""},{"location":"api/selection_policies/#energy_repset.selection_policies.WeightedSumPolicy","title":"WeightedSumPolicy","text":"<p>               Bases: <code>SelectionPolicy</code></p> <p>Selects the combination minimizing a weighted sum of objectives.</p> <p>Combines multiple objectives into a single scalar score using weighted averaging. Objectives are oriented for minimization (max objectives are negated), optionally normalized, then combined using weights from the ObjectiveSet (which can be overridden).</p> <p>This is the simplest multi-objective selection strategy and works well when relative importance of objectives is known.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset import ObjectiveSet, ObjectiveSpec\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity, CorrelationFidelity\n&gt;&gt;&gt; # Default: use weights from ObjectiveSet\n&gt;&gt;&gt; policy = WeightedSumPolicy()\n&gt;&gt;&gt; objectives = ObjectiveSet([\n...     ObjectiveSpec('wasserstein', WassersteinFidelity(), weight=1.0),\n...     ObjectiveSpec('correlation', CorrelationFidelity(), weight=0.5)\n... ])\n&gt;&gt;&gt; # Final score = 1.0*wasserstein + 0.5*correlation\n</code></pre> <pre><code>&gt;&gt;&gt; # Override weights in policy\n&gt;&gt;&gt; policy = WeightedSumPolicy(\n...     overrides={'wasserstein': 2.0, 'correlation': 1.0}\n... )\n&gt;&gt;&gt; # Final score = 2.0*wasserstein + 1.0*correlation\n</code></pre> <pre><code>&gt;&gt;&gt; # With normalization to make objectives comparable\n&gt;&gt;&gt; policy = WeightedSumPolicy(\n...     normalization='robust_minmax',  # Scale to [0, 1] using 5th-95th percentiles\n...     tie_breakers=('wasserstein',),  # Break ties by wasserstein\n...     tie_dirs=('min',)\n... )\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.WeightedSumPolicy.__init__","title":"__init__","text":"<pre><code>__init__(overrides: dict[str, float] | None = None, normalization: Normalization = 'none', tie_breakers: tuple[str, ...] = (), tie_dirs: tuple[ScoreComponentDirection, ...] = ()) -&gt; None\n</code></pre> <p>Initialize weighted sum policy.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>dict[str, float] | None</code> <p>Optional dict mapping objective names to weights, overriding weights from ObjectiveSet.</p> <code>None</code> <code>normalization</code> <code>Normalization</code> <p>How to normalize objectives before weighting: - \"none\": No normalization - \"robust_minmax\": Scale to [0, 1] using 5th-95th percentiles - \"zscore_iqr\": Z-score using median and IQR</p> <code>'none'</code> <code>tie_breakers</code> <code>tuple[str, ...]</code> <p>Tuple of objective names to use for tie-breaking.</p> <code>()</code> <code>tie_dirs</code> <code>tuple[ScoreComponentDirection, ...]</code> <p>Corresponding directions (\"min\" or \"max\") for tie-breakers.</p> <code>()</code>"},{"location":"api/selection_policies/#energy_repset.selection_policies.WeightedSumPolicy.select_best","title":"select_best","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select combination with minimum weighted sum score.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations_df</code> <code>DataFrame</code> <p>DataFrame with 'slices' column and objective scores.</p> required <code>objective_set</code> <code>ObjectiveSet</code> <p>Provides component metadata (direction, weights).</p> required <p>Returns:</p> Type Description <code>tuple[Hashable, ...]</code> <p>Tuple of slice identifiers with the lowest weighted sum score.</p>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoMaxMinStrategy","title":"ParetoMaxMinStrategy","text":"<p>               Bases: <code>ParetoUtopiaPolicy</code></p>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoMaxMinStrategy.select_best","title":"select_best","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select best solution using Pareto max-min approach.</p>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoUtopiaPolicy","title":"ParetoUtopiaPolicy","text":"<p>               Bases: <code>SelectionPolicy</code></p>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoUtopiaPolicy.select_best","title":"select_best","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select best solution using Pareto utopia approach.</p>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoOutcome","title":"ParetoOutcome  <code>dataclass</code>","text":"<p>               Bases: <code>PolicyOutcome</code></p>"},{"location":"api/workflow/","title":"Workflow &amp; Experiment","text":""},{"location":"api/workflow/#energy_repset.workflow.Workflow","title":"Workflow  <code>dataclass</code>","text":"<p>A serializable object that defines a complete selection problem.</p> <p>This dataclass encapsulates all components needed to execute a representative subset selection workflow: feature engineering, search algorithm, representation model, and the target number of periods to select.</p> <p>Attributes:</p> Name Type Description <code>feature_engineer</code> <code>FeatureEngineer</code> <p>Component that transforms raw time-series into features.</p> <code>search_algorithm</code> <code>SearchAlgorithm</code> <p>Algorithm that finds the optimal subset of k periods.</p> <code>representation_model</code> <code>RepresentationModel</code> <p>Model that calculates responsibility weights for selected periods.</p> <code>k</code> <code>RepresentationModel</code> <p>Number of representative periods to select.</p> <p>Examples:</p> <p>Define a complete workflow:</p> <pre><code>&gt;&gt;&gt; from energy_repset.workflow import Workflow\n&gt;&gt;&gt; from energy_repset.feature_engineering import StandardStatsFeatureEngineer\n&gt;&gt;&gt; from energy_repset.search_algorithms import ObjectiveDrivenCombinatorialSearchAlgorithm\n&gt;&gt;&gt; from energy_repset.representation import UniformRepresentationModel\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; from energy_repset.selection_policies import ParetoMaxMinStrategy\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create components\n&gt;&gt;&gt; feature_eng = StandardStatsFeatureEngineer()\n&gt;&gt;&gt; objective_set = ObjectiveSet({'wass': (1.0, WassersteinFidelity())})\n&gt;&gt;&gt; policy = ParetoMaxMinStrategy()\n&gt;&gt;&gt; combi_gen = ExhaustiveCombiGen(k=3)\n&gt;&gt;&gt; search_algo = ObjectiveDrivenCombinatorialSearchAlgorithm(\n...     objective_set, policy, combi_gen\n... )\n&gt;&gt;&gt; repr_model = UniformRepresentationModel()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create workflow\n&gt;&gt;&gt; workflow = Workflow(\n...     feature_engineer=feature_eng,\n...     search_algorithm=search_algo,\n...     representation_model=repr_model,\n... )\n</code></pre>"},{"location":"api/workflow/#energy_repset.workflow.Workflow.save","title":"save","text":"<pre><code>save(filepath: str | Path)\n</code></pre> <p>Save workflow configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path where workflow configuration will be saved.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Workflow serialization is not yet implemented.</p>"},{"location":"api/workflow/#energy_repset.workflow.Workflow.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: str | Path) -&gt; 'Workflow'\n</code></pre> <p>Load workflow configuration from file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to workflow configuration file.</p> required <p>Returns:</p> Name Type Description <code>Workflow</code> <code>'Workflow'</code> <p>Reconstructed Workflow instance.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Workflow deserialization is not yet implemented.</p>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment","title":"RepSetExperiment","text":"<p>Orchestrate a complete and self-contained representative subset experiment.</p> <p>This class manages the execution of a full workflow from raw data to final selection results. It handles feature engineering, search execution, and weight calculation while maintaining references to intermediate states.</p> <p>Attributes:</p> Name Type Description <code>raw_context</code> <p>Initial ProblemContext containing raw time-series data.</p> <code>workflow</code> <p>Workflow definition containing all algorithm components.</p> <code>result</code> <code>RepSetResult</code> <p>Final RepSetResult after run() completes (None before execution).</p> <p>Examples:</p> <p>Run a complete experiment:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.problem import RepSetExperiment\n&gt;&gt;&gt; from energy_repset.context import ProblemContext\n&gt;&gt;&gt; from energy_repset.workflow import Workflow\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; # ... (imports for feature engineer, search algo, etc.)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data and context\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; df = pd.DataFrame({'demand': np.random.rand(8760)}, index=dates)\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; context = ProblemContext(df_raw=df, slicer=slicer)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create workflow (see Workflow docs for details)\n&gt;&gt;&gt; workflow = Workflow(\n...     feature_engineer=feature_eng,\n...     search_algorithm=search_algo,\n...     representation_model=repr_model,\n...     k=3\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run experiment\n&gt;&gt;&gt; experiment = RepSetExperiment(context, workflow)\n&gt;&gt;&gt; result = experiment.run()\n&gt;&gt;&gt; print(result.selection)  # Selected periods\n&gt;&gt;&gt; print(result.weights)    # Responsibility weights\n</code></pre>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.feature_context","title":"feature_context  <code>property</code>","text":"<pre><code>feature_context: ProblemContext\n</code></pre> <p>Get the context with computed features.</p> <p>Returns:</p> Type Description <code>ProblemContext</code> <p>ProblemContext with df_features populated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run() or run_feature_engineer() has not been called yet.</p>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.__init__","title":"__init__","text":"<pre><code>__init__(context: ProblemContext, workflow: Workflow)\n</code></pre> <p>Initialize experiment with raw data context and workflow.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>ProblemContext containing raw time-series data and metadata.</p> required <code>workflow</code> <code>Workflow</code> <p>Workflow defining feature engineering, search, and representation.</p> required"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.run_feature_engineer","title":"run_feature_engineer","text":"<pre><code>run_feature_engineer() -&gt; ProblemContext\n</code></pre> <p>Run only the feature engineering step.</p> <p>This method allows you to inspect features before running the full workflow.</p> <p>Returns:</p> Type Description <code>ProblemContext</code> <p>ProblemContext with df_features populated.</p>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.run","title":"run","text":"<pre><code>run() -&gt; RepSetResult\n</code></pre> <p>Execute the entire workflow from feature engineering to final result.</p> <p>This method orchestrates the complete selection process: 1. Runs the feature engineer to create a new, feature-rich context 2. Stores this feature_context for user inspection 3. Runs the search algorithm on the feature_context 4. Fits the representation model 5. Calculates the final weights 6. Stores and returns the final result</p> <p>Returns:</p> Name Type Description <code>RepSetResult</code> <code>RepSetResult</code> <p>The selected periods, weights, scores, and diagnostics.</p>"},{"location":"api/workflow/#energy_repset.results.RepSetResult","title":"RepSetResult  <code>dataclass</code>","text":"<p>The standardized output object.</p>"},{"location":"gallery/","title":"Example Gallery","text":"<p>Each example script produces interactive Plotly visualizations saved as HTML. Run the scripts to regenerate outputs in <code>docs/gallery/</code>.</p>"},{"location":"gallery/#example-1-getting-started","title":"Example 1: Getting Started","text":"<p>The simplest possible end-to-end workflow. Selects 4 representative months using a single objective and uniform weights. A minimal \"hello world\" for onboarding.</p> <p>Components: <code>StandardStatsFeatureEngineer</code> | <code>WassersteinFidelity</code> | <code>WeightedSumPolicy</code> | <code>UniformRepresentationModel</code> | <code>ExhaustiveCombiGen(k=4)</code></p> <p>View details -&gt;</p>"},{"location":"gallery/#example-2-feature-space-exploration","title":"Example 2: Feature Space Exploration","text":"<p>A comprehensive workflow with monthly slicing, PCA feature engineering, Pareto selection, and KMedoids cluster-size representation. Showcases the full range of feature-space diagnostics (scatter plots, correlation heatmap, PCA variance, feature distributions) and score-component diagnostics (ECDF overlay, correlation difference, diurnal profiles).</p> <p>Components: <code>FeaturePipeline</code> (Stats + PCA) | <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> + <code>CentroidBalance</code> | <code>ParetoMaxMinStrategy</code> | <code>KMedoidsClustersizeRepresentation</code> | <code>ExhaustiveCombiGen(k=3)</code></p> <p>View details -&gt;</p>"},{"location":"gallery/#example-3-hierarchical-seasonal-selection","title":"Example 3: Hierarchical Seasonal Selection","text":"<p>Selects 4 months (one per season) using day-level features and hierarchical combination generation. Demonstrates seasonal constraints via <code>GroupQuotaHierarchicalCombiGen</code> and Pareto front visualizations including parallel coordinates.</p> <p>Components: <code>StandardStatsFeatureEngineer</code> (daily) | <code>GroupQuotaHierarchicalCombiGen</code> | <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> | <code>ParetoMaxMinStrategy</code> | <code>KMedoidsClustersizeRepresentation</code></p> <p>View details -&gt;</p>"},{"location":"gallery/#example-4-comparing-representation-models","title":"Example 4: Comparing Representation Models","text":"<p>Runs a single search and then applies three different representation models to the same winning selection, comparing how each distributes responsibility across the selected months.</p> <p>Components: <code>FeaturePipeline</code> (Stats + PCA) | <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> | <code>WeightedSumPolicy</code> | <code>ExhaustiveCombiGen(k=3)</code> | <code>UniformRepresentationModel</code> + <code>KMedoidsClustersizeRepresentation</code> + <code>BlendedRepresentationModel</code></p> <p>View details -&gt;</p>"},{"location":"gallery/#example-5-multi-objective-exploration","title":"Example 5: Multi-Objective Exploration","text":"<p>Demonstrates how different score components and selection policies affect the outcome. Uses 4 objectives and compares <code>ParetoMaxMinStrategy</code> vs <code>WeightedSumPolicy</code>.</p> <p>Components: <code>FeaturePipeline</code> (Stats + PCA) | <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> + <code>DurationCurveFidelity</code> + <code>DiversityReward</code> | <code>ParetoMaxMinStrategy</code> vs <code>WeightedSumPolicy</code> | <code>UniformRepresentationModel</code> | <code>ExhaustiveCombiGen(k=3)</code></p> <p>View details -&gt;</p>"},{"location":"gallery/#running-the-examples","title":"Running the Examples","text":"<pre><code># Install the package\npip install -e .\n\n# Run any example\npython examples/ex1_getting_started.py\npython examples/ex2_feature_space.py\npython examples/ex3_hierarchical_selection.py\npython examples/ex4_representation_models.py\npython examples/ex5_multi_objective.py\n</code></pre> <p>Output HTML files are written to <code>docs/gallery/&lt;example&gt;/</code> and can be opened directly in a browser or served via <code>mkdocs serve</code>.</p>"},{"location":"gallery/ex1/","title":"Example 1: Getting Started","text":"<p>The simplest possible end-to-end workflow. Selects 4 representative months from a year of hourly time-series data using a single objective (Wasserstein fidelity), a weighted-sum policy, and uniform weights. A minimal \"hello world\" for onboarding.</p> <p>Script: <code>examples/ex1_getting_started.py</code></p> Pillar Component F <code>StandardStatsFeatureEngineer</code> O <code>WassersteinFidelity</code> S <code>ExhaustiveCombiGen(k=4)</code> R <code>UniformRepresentationModel</code> A <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> with <code>WeightedSumPolicy</code>"},{"location":"gallery/ex1/#visualizations","title":"Visualizations","text":""},{"location":"gallery/ex1/#responsibility-weights","title":"Responsibility Weights","text":"<p>Uniform 1/k weights for each selected month.</p>"},{"location":"gallery/ex2/","title":"Example 2: Feature Space Exploration","text":"<p>A comprehensive workflow with monthly slicing, PCA feature engineering, Pareto selection, and KMedoids cluster-size representation. Showcases the full range of feature-space diagnostics and score-component diagnostics.</p> <p>Script: <code>examples/ex2_feature_space.py</code></p> Pillar Component F <code>FeaturePipeline</code> (Stats + PCA) O <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> + <code>CentroidBalance</code> S <code>ExhaustiveCombiGen(k=3)</code> R <code>KMedoidsClustersizeRepresentation</code> A <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> with <code>ParetoMaxMinStrategy</code>"},{"location":"gallery/ex2/#visualizations","title":"Visualizations","text":""},{"location":"gallery/ex2/#feature-space","title":"Feature Space","text":""},{"location":"gallery/ex2/#pca-variance-explained","title":"PCA Variance Explained","text":"<p>Cumulative variance captured by each principal component.</p>"},{"location":"gallery/ex2/#2d-feature-scatter-pc0-vs-pc1","title":"2D Feature Scatter (PC0 vs PC1)","text":"<p>Months plotted in the first two principal components.</p>"},{"location":"gallery/ex2/#3d-pca-projection","title":"3D PCA Projection","text":"<p>Interactive 3D view of the first three principal components.</p>"},{"location":"gallery/ex2/#feature-correlation-heatmap","title":"Feature Correlation Heatmap","text":"<p>Pearson correlations between statistical features.</p>"},{"location":"gallery/ex2/#scatter-matrix-first-4-pcs","title":"Scatter Matrix (First 4 PCs)","text":"<p>Pairwise scatter plots of the first four principal components.</p>"},{"location":"gallery/ex2/#feature-distributions","title":"Feature Distributions","text":"<p>Histograms of each feature column.</p>"},{"location":"gallery/ex2/#results","title":"Results","text":""},{"location":"gallery/ex2/#feature-space-with-selection","title":"Feature Space with Selection","text":"<p>Selected months highlighted in the PC0-PC1 plane.</p>"},{"location":"gallery/ex2/#responsibility-weights","title":"Responsibility Weights","text":"<p>Cluster-proportional weights from KMedoids representation.</p>"},{"location":"gallery/ex2/#score-component-diagnostics","title":"Score Component Diagnostics","text":""},{"location":"gallery/ex2/#ecdf-load","title":"ECDF: Load","text":"<p>Empirical CDF comparison of load between full year and selection.</p>"},{"location":"gallery/ex2/#correlation-difference","title":"Correlation Difference","text":"<p>Heatmap of the correlation matrix difference (selection minus full).</p>"},{"location":"gallery/ex2/#diurnal-profiles","title":"Diurnal Profiles","text":"<p>Hour-of-day profiles comparing full year and selection for all variables.</p>"},{"location":"gallery/ex3/","title":"Example 3: Hierarchical Seasonal Selection","text":"<p>Selects 4 months (one per season) using day-level features and hierarchical combination generation. Demonstrates seasonal constraints via <code>GroupQuotaHierarchicalCombiGen</code> and Pareto front visualizations.</p> <p>Script: <code>examples/ex3_hierarchical_selection.py</code></p> Pillar Component F <code>StandardStatsFeatureEngineer</code> (daily features) O <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> S <code>GroupQuotaHierarchicalCombiGen</code> (1 month per season) R <code>KMedoidsClustersizeRepresentation</code> A <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> with <code>ParetoMaxMinStrategy</code>"},{"location":"gallery/ex3/#visualizations","title":"Visualizations","text":""},{"location":"gallery/ex3/#results","title":"Results","text":""},{"location":"gallery/ex3/#responsibility-weights","title":"Responsibility Weights","text":"<p>Cluster-proportional weights for the selected daily slices.</p>"},{"location":"gallery/ex3/#pareto-front-2d","title":"Pareto Front (2D)","text":"<p>Trade-off between Wasserstein distance and correlation fidelity.</p>"},{"location":"gallery/ex3/#pareto-parallel-coordinates","title":"Pareto Parallel Coordinates","text":"<p>All objectives shown as parallel axes.</p>"},{"location":"gallery/ex3/#score-contributions","title":"Score Contributions","text":"<p>Normalized contributions of each score component.</p>"},{"location":"gallery/ex3/#feature-space","title":"Feature Space","text":""},{"location":"gallery/ex3/#feature-scatter-with-selection","title":"Feature Scatter with Selection","text":"<p>First two feature columns with selected days highlighted.</p>"},{"location":"gallery/ex3/#distribution-fidelity-ecdf","title":"Distribution Fidelity (ECDF)","text":""},{"location":"gallery/ex3/#load","title":"Load","text":""},{"location":"gallery/ex3/#onshore-wind","title":"Onshore Wind","text":""},{"location":"gallery/ex3/#offshore-wind","title":"Offshore Wind","text":""},{"location":"gallery/ex3/#solar","title":"Solar","text":""},{"location":"gallery/ex4/","title":"Example 4: Comparing Representation Models","text":"<p>Runs a single search to find the best 3-month selection, then applies three different representation models to the same winning selection: Uniform, KMedoids cluster-size, and Blended (soft assignment). Illustrates the R (representation) pillar of the framework.</p> <p>Script: <code>examples/ex4_representation_models.py</code></p> Pillar Component F <code>FeaturePipeline</code> (Stats + PCA) O <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> S <code>ExhaustiveCombiGen(k=3)</code> R <code>UniformRepresentationModel</code> + <code>KMedoidsClustersizeRepresentation</code> + <code>BlendedRepresentationModel</code> A <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> with <code>WeightedSumPolicy</code>"},{"location":"gallery/ex4/#visualizations","title":"Visualizations","text":""},{"location":"gallery/ex4/#responsibility-weights","title":"Responsibility Weights","text":""},{"location":"gallery/ex4/#uniform-weights","title":"Uniform Weights","text":"<p>Equal 1/k weights for each selected month.</p>"},{"location":"gallery/ex4/#kmedoids-cluster-size-weights","title":"KMedoids Cluster-Size Weights","text":"<p>Weights proportional to the number of months assigned to each cluster.</p>"},{"location":"gallery/ex4/#blended-weights-aggregated","title":"Blended Weights (Aggregated)","text":"<p>Soft assignment weights aggregated and normalized to sum to 1.0.</p>"},{"location":"gallery/ex4/#blended-weight-matrix","title":"Blended Weight Matrix","text":"<p>Heatmap of the full weight matrix: rows are all months, columns are representatives.</p>"},{"location":"gallery/ex4/#feature-space","title":"Feature Space","text":""},{"location":"gallery/ex4/#feature-scatter-with-selection","title":"Feature Scatter with Selection","text":"<p>Selected months highlighted in PCA space (PC0 vs PC1).</p>"},{"location":"gallery/ex4/#distribution-fidelity-ecdf","title":"Distribution Fidelity (ECDF)","text":""},{"location":"gallery/ex4/#load","title":"Load","text":""},{"location":"gallery/ex4/#onshore-wind","title":"Onshore Wind","text":""},{"location":"gallery/ex4/#offshore-wind","title":"Offshore Wind","text":""},{"location":"gallery/ex4/#solar","title":"Solar","text":""},{"location":"gallery/ex5/","title":"Example 5: Multi-Objective Exploration","text":"<p>Demonstrates how different score components and selection policies affect the outcome. Uses 4 objectives (Wasserstein, Correlation, Duration Curve, Diversity) and compares <code>ParetoMaxMinStrategy</code> vs <code>WeightedSumPolicy</code>.</p> <p>Script: <code>examples/ex5_multi_objective.py</code></p> Pillar Component F <code>FeaturePipeline</code> (Stats + PCA) O <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> + <code>DurationCurveFidelity</code> + <code>DiversityReward</code> S <code>ExhaustiveCombiGen(k=3)</code> R <code>UniformRepresentationModel</code> A <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> with <code>ParetoMaxMinStrategy</code> and <code>WeightedSumPolicy</code>"},{"location":"gallery/ex5/#visualizations","title":"Visualizations","text":""},{"location":"gallery/ex5/#pareto-front","title":"Pareto Front","text":""},{"location":"gallery/ex5/#2d-scatter-wasserstein-vs-correlation","title":"2D Scatter (Wasserstein vs Correlation)","text":"<p>Trade-off frontier between distribution fidelity and correlation preservation.</p>"},{"location":"gallery/ex5/#scatter-matrix","title":"Scatter Matrix","text":"<p>Pairwise objective-space scatter matrix for all 4 objectives.</p>"},{"location":"gallery/ex5/#score-contributions","title":"Score Contributions","text":""},{"location":"gallery/ex5/#paretomaxminstrategy","title":"ParetoMaxMinStrategy","text":""},{"location":"gallery/ex5/#weightedsumpolicy","title":"WeightedSumPolicy","text":""},{"location":"gallery/ex5/#responsibility-weights","title":"Responsibility Weights","text":""},{"location":"gallery/ex5/#paretomaxminstrategy_1","title":"ParetoMaxMinStrategy","text":""},{"location":"gallery/ex5/#weightedsumpolicy_1","title":"WeightedSumPolicy","text":""},{"location":"gallery/ex5/#distribution-overlays-pareto-selection","title":"Distribution Overlays (Pareto Selection)","text":""},{"location":"gallery/ex5/#load","title":"Load","text":""},{"location":"gallery/ex5/#onshore-wind","title":"Onshore Wind","text":""},{"location":"gallery/ex5/#offshore-wind","title":"Offshore Wind","text":""},{"location":"gallery/ex5/#solar","title":"Solar","text":""},{"location":"gallery/ex5/#feature-distributions","title":"Feature Distributions","text":"<p>Histograms of all feature columns.</p>"},{"location":"gallery/ex5/#diurnal-profiles","title":"Diurnal Profiles","text":"<p>Hour-of-day profiles comparing full year and Pareto selection.</p>"},{"location":"gallery/ex5/#correlation-difference","title":"Correlation Difference","text":"<p>Heatmap of the correlation matrix difference (selection minus full).</p>"}]}