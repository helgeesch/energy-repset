{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"energy-repset","text":"<p>A unified, modular framework for representative subset selection in multi-variate time-series spaces.</p>"},{"location":"#why-this-package","title":"Why this package?","text":"<p>Energy system models, capacity expansion studies, and other time-series-heavy applications often need to reduce a full year (or longer) of hourly data to a small set of representative periods -- days, weeks, or months -- without losing what matters. The literature offers many methods (k-means, k-medoids, MILP-based selection, genetic algorithms, etc.), but the landscape is dense and tangled: each method bundles multiple decisions -- how to represent data, what to optimize, how to search -- into a single procedure, making it hard to see which choices matter, compare approaches on equal footing, or adapt a method to your specific problem.</p> <p><code>energy-repset</code> clears a path through the jungle in two ways:</p> <ol> <li> <p>A unified framework that decomposes any representative period selection method into five interchangeable components. Every established methodology is a specific instantiation of this structure. The framework provides a common language for describing, comparing, and assembling methods. The full theoretical treatment is available in the Unified Framework document.</p> </li> <li> <p>A modular Python package that implements this framework as a library of composable, protocol-based modules. You pick one implementation per component, wire them together, and run. Adding a new algorithm or score metric means implementing a single protocol -- everything else stays the same.</p> </li> </ol>"},{"location":"#the-five-components","title":"The Five Components","text":"Component Symbol Role Feature Space F How raw time-series are transformed into comparable representations Objective O How candidate selections are scored for quality Selection Space S What is being selected (historical subsets, synthetic archetypes, etc.) Representation Model R How selected periods represent the full dataset Search Algorithm A The engine that finds optimal selections"},{"location":"#navigating-the-project","title":"Navigating the project","text":"<p>Website: energy-repset.mesqual.io</p> <p>Documentation site (energy-repset-docs.mesqual.io):</p> Section What you'll find Unified Framework The theoretical paper: problem decomposition, component taxonomy, method comparison Workflow Types The three workflow patterns: generate-and-test, constructive, direct optimization Modules &amp; Components Inventory of all implemented modules and how they map to the five components Configuration Advisor Decision guide for choosing components based on your problem Getting Started End-to-end walkthrough from data to result Examples Worked examples showcasing different configurations API Reference Auto-generated class and method documentation <p>Package structure (<code>energy_repset/</code>):</p> Module Framework component <code>context</code>, <code>time_slicer</code> Problem definition and data container <code>feature_engineering/</code> F -- Feature engineers (statistical summaries, PCA, pipelines) <code>objectives</code>, <code>score_components/</code> O -- Objective sets and scoring metrics <code>combi_gens/</code> S -- Combination generators (exhaustive, group-quota, hierarchical) <code>representation/</code> R -- Representation models (uniform, cluster-based, blended) <code>search_algorithms/</code>, <code>selection_policies/</code> A -- Search algorithms and selection policies <code>workflow</code>, <code>problem</code>, <code>results</code> Orchestration: wire components, run, collect results <code>diagnostics/</code> Visualization and analysis of features, scores, and results"},{"location":"#installation","title":"Installation","text":"<p>Option 1 -- Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/mesqual/energy-repset.git\n</code></pre> <p>Option 2 -- Clone and install in editable mode:</p> <pre><code>git clone https://github.com/mesqual/energy-repset.git\ncd energy-repset\npip install -e .\n</code></pre> <p>Option 3 -- Add as a Git submodule (useful for monorepos):</p> <pre><code>git submodule add https://github.com/mesqual/energy-repset.git\npip install -e energy-repset\n</code></pre> <p>Alternatively, skip the install and mark the <code>energy-repset</code> directory as a source root in your IDE so that <code>import energy_repset</code> resolves directly.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import pandas as pd\nimport energy_repset as rep\n\n# Load hourly time-series data (columns = variables, index = datetime)\ndf_raw = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=True)\n\n# Define problem: slice the year into monthly candidate periods\nslicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw, slicer)\n\n# Feature engineering: statistical summaries per month\nfeature_engineer = rep.StandardStatsFeatureEngineer()\n\n# Objective: score each candidate selection on distribution fidelity\nobjective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n})\n\n# Search: evaluate all 4-of-12 monthly combinations\npolicy = rep.WeightedSumPolicy()\ncombi_gen = rep.ExhaustiveCombiGen(k=4)\nsearch = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\n\n# Representation: equal 1/k weights per selected month\nrepresentation = rep.UniformRepresentationModel()\n\n# Assemble and run\nworkflow = rep.Workflow(feature_engineer, search, representation)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n\nprint(result.selection)  # e.g., (Period('2019-01', 'M'), Period('2019-04', 'M'), ...)\nprint(result.weights)    # e.g., {Period('2019-01', 'M'): 3.0, ...}\nprint(result.scores)     # e.g., {'wasserstein': 0.023, 'correlation': 0.015}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Apache-2.0</p>"},{"location":"advisor/","title":"Configuration Advisor","text":"<p>This document serves a dual purpose:</p> <ol> <li>Human guide -- a structured decision tree for choosing energy-repset components.</li> <li>AI system prompt -- a self-contained reference an LLM can use to interactively guide users through configuration.</li> </ol> <p>For theory, see Unified Framework. For API details, see Modules &amp; Components.</p>"},{"location":"advisor/#component-catalog","title":"Component Catalog","text":""},{"location":"advisor/#f-feature-engineering","title":"F: Feature Engineering","text":"Class Import Description <code>StandardStatsFeatureEngineer</code> <code>energy_repset.feature_engineering</code> Statistical summaries per slice (mean, std, IQR, quantiles, ramp rates). Z-score normalized. <code>PCAFeatureEngineer</code> <code>energy_repset.feature_engineering</code> PCA dimensionality reduction. Supports variance-threshold or fixed component count. <code>DirectProfileFeatureEngineer</code> <code>energy_repset.feature_engineering</code> Flattened raw hourly profiles per slice. Used by Snippet and DTW-based methods. <code>FeaturePipeline</code> <code>energy_repset.feature_engineering</code> Chains multiple engineers sequentially. <p>Typical pipeline: <code>StandardStatsFeatureEngineer</code> -&gt; <code>PCAFeatureEngineer</code> (via <code>FeaturePipeline</code>). Direct profiles: <code>DirectProfileFeatureEngineer</code> for algorithms that compare raw time-series shapes.</p>"},{"location":"advisor/#o-score-components","title":"O: Score Components","text":"<p>All components implement the <code>ScoreComponent</code> protocol with <code>prepare(context)</code> and <code>score(combination)</code>.</p> Class Direction What it Measures Import <code>WassersteinFidelity</code> min Marginal distribution similarity (Wasserstein distance, IQR-normalized) <code>energy_repset.score_components</code> <code>CorrelationFidelity</code> min Cross-variable correlation preservation (Frobenius norm) <code>energy_repset.score_components</code> <code>DurationCurveFidelity</code> min Duration curve match (quantile-based NRMSE) <code>energy_repset.score_components</code> <code>NRMSEFidelity</code> min Duration curve match (full interpolation NRMSE) <code>energy_repset.score_components</code> <code>DiurnalFidelity</code> min Hour-of-day profile preservation (normalized MSE) <code>energy_repset.score_components</code> <code>DiurnalDTWFidelity</code> min Hour-of-day profile preservation (DTW distance) <code>energy_repset.score_components</code> <code>DTWFidelity</code> min Full series shape similarity (Dynamic Time Warping) <code>energy_repset.score_components</code> <code>DiversityReward</code> max Spread in feature space (avg pairwise distance) <code>energy_repset.score_components</code> <code>CentroidBalance</code> min Feature centroid deviation from global mean <code>energy_repset.score_components</code> <code>CoverageBalance</code> min Balanced coverage via RBF kernel soft assignment <code>energy_repset.score_components</code> <p>Components are bundled into an <code>ObjectiveSet</code> (<code>energy_repset.objectives</code>) with per-component weights.</p>"},{"location":"advisor/#s-combination-generators","title":"S: Combination Generators","text":"Class Import Description <code>ExhaustiveCombiGen</code> <code>energy_repset.combi_gens</code> All k-of-n combinations. <code>GroupQuotaCombiGen</code> <code>energy_repset.combi_gens</code> Exact quotas per group (e.g., 1 per season). <code>ExhaustiveHierarchicalCombiGen</code> <code>energy_repset.combi_gens</code> Selects parent groups, evaluates on child slices. <code>GroupQuotaHierarchicalCombiGen</code> <code>energy_repset.combi_gens</code> Hierarchical + group quotas. Has <code>from_slicers_with_seasons()</code> factory."},{"location":"advisor/#r-representation-models","title":"R: Representation Models","text":"Class Import Description <code>UniformRepresentationModel</code> <code>energy_repset.representation</code> Equal 1/k weights. Returns <code>dict</code>. <code>KMedoidsClustersizeRepresentation</code> <code>energy_repset.representation</code> Cluster-proportional weights via k-medoids. Returns <code>dict</code>. <code>BlendedRepresentationModel</code> <code>energy_repset.representation</code> Soft assignment (convex combination). Returns weight <code>DataFrame</code>."},{"location":"advisor/#a-search-algorithms","title":"A: Search Algorithms","text":"Class Workflow Import <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> Generate-and-Test <code>energy_repset.search_algorithms</code> <code>HullClusteringSearch</code> Constructive <code>energy_repset.search_algorithms</code> <code>CTPCSearch</code> Constructive <code>energy_repset.search_algorithms</code> <code>SnippetSearch</code> Constructive <code>energy_repset.search_algorithms</code> <p>Not yet implemented: Direct Optimization (MILP).</p> <p>See Constructive Algorithms for algorithm details and paper references.</p>"},{"location":"advisor/#pi-selection-policies","title":"Pi: Selection Policies","text":"Class Import Description <code>WeightedSumPolicy</code> <code>energy_repset.selection_policies</code> Scalar aggregation. Supports <code>normalization='robust_minmax'</code>. <code>ParetoMaxMinStrategy</code> <code>energy_repset.selection_policies</code> Pareto-optimal solution maximizing worst objective. <code>ParetoUtopiaPolicy</code> <code>energy_repset.selection_policies</code> Pareto-optimal solution closest to utopia point."},{"location":"advisor/#decision-tree","title":"Decision Tree","text":""},{"location":"advisor/#step-1-understand-your-data","title":"Step 1: Understand your data","text":"<p>Ask yourself:</p> <ul> <li>Resolution: Hourly? 15-minute? Daily?</li> <li>Variables: How many time-series (load, wind, solar, prices, ...)?</li> <li>Regions: How many regions or zones? Each region-variable pair adds dimensions.</li> <li>Horizon: One year? Multiple years?</li> <li>Candidate count: How many slices does your <code>TimeSlicer</code> produce?</li> <li>12 months -&gt; C(12,3) = 220 candidates for k=3</li> <li>52 weeks -&gt; C(52,8) = 752 million candidates for k=8</li> <li>Feature dimensionality: How many features will your feature space produce? With V variables across R regions, <code>StandardStatsFeatureEngineer</code> produces multiple statistics per variable (mean, std, quantiles, ramp rates, etc.), so the total feature count grows as V x R x (number of statistics). If this count is high relative to the number of slices, consider PCA dimensionality reduction via <code>FeaturePipeline</code>.</li> </ul> <p>This determines whether exhaustive search is feasible or you need constrained/hierarchical generation.</p>"},{"location":"advisor/#step-1b-normalization-and-weighting","title":"Step 1b: Normalization and weighting","text":"<ul> <li>Normalization: <code>StandardStatsFeatureEngineer</code> z-score normalizes features by default. If using <code>DirectProfileFeatureEngineer</code>, your variables may have very different scales (e.g., MW demand vs. capacity factors between 0 and 1), and distance-based methods will be dominated by the high-magnitude variables. Consider whether explicit scaling is needed.</li> <li>Feature/variable weighting: If certain variables matter more for the downstream model (e.g., load is more critical than temperature), consider using <code>variable_weights</code> in score components such as <code>WassersteinFidelity(variable_weights=...)</code> or passing importance weights through the feature engineer. This lets the selection process prioritize fidelity on the variables that matter most.</li> </ul>"},{"location":"advisor/#step-2-downstream-model-constraints","title":"Step 2: Downstream model constraints","text":"<p>Your energy system model may impose constraints on the representation:</p> Constraint Implication Model requires equal-length periods with scalar weights Use <code>UniformRepresentationModel</code> or <code>KMedoidsClustersizeRepresentation</code> Model can accept blended inputs (e.g., weighted hourly profiles) Use <code>BlendedRepresentationModel</code> Must cover all seasons Use <code>GroupQuotaCombiGen</code> or <code>GroupQuotaHierarchicalCombiGen</code> Must preserve temporal coupling within periods (e.g., multi-day storage) Prefer weekly/multi-day slicing over monthly"},{"location":"advisor/#step-3-computational-budget","title":"Step 3: Computational budget","text":"Candidate space size Recommended generator &lt; 10,000 <code>ExhaustiveCombiGen</code> -- evaluate all 10,000 -- 1,000,000 <code>GroupQuotaCombiGen</code> to constrain, or hierarchical generators &gt; 1,000,000 Hierarchical generators, or future genetic/constructive algorithms <p>Hierarchical trick: Select at the month level (small combinatorial space) but evaluate on day-level features (high resolution). Use <code>ExhaustiveHierarchicalCombiGen</code> or <code>GroupQuotaHierarchicalCombiGen</code>.</p>"},{"location":"advisor/#step-4-quality-goals","title":"Step 4: Quality goals","text":"<p>Choose score components based on what matters for your downstream model:</p> Goal Recommended Components Preserve marginal distributions (load duration curves) <code>WassersteinFidelity</code>, <code>DurationCurveFidelity</code>, <code>NRMSEFidelity</code> Preserve variable correlations (wind-solar complementarity) <code>CorrelationFidelity</code> Preserve diurnal patterns (solar noon peak, evening ramp) <code>DiurnalFidelity</code>, <code>DiurnalDTWFidelity</code> Preserve overall time-series shape <code>DTWFidelity</code> Ensure diverse representatives (avoid redundancy) <code>DiversityReward</code> Balanced coverage of the feature space <code>CentroidBalance</code>, <code>CoverageBalance</code> <p>Start simple: <code>WassersteinFidelity</code> + <code>CorrelationFidelity</code> covers most needs. Add more components only if you observe specific deficiencies in the results.</p>"},{"location":"advisor/#step-5-selection-policy","title":"Step 5: Selection policy","text":"Situation Recommended Policy Single objective or clear priority ranking <code>WeightedSumPolicy</code> (default) Multiple objectives, want balanced trade-off <code>WeightedSumPolicy(normalization='robust_minmax')</code> Multiple objectives, want to avoid worst-case failure <code>ParetoMaxMinStrategy</code> Multiple objectives, want closest to ideal <code>ParetoUtopiaPolicy</code>"},{"location":"advisor/#common-configurations","title":"Common Configurations","text":"<p>All examples below assume <code>import energy_repset as rep</code>.</p>"},{"location":"advisor/#minimal-single-objective-monthly-selection","title":"Minimal: single-objective monthly selection","text":"<pre><code>import energy_repset as rep\n\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n        rep.ObjectiveSet({'wass': (1.0, rep.WassersteinFidelity())}),\n        rep.WeightedSumPolicy(),\n        rep.ExhaustiveCombiGen(k=4),\n    ),\n    representation_model=rep.UniformRepresentationModel(),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#multi-objective-with-pca-features","title":"Multi-objective with PCA features","text":"<pre><code>feature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n\nobjective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n    'diversity':   (0.5, rep.DiversityReward()),\n})\n\nworkflow = rep.Workflow(\n    feature_engineer=feature_pipeline,\n    search_algorithm=rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n        objective_set, rep.ParetoMaxMinStrategy(), rep.ExhaustiveCombiGen(k=3),\n    ),\n    representation_model=rep.KMedoidsClustersizeRepresentation(),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#seasonal-constraints-with-hierarchical-search","title":"Seasonal constraints with hierarchical search","text":"<pre><code>child_slicer = rep.TimeSlicer(unit=\"day\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=child_slicer)\n\ncombi_gen = rep.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n    parent_k=4,\n    dt_index=df_raw.index,\n    child_slicer=child_slicer,\n    group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1},\n)\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n        objective_set, rep.WeightedSumPolicy(), combi_gen,\n    ),\n    representation_model=rep.KMedoidsClustersizeRepresentation(),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#blended-soft-representation","title":"Blended (soft) representation","text":"<pre><code>workflow = rep.Workflow(\n    feature_engineer=feature_pipeline,\n    search_algorithm=search_algorithm,\n    representation_model=rep.BlendedRepresentationModel(blend_type='convex'),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n# result.weights is a DataFrame (not a dict) for blended models\n</code></pre>"},{"location":"advisor/#constructive-hull-clustering-with-blended-weights","title":"Constructive: Hull Clustering with blended weights","text":"<pre><code>import energy_repset as rep\n\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.HullClusteringSearch(k=3, hull_type='convex'),\n    representation_model=rep.BlendedRepresentationModel(blend_type='convex'),\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n</code></pre>"},{"location":"advisor/#constructive-ctpc-with-contiguous-segments","title":"Constructive: CTPC with contiguous segments","text":"<pre><code>import energy_repset as rep\n\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.CTPCSearch(k=4, linkage='ward'),\n    representation_model=None,\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n# result.weights are pre-computed segment fractions\n</code></pre>"},{"location":"advisor/#constructive-snippet-with-multi-day-periods","title":"Constructive: Snippet with multi-day periods","text":"<pre><code>import energy_repset as rep\n\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"day\"))\nworkflow = rep.Workflow(\n    feature_engineer=rep.DirectProfileFeatureEngineer(),\n    search_algorithm=rep.SnippetSearch(k=4, period_length_days=7, step_days=7),\n    representation_model=None,\n)\nresult = rep.RepSetExperiment(context, workflow).run()\n# result.weights are pre-computed assignment fractions\n</code></pre>"},{"location":"advisor/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Blended weight aggregation: <code>BlendedRepresentationModel.weigh()</code> returns a weight matrix. If you sum columns for visualization, normalize the result so weights sum to 1.0 (otherwise bars show raw sums that scale with N).</p> </li> <li> <p>Combinatorial explosion: C(52, 8) = 752 million. Always check <code>combi_gen.count(slices)</code> before running. Use hierarchical generators or group quotas to reduce the search space.</p> </li> <li> <p>PCA without stats: <code>PCAFeatureEngineer</code> operates on existing features. It must come after <code>StandardStatsFeatureEngineer</code> in a <code>FeaturePipeline</code>, not as a standalone.</p> </li> <li> <p>DTW components are slow: <code>DTWFidelity</code> and <code>DiurnalDTWFidelity</code> use dynamic time warping which is O(n^2) per pair. Suitable for small candidate sets; consider cheaper alternatives for large searches.</p> </li> <li> <p>Direction confusion: Most fidelity components use <code>direction=\"min\"</code> (lower is better). <code>DiversityReward</code> uses <code>direction=\"max\"</code>. The <code>ObjectiveSet</code> and selection policies handle direction automatically -- you do not need to negate scores.</p> </li> <li> <p>Single vs multi-objective: With a single score component, <code>WeightedSumPolicy</code> and <code>ParetoMaxMinStrategy</code> produce identical results. Pareto-based policies only add value with 2+ objectives.</p> </li> <li> <p>High-dimensional feature spaces: With many variables and regions, <code>StandardStatsFeatureEngineer</code> can produce hundreds of features while you may have only 12--52 candidate slices. In high dimensions, distances concentrate and clustering/selection degrades. Check the feature-to-sample ratio and use PCA (via <code>FeaturePipeline</code>) to reduce dimensionality when it is large.</p> </li> <li> <p>Unweighted variables: By default, all variables contribute equally to the objective. If your downstream model is more sensitive to some variables (e.g., load matters more than temperature), the selection may over-optimize for less important variables. Use <code>variable_weights</code> in score components to reflect what actually matters.</p> </li> </ol>"},{"location":"constructive_algorithms/","title":"Constructive Algorithms","text":"<p>Constructive algorithms build representative selections iteratively using their own internal objectives, rather than scoring pre-generated candidate combinations. They implement Workflow 2 of the three generalized workflows: the search algorithm does all the heavy lifting, while the <code>ObjectiveSet</code> is only used for post-hoc evaluation.</p> <p>energy-repset provides four constructive algorithms, each grounded in a published methodology:</p> Algorithm Idea Selection Space Weights Reference <code>KMedoidsSearch</code> K-medoids partitioning with medoid selection Subset Pre-computed (cluster fractions) Kaufman &amp; Rousseeuw (1990) <code>HullClusteringSearch</code> Farthest-point greedy hull vertex selection Subset External (via <code>RepresentationModel</code>) Neustroev et al. (2025) <code>CTPCSearch</code> Contiguity-constrained hierarchical clustering Chronological segments Pre-computed (segment fractions) Pineda &amp; Morales (2018) <code>SnippetSearch</code> Greedy p-median selection of multi-day subsequences Subset (sliding windows) Pre-computed (assignment fractions) Anderson et al. (2024)"},{"location":"constructive_algorithms/#k-medoids-clustering","title":"K-Medoids Clustering","text":""},{"location":"constructive_algorithms/#idea","title":"Idea","text":"<p>K-medoids (PAM --- Partitioning Around Medoids) is the most straightforward clustering-based approach to representative period selection. It partitions \\(N\\) periods into \\(k\\) clusters and selects the medoid of each cluster --- the actual data point closest to the cluster center --- as the representative.</p> <p>Unlike k-means, which produces synthetic centroids that may not correspond to any real period, k-medoids always selects actual historical periods. This makes it a natural fit for the subset selection space (\\(\\mathcal{S}_{\\text{subset}}\\)).</p>"},{"location":"constructive_algorithms/#algorithm","title":"Algorithm","text":"<ol> <li>Initialize \\(k\\) medoids (using k-medoids++ or random initialization).</li> <li>Assign each period to the nearest medoid.</li> <li>For each cluster, swap the medoid with the member that minimizes within-cluster distance.</li> <li>Repeat steps 2--3 until convergence or <code>max_iter</code>.</li> <li>Compute weights as cluster-size fractions: \\(w_j = n_j / N\\).</li> </ol>"},{"location":"constructive_algorithms/#framework-decomposition","title":"Framework Decomposition","text":"Pillar Setting F Any feature space (<code>StandardStatsFeatureEngineer</code>, <code>PCAFeatureEngineer</code>, <code>DirectProfileFeatureEngineer</code>) O Internal: within-cluster sum of squares (WCSS). External <code>ObjectiveSet</code> for post-hoc only. S Subset (\\(\\mathcal{S} \\subset\\) original periods) R Pre-computed (cluster-size fractions). The external <code>RepresentationModel</code> is skipped. A Iterative partitioning (k-medoids)"},{"location":"constructive_algorithms/#usage","title":"Usage","text":"<p>K-medoids pre-computes weights, so no external <code>RepresentationModel</code> is needed:</p> <pre><code>import energy_repset as rep\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.KMedoidsSearch(k=4, random_state=42),\n    representation_model=None,\n)\n</code></pre> <p>Key parameters:</p> Parameter Effect <code>k</code> Number of clusters / representative periods <code>metric</code> Distance metric (default <code>'euclidean'</code>) <code>method</code> <code>'alternate'</code> (fast, default) or <code>'pam'</code> (exact, slower) <code>init</code> Initialization strategy (default <code>'k-medoids++'</code>) <p>For a hands-on demo, see Example 7: K-Medoids Clustering.</p>"},{"location":"constructive_algorithms/#hull-clustering","title":"Hull Clustering","text":"<p>Reference: G. Neustroev, D. A. Tejada-Arango, G. Morales-Espana, M. M. de Weerdt. \"Hull Clustering with Blended Representative Periods for Energy System Optimization Models.\" arXiv: 2508.21641, 2025.</p>"},{"location":"constructive_algorithms/#idea_1","title":"Idea","text":"<p>Hull Clustering treats representative period selection as a projection problem. Given \\(N\\) periods in a \\(p\\)-dimensional feature space, the goal is to find \\(k\\) \"hull vertices\" such that every period can be well approximated as a combination of these vertices.</p> <p>The key insight is geometric: the selected representatives should span the feature space so that no period is far from the convex hull they define. This naturally produces representatives that cover the extremes of the data --- high-demand winter days, peak-solar summer days, calm wind periods --- because these are the vertices needed to enclose the rest.</p>"},{"location":"constructive_algorithms/#algorithm_1","title":"Algorithm","text":"<p>Farthest-point greedy forward selection (Algorithm 2 in the paper):</p> <ol> <li>Initialization: select the period furthest from the dataset mean in feature space.</li> <li>For iterations 2 through \\(k\\), compute each remaining period's projection error (hull distance) by solving:</li> </ol> \\[ \\min_{\\mathbf{w}} \\| \\mathbf{z}_i - \\mathbf{Z}_{\\mathcal{S}} \\mathbf{w} \\|^2 \\] <p>with constraints depending on the hull type:</p> <ul> <li>Convex (\\(\\mathbf{w} \\geq 0\\), \\(\\sum w_j = 1\\)): each period is a convex combination of the representatives.</li> <li> <p>Conic (\\(\\mathbf{w} \\geq 0\\)): each period is a non-negative combination (more relaxed).</p> </li> <li> <p>Select the remaining period with the maximum projection error (furthest from the current hull).</p> </li> </ul> <p>This farthest-point strategy naturally selects extreme/boundary periods first --- high-demand winter months, peak-solar summer months, etc. --- producing a hull that spans the data well.</p>"},{"location":"constructive_algorithms/#framework-decomposition_1","title":"Framework Decomposition","text":"Pillar Setting F Any feature space (<code>StandardStatsFeatureEngineer</code>, <code>PCAFeatureEngineer</code>, <code>DirectProfileFeatureEngineer</code>) O Internal: total projection error. External <code>ObjectiveSet</code> for post-hoc evaluation only. S Subset (\\(\\mathcal{S} \\subset\\) original periods) R External --- typically <code>BlendedRepresentationModel(blend_type='convex')</code> to compute soft-assignment weights A Greedy constructive (farthest-point forward selection)"},{"location":"constructive_algorithms/#usage_1","title":"Usage","text":"<p>Hull Clustering leaves <code>weights=None</code> in the result, so it is naturally paired with <code>BlendedRepresentationModel</code> which computes soft-assignment weights using the same convex projection logic:</p> <pre><code>import energy_repset as rep\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.HullClusteringSearch(k=4, hull_type='convex'),\n    representation_model=rep.BlendedRepresentationModel(blend_type='convex'),\n)\n</code></pre>"},{"location":"constructive_algorithms/#ctpc-chronological-time-period-clustering","title":"CTPC (Chronological Time-Period Clustering)","text":"<p>Reference: S. Pineda, J. M. Morales. \"Chronological Time-Period Clustering for Optimal Capacity Expansion Planning With Storage.\" IEEE Transactions on Power Systems, 33(6), 7162--7170, 2018. DOI: 10.1109/TPWRS.2018.2842093</p>"},{"location":"constructive_algorithms/#idea_2","title":"Idea","text":"<p>CTPC applies hierarchical agglomerative clustering with a contiguity constraint: only temporally adjacent periods may merge. This guarantees that the resulting clusters are contiguous time segments --- for example, \"January 1--March 15\" and \"March 16--June 30\" rather than scattered individual periods.</p> <p>This is valuable when the downstream model needs to preserve temporal coupling across periods (e.g., multi-day storage dispatch, seasonal hydro scheduling) or when the user wants to reason about contiguous blocks of time.</p>"},{"location":"constructive_algorithms/#algorithm_2","title":"Algorithm","text":"<ol> <li>Arrange all \\(N\\) time slices in chronological order.</li> <li>Build a tridiagonal connectivity matrix: each slice connects only to its immediate neighbors.</li> <li>Run agglomerative clustering (Ward, complete, average, or single linkage) with this connectivity constraint, stopping at \\(k\\) clusters.</li> <li>Within each cluster, select the medoid (period closest to the cluster centroid) as the representative.</li> <li>Compute weights as the fraction of time covered by each segment: \\(w_j = |\\text{segment}_j| / N\\).</li> </ol>"},{"location":"constructive_algorithms/#framework-decomposition_2","title":"Framework Decomposition","text":"Pillar Setting F Any feature space (<code>StandardStatsFeatureEngineer</code>, <code>DirectProfileFeatureEngineer</code>) O Internal: within-cluster sum of squares (WCSS). External <code>ObjectiveSet</code> for post-hoc only. S Chronological (contiguous time segments) R Pre-computed (segment size fractions). The external <code>RepresentationModel</code> is skipped. A Hierarchical agglomerative with contiguity constraint"},{"location":"constructive_algorithms/#usage_2","title":"Usage","text":"<p>CTPC pre-computes weights, so no external <code>RepresentationModel</code> is needed:</p> <pre><code>import energy_repset as rep\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.CTPCSearch(k=4, linkage='ward'),\n    representation_model=None,\n)\n</code></pre> <p>The <code>linkage</code> parameter controls how inter-cluster distance is measured during merging:</p> Linkage Behavior <code>'ward'</code> Minimizes within-cluster variance (default, usually best) <code>'complete'</code> Maximum distance between any two points in different clusters <code>'average'</code> Mean distance between all pairs across clusters <code>'single'</code> Minimum distance between any two points in different clusters"},{"location":"constructive_algorithms/#snippet-algorithm","title":"Snippet Algorithm","text":"<p>Reference: O. Anderson, N. Yu, K. Oikonomou, D. Wu. \"On the Selection of Intermediate Length Representative Periods for Capacity Expansion.\" arXiv: 2401.02888, 2024.</p>"},{"location":"constructive_algorithms/#idea_3","title":"Idea","text":"<p>The Snippet algorithm addresses a specific limitation of period-level selection: when selecting entire weeks (or multi-day blocks), a single anomalous day can make an otherwise typical week appear unrepresentative. Snippet solves this by comparing at the day level within multi-day windows.</p> <p>The algorithm selects \\(k\\) sliding-window subsequences (e.g., 7-day blocks) from the time horizon. The distance from any individual day to a candidate subsequence is the minimum distance to any of that subsequence's constituent daily profiles. This means a candidate week is \"close\" to a given day if any of its 7 days resembles that day --- not just the week as a whole.</p>"},{"location":"constructive_algorithms/#algorithm_3","title":"Algorithm","text":"<ol> <li>Flatten each day's hourly values across all variables into a profile vector \\(\\mathbf{d}_i \\in \\mathbb{R}^{H \\times V}\\) (e.g., 24 hours \\(\\times\\) 3 variables = 72 dimensions).</li> <li>Generate \\(C\\) sliding-window candidates of length \\(L\\) days with stride \\(s\\): $$ C = \\lfloor (N - L) / s \\rfloor + 1 $$</li> <li>Compute a distance matrix \\(\\mathbf{D} \\in \\mathbb{R}^{N \\times C}\\) where: $$ D_{i,j} = \\min_{l \\in \\text{candidate}_j} | \\mathbf{d}_i - \\mathbf{d}_l |^2 $$</li> <li>Greedy p-median selection (\\(k\\) iterations): at each step, pick the candidate that most reduces the total per-day minimum distance.</li> <li>Assign each day to its nearest selected candidate. Weights are the fraction of assigned days: \\(w_j = n_j / N\\).</li> </ol>"},{"location":"constructive_algorithms/#framework-decomposition_3","title":"Framework Decomposition","text":"Pillar Setting F <code>DirectProfileFeatureEngineer</code> (raw hourly profiles, flattened) O Internal: total per-day minimum distance. External <code>ObjectiveSet</code> for post-hoc only. S Subset (sliding-window subsequences) R Pre-computed (assignment fractions). The external <code>RepresentationModel</code> is skipped. A Greedy p-median"},{"location":"constructive_algorithms/#usage_3","title":"Usage","text":"<p>Snippet requires daily slicing and works best with <code>DirectProfileFeatureEngineer</code>:</p> <pre><code>import energy_repset as rep\n\nslicer = rep.TimeSlicer(unit='day')\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.DirectProfileFeatureEngineer(),\n    search_algorithm=rep.SnippetSearch(k=4, period_length_days=7, step_days=1),\n    representation_model=None\n)\n</code></pre> <p>Key parameters:</p> Parameter Effect <code>period_length_days</code> Length of each representative subsequence (e.g., 7 for weekly blocks) <code>step_days</code> Stride between consecutive candidates. <code>step_days=1</code> gives maximum overlap (most candidates); <code>step_days=7</code> gives non-overlapping windows (fewest candidates, fastest). <p>Implementation notes:</p> <ul> <li>The original paper formulates the selection as a MILP (mixed-integer linear program). The energy-repset implementation uses a greedy p-median heuristic instead, which provides a \\((1 - 1/e)\\) approximation guarantee and avoids requiring a MILP solver.</li> <li>Distances use squared Euclidean distance (\\(\\|\\mathbf{d}_i - \\mathbf{d}_l\\|^2\\)) rather than the paper's Euclidean norm. This preserves the selection and assignment outcomes (monotone transform) but the reported <code>total_distance</code> score is on a squared scale.</li> <li>Weights are normalized to sum to 1 (\\(w_j = n_j / N\\)) for consistency with the rest of the energy-repset package, rather than the paper's multiplicity convention (\\(w_j = n_j / L\\)).</li> </ul>"},{"location":"constructive_algorithms/#comparing-the-algorithms","title":"Comparing the Algorithms","text":"<p>Each constructive algorithm makes different trade-offs:</p> Aspect K-Medoids Hull Clustering CTPC Snippet Best for Standard clustering-based selection Selecting extreme/boundary periods Contiguous time blocks Multi-day representative periods Weights Built-in (cluster fractions) External (soft assignment) Built-in (segment fractions) Built-in (assignment fractions) Temporal structure None (any subset) None (any subset) Enforced (contiguous segments) Partial (sliding windows) Typical slicing Monthly or weekly Monthly or weekly Any (monthly, weekly, daily) Daily (required) Computational cost \\(O(k \\cdot N \\cdot I)\\) iterations \\(O(k \\cdot N)\\) QP solves \\(O(N^2)\\) agglomerative clustering \\(O(k \\cdot N \\cdot C)\\) distance evaluations Key strength Simple, fast, well-understood Geometric coverage of feature space Preserves temporal coupling Day-level matching within multi-day blocks <p>For hands-on comparisons, see Example 6: K-Medoids Clustering and Example 7: Constructive Algorithms.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide walks through a minimal end-to-end workflow, explaining each step and how it maps to the five-pillar framework (F, O, S, R, A).</p> <p>By the end, you will have selected 4 representative months from a year of hourly time-series data, scored them on distribution fidelity, and generated a simple diagnostic chart.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>Option 1 -- Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/mesqual/energy-repset.git\n</code></pre> <p>Option 2 -- Clone and install in editable mode:</p> <pre><code>git clone https://github.com/mesqual/energy-repset.git\ncd energy-repset\npip install -e .\n</code></pre> <p>Option 3 -- Add as a Git submodule (useful for monorepos):</p> <pre><code>git submodule add https://github.com/mesqual/energy-repset.git\npip install -e energy-repset\n</code></pre> <p>Alternatively, skip the install and mark the <code>energy-repset</code> directory as a source root in your IDE so that <code>import energy_repset</code> resolves directly.</p>"},{"location":"getting_started/#imports","title":"Imports","text":"<p>All framework classes are available from the top-level namespace. Diagnostics live one level down:</p> <pre><code>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\n</code></pre>"},{"location":"getting_started/#load-data","title":"Load Data","text":"<p>energy-repset works with any <code>pandas.DataFrame</code> where the index is a <code>DatetimeIndex</code> and each column is a variable (e.g., load, wind, solar):</p> <pre><code>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n</code></pre>"},{"location":"getting_started/#define-the-problem-context","title":"Define the Problem Context","text":"<p>The <code>ProblemContext</code> combines the raw data with a <code>TimeSlicer</code> that defines how the time axis is divided into candidate periods. Here, each calendar month becomes one candidate:</p> <pre><code>slicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\nprint(f\"Candidate slices: {context.get_unique_slices()}\")\n# -&gt; 12 monthly periods\n</code></pre>"},{"location":"getting_started/#pillar-f-feature-engineering","title":"Pillar F: Feature Engineering","text":"<p>Feature engineering transforms the raw time-series into a compact representation that can be compared across candidate periods. <code>StandardStatsFeatureEngineer</code> computes statistical summaries (mean, std, quantiles, ramp rates) per slice and variable:</p> <pre><code>feature_engineer = rep.StandardStatsFeatureEngineer()\n</code></pre> <p>For richer feature spaces, you can chain engineers with a <code>FeaturePipeline</code>:</p> <pre><code>feature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n</code></pre> <p>In this guide we keep it simple and use only the statistical features.</p>"},{"location":"getting_started/#pillar-o-objective","title":"Pillar O: Objective","text":"<p>The <code>ObjectiveSet</code> defines how candidate selections are scored. Each entry maps a name to a <code>(weight, ScoreComponent)</code> tuple. Here we use a single objective: Wasserstein distance between the marginal distributions of the selection and the full year.</p> <pre><code>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n})\n</code></pre> <p>Multiple objectives are easy to add:</p> <pre><code>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n})\n</code></pre>"},{"location":"getting_started/#pillar-s-selection-space","title":"Pillar S: Selection Space","text":"<p>A <code>CombinationGenerator</code> defines which subsets are considered. <code>ExhaustiveCombiGen</code> evaluates every possible k-of-n combination:</p> <pre><code>k = 4\ncombi_gen = rep.ExhaustiveCombiGen(k=k)\n# For 12 months, k=4 -&gt; C(12,4) = 495 candidates\n</code></pre>"},{"location":"getting_started/#pillar-a-search-algorithm","title":"Pillar A: Search Algorithm","text":"<p>The search algorithm orchestrates the evaluation loop. In the generate-and-test workflow, it generates candidates via the <code>CombinationGenerator</code>, scores each with the <code>ObjectiveSet</code>, and picks a winner using the <code>SelectionPolicy</code>:</p> <pre><code>policy = rep.WeightedSumPolicy()\nsearch_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    objective_set, policy, combi_gen\n)\n</code></pre>"},{"location":"getting_started/#pillar-r-representation-model","title":"Pillar R: Representation Model","text":"<p>The representation model determines how the selected periods represent the full year. <code>UniformRepresentationModel</code> assigns equal weight to each selected period:</p> <pre><code>representation_model = rep.UniformRepresentationModel()\n</code></pre>"},{"location":"getting_started/#run-the-workflow","title":"Run the Workflow","text":"<p>Assemble all components into a <code>Workflow</code>, wrap it in a <code>RepSetExperiment</code>, and run:</p> <pre><code>workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n</code></pre>"},{"location":"getting_started/#inspect-results","title":"Inspect Results","text":"<p>The <code>RepSetResult</code> contains the selected periods, their weights, and the objective scores:</p> <pre><code>print(f\"Selected months: {result.selection}\")\nprint(f\"Weights: {result.weights}\")\nprint(f\"Wasserstein score: {result.scores['wasserstein']:.4f}\")\n</code></pre>"},{"location":"getting_started/#diagnostic-responsibility-bars","title":"Diagnostic: Responsibility Bars","text":"<p>energy-repset includes interactive Plotly diagnostics. A responsibility bar chart shows how the total representation weight is distributed across selected periods:</p> <pre><code>fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True)\nfig.show()\n</code></pre>"},{"location":"getting_started/#full-script","title":"Full Script","text":"<p>The complete code is available at <code>examples/ex1_getting_started.py</code>.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Swap components: Try <code>rep.ParetoMaxMinStrategy</code> instead of <code>rep.WeightedSumPolicy</code>,   or <code>rep.KMedoidsClustersizeRepresentation</code> instead of uniform weights.   See the Modules &amp; Components page for all available implementations.</li> <li>Add objectives: Add <code>rep.CorrelationFidelity</code>, <code>rep.DurationCurveFidelity</code>, or   <code>rep.DiversityReward</code> to the <code>ObjectiveSet</code>.</li> <li>Browse examples: The Examples show more advanced   configurations with interactive visualizations.</li> </ul>"},{"location":"modules/","title":"Modules &amp; Components","text":"<p>energy-repset decomposes any representative period selection method into five interchangeable pillars. Each pillar has a protocol (interface) and one or more concrete implementations. Swapping a single component changes the behavior without affecting the rest of the pipeline.</p>"},{"location":"modules/#the-five-pillars","title":"The Five Pillars","text":"<pre><code>Raw DataFrame\n  -&gt; TimeSlicer (defines candidate periods)\n  -&gt; ProblemContext (holds data + metadata)\n  -&gt; [F] FeatureEngineer (creates feature vectors per slice)\n  -&gt; [A] SearchAlgorithm (finds optimal selection using [O] ObjectiveSet)\n  -&gt; [R] RepresentationModel (calculates weights)\n  -&gt; RepSetResult (selection, weights, scores)\n</code></pre>"},{"location":"modules/#f-feature-space","title":"F: Feature Space","text":"<p>Transforms raw time-series slices into comparable feature vectors.</p> Implementation Description <code>StandardStatsFeatureEngineer</code> Statistical summaries per slice (mean, std, IQR, quantiles, ramp rates). Z-score normalized. <code>PCAFeatureEngineer</code> PCA dimensionality reduction on existing features. Supports variance-threshold or fixed component count. <code>DirectProfileFeatureEngineer</code> Flattened raw hourly profiles per slice. Preserves full temporal shape. Used by Snippet and DTW-based methods. <code>FeaturePipeline</code> Chains multiple engineers sequentially and concatenates their outputs. <pre><code>import energy_repset as rep\n\n# Single engineer\nfeature_engineer = rep.StandardStatsFeatureEngineer()\n\n# Chained pipeline: compute stats, then reduce with PCA\nfeature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n\n# Direct profile vectors (for Snippet, DTW-based methods)\ndirect = rep.DirectProfileFeatureEngineer()\n</code></pre>"},{"location":"modules/#o-objective","title":"O: Objective","text":"<p>An <code>ObjectiveSet</code> holds one or more weighted <code>ScoreComponent</code> instances. Each component evaluates how well a candidate selection represents the full dataset along a specific dimension.</p> Component Name Direction What it Measures <code>WassersteinFidelity</code> <code>wasserstein</code> min Marginal distribution similarity (Wasserstein distance, IQR-normalized) <code>CorrelationFidelity</code> <code>correlation</code> min Cross-variable correlation preservation (Frobenius norm) <code>DurationCurveFidelity</code> <code>nrmse_duration_curve</code> min Duration curve match (quantile-based NRMSE) <code>NRMSEFidelity</code> <code>nrmse</code> min Duration curve match (full interpolation NRMSE) <code>DiurnalFidelity</code> <code>diurnal</code> min Hour-of-day profile preservation (normalized MSE) <code>DiurnalDTWFidelity</code> <code>diurnal_dtw</code> min Hour-of-day profile preservation (DTW distance) <code>DTWFidelity</code> <code>dtw</code> min Full series shape similarity (Dynamic Time Warping) <code>DiversityReward</code> <code>diversity</code> max Spread of representatives in feature space (avg pairwise distance) <code>CentroidBalance</code> <code>centroid_balance</code> min Feature centroid deviation from global mean <code>CoverageBalance</code> <code>coverage_balance</code> min Balanced coverage via RBF kernel soft assignment <pre><code>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n    'diversity':   (0.5, rep.DiversityReward()),\n})\n</code></pre> <p>The weight (first element of each tuple) expresses relative importance. Components with <code>direction=\"min\"</code> are better when smaller; <code>direction=\"max\"</code> are better when larger.</p>"},{"location":"modules/#s-selection-space","title":"S: Selection Space","text":"<p>A <code>CombinationGenerator</code> defines which subsets the search algorithm considers.</p> Implementation Description <code>ExhaustiveCombiGen</code> All k-of-n combinations. Feasible for small n (e.g., 12 months, k=4 gives 495 candidates). <code>GroupQuotaCombiGen</code> Enforces exact quotas per group (e.g., 1 month per season). <code>ExhaustiveHierarchicalCombiGen</code> Selects parent groups (e.g., months) but evaluates on child slices (e.g., days). <code>GroupQuotaHierarchicalCombiGen</code> Combines hierarchical selection with group quotas. <pre><code># Simple: all 4-of-12 monthly combinations\ncombi_gen = rep.ExhaustiveCombiGen(k=4)\n\n# Hierarchical with seasonal constraints\ncombi_gen = rep.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n    parent_k=4,\n    dt_index=df_raw.index,\n    child_slicer=rep.TimeSlicer(unit=\"day\"),\n    group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1},\n)\n</code></pre>"},{"location":"modules/#r-representation-model","title":"R: Representation Model","text":"<p>Determines how selected periods represent the full dataset through responsibility weights.</p> Implementation Description <code>UniformRepresentationModel</code> Equal 1/k weights. Simplest option. <code>KMedoidsClustersizeRepresentation</code> Weights proportional to cluster sizes from k-medoids hard assignment. <code>BlendedRepresentationModel</code> Soft assignment: each original slice is a convex combination of representatives. Returns a weight matrix instead of a weight dict. <pre><code># Equal weights\nuniform = rep.UniformRepresentationModel()\n\n# Cluster-proportional weights\nkmedoids = rep.KMedoidsClustersizeRepresentation()\n\n# Soft blending (returns a DataFrame, not a dict)\nblended = rep.BlendedRepresentationModel(blend_type='convex')\n</code></pre>"},{"location":"modules/#a-search-algorithm","title":"A: Search Algorithm","text":"<p>The engine that finds the optimal selection.</p> Implementation Workflow Type Description <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> Generate-and-Test Evaluates all candidate combinations and selects the winner via a <code>SelectionPolicy</code>. <code>HullClusteringSearch</code> Constructive Greedy forward selection minimizing total projection error. Leaves <code>weights=None</code> for external representation model. <code>CTPCSearch</code> Constructive Contiguity-constrained hierarchical clustering. Pre-computes weights as segment size fractions. <code>SnippetSearch</code> Constructive Greedy p-median selection of multi-day subsequences. Requires daily slicing. Pre-computes weights. <p>For details on the constructive algorithms, see Constructive Algorithms.</p>"},{"location":"modules/#selection-policies","title":"Selection Policies","text":"<p>The policy decides how to pick a winner from the scored candidates:</p> Policy Description <code>WeightedSumPolicy</code> Scalar aggregation of scores. Supports <code>normalization='robust_minmax'</code> for multi-objective balance. <code>ParetoMaxMinStrategy</code> Selects the Pareto-optimal solution that maximizes its worst-performing objective. <code>ParetoUtopiaPolicy</code> Selects the Pareto-optimal solution closest to the utopia point. <pre><code># Weighted sum (default)\npolicy = rep.WeightedSumPolicy(normalization='robust_minmax')\nsearch = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\n\n# Pareto max-min\npolicy = rep.ParetoMaxMinStrategy()\nsearch = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\n\n# Constructive algorithms (no ObjectiveSet or policy needed)\nhull = rep.HullClusteringSearch(k=4, hull_type='convex')\nctpc = rep.CTPCSearch(k=4, linkage='ward')\nsnippet = rep.SnippetSearch(k=4, period_length_days=7, step_days=7)\n</code></pre>"},{"location":"modules/#diagnostics","title":"Diagnostics","text":"<p>Interactive Plotly visualizations for inspecting results, feature spaces, and score component behavior. See the Examples for rendered examples.</p>"},{"location":"modules/#feature-space","title":"Feature Space","text":"Class Purpose <code>FeatureSpaceScatter2D</code> 2D scatter plot of feature space <code>FeatureSpaceScatter3D</code> 3D scatter plot of feature space <code>FeatureSpaceScatterMatrix</code> Pairwise scatter matrix <code>PCAVarianceExplained</code> Cumulative variance explained by PCA components <code>FeatureCorrelationHeatmap</code> Correlation heatmap between features <code>FeatureDistributions</code> Distribution histograms per feature"},{"location":"modules/#results","title":"Results","text":"Class Purpose <code>ResponsibilityBars</code> Weight distribution across selected representatives <code>ParetoScatter2D</code> 2D objective-space scatter with Pareto front <code>ParetoScatterMatrix</code> Pairwise objective-space scatter matrix <code>ParetoParallelCoordinates</code> Parallel coordinates of Pareto front <code>ScoreContributionBars</code> Per-component score breakdown"},{"location":"modules/#score-components","title":"Score Components","text":"Class Purpose <code>DistributionOverlayECDF</code> ECDF comparison of full vs selected data <code>DistributionOverlayHistogram</code> Histogram comparison of full vs selected data <code>CorrelationDifferenceHeatmap</code> Correlation matrix difference heatmap <code>DiurnalProfileOverlay</code> Diurnal profile comparison"},{"location":"modules/#putting-it-together","title":"Putting It Together","text":"<pre><code>workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n\n# result.selection -&gt; tuple of selected slice identifiers\n# result.weights   -&gt; dict mapping each selected slice to its weight\n# result.scores    -&gt; dict mapping each objective name to its score\n</code></pre> <p>For a complete walkthrough, see the Getting Started guide. For the theoretical foundations, see the Unified Framework.</p>"},{"location":"unified_framework/","title":"A Unified Framework for Representative Subset Selection in Energy Time Series","text":"<p>Note: I developed this framework and prompted this document into existence with the help of my coworker Claude Code. The ideas come from me (the human); Claude helped me write them down better than I could ever have done alone.</p>"},{"location":"unified_framework/#abstract","title":"Abstract","text":"<p>Representative period selection (RPS), the process of selecting or constructing a small set of representative periods from a larger dataset, is essential for making computationally expensive energy system models tractable. The field has produced a rich but fragmented array of methods: clustering, mathematical programming, autoencoders, greedy algorithms, and more. Each method is typically presented as a monolithic procedure, making it difficult to compare methods, understand their implicit assumptions, or assemble custom pipelines.</p> <p>This paper proposes a unified framework that decomposes any RPS method into five fundamental components: the Feature Space (how periods are represented), the Objective (what quality means), the Selection Space (what form the output takes), the Representation Model (how the selection approximates the whole), and the Search Algorithm (how the solution is found). We show that every established methodology is a specific instantiation of this five-component structure, and that the framework enables systematic comparison, exposes trade-offs, and provides an architectural blueprint for modular software.</p>"},{"location":"unified_framework/#1-introduction","title":"1. Introduction","text":"<p>Energy system models (ESMs) are central analytical tools for energy system planning and research. The integration of variable renewable energy sources (VRES), storage technologies, and cross-sectoral coupling demands high temporal resolution, typically hourly or sub-hourly, across a full year or longer. For many ESMs, particularly those used in capacity expansion planning, investment optimization, or market studies, this produces optimization problems that are computationally intractable.</p> <p>Reducing the temporal complexity of energy system inputs involves two orthogonal dimensions. The first is between-period reduction: selecting a small number \\(k\\) of representative periods (days, weeks, months, or even years) from the \\(N\\) candidates in the full dataset. The second is within-period reduction: adjusting the temporal resolution inside each period, from simple uniform downsampling (e.g., converting 15-minute to hourly data) to domain-informed selective segmentation, where, for example, overnight hours are aggregated into a single block while solar ramp hours remain at full resolution. The two dimensions are independent and compose naturally: within-period reduction can be applied before representative period selection (as preprocessing) or after (as postprocessing on the selected periods before they enter the downstream model). The Python package <code>tsam</code> (Kotzur et al., 2018) supports both dimensions.</p> <p>This framework focuses exclusively on between-period reduction \u2014 representative period selection. A well-chosen selection of \\(k \\ll N\\) periods can reduce computation by orders of magnitude while maintaining the fidelity of model results. The challenge is that \"representativeness\" is not a single, well-defined concept. At a fundamental level, the modeler must decide what to represent: the statistical properties of the input data (e.g., weather patterns, demand profiles) or the outcomes of the downstream model (e.g., socio-economic welfare, system cost, capacity investments). Within these categories, the practical meaning of \"representative\" varies further depending on the modeling question:</p> <ul> <li>Aggregate fidelity: the selected periods, when weighted appropriately, reproduce the overall statistics (means, distributions, correlations) of the full dataset, so that annualized system cost is captured accurately.</li> <li>State-space coverage: the selected periods span the diversity of conditions that occur (high wind with high demand, low wind with high solar, peak events, etc.) so that the model encounters all operationally distinct system states.</li> <li>A combination: cover the breadth of system states while also matching aggregate statistics, often under the constraint that all periods carry equal weight.</li> </ul> <p>Different RPS methods make different implicit choices about what to preserve, how to search, and what form the output takes. These choices are rarely made explicit, which makes comparison difficult.</p> <p>Inspired by unifying efforts in other fields (notably Warren B. Powell's A unified framework for stochastic optimization), we propose a decomposition of the RPS problem into five modular, interchangeable components. Any concrete method is a specific instantiation of this structure. The framework does not prescribe a single best method; it provides a common language for describing, comparing, and assembling methods.</p> <p>The remainder of this paper is organized as follows. Section 2 presents the five-component framework. Section 3 demonstrates how established methods decompose into the framework. Section 4 discusses practical implications and open questions.</p>"},{"location":"unified_framework/#2-the-unified-framework","title":"2. The Unified Framework","text":""},{"location":"unified_framework/#21-overview","title":"2.1 Overview","text":"<p>Consider a dataset \\(D = \\{d_1, \\ldots, d_N\\}\\) of \\(N\\) time periods, where each \\(d_i\\) is a multivariate time series for period \\(i\\). The variables may include load, wind capacity factors, solar irradiance, temperature, and others, potentially across multiple regions (each region-variable pair is simply an additional dimension of the time series vector). The temporal granularity (days, weeks, months, years) is a problem parameter that depends on the application: one study may select representative days from a year, another representative weeks, and yet another a subset of representative years from a multi-decadal climate dataset.</p> <p>The goal is to find a selection \\(x\\) of \\(k \\ll N\\) representative periods (or constructs derived from them) such that some quality measure is optimized. We formalize this as:</p> \\[ x^* \\;=\\; \\underset{x \\,\\in\\, \\mathcal{S}}{\\arg\\min}\\; \\mathcal{O}\\!\\bigl(\\mathcal{R}(x,\\, D)\\bigr) \\] <p>where:</p> Symbol Component Role \\(\\mathcal{F}\\) Feature Space How periods are represented mathematically \\(\\mathcal{O}\\) Objective What quality measure is optimized \\(\\mathcal{S}\\) Selection Space What structural form the output takes \\(\\mathcal{R}\\) Representation Model How the selection approximates the full dataset \\(\\mathcal{A}\\) Search Algorithm How the optimal selection is found <p>Any concrete RPS method is defined by a specific 5-tuple \\((\\mathcal{F},\\, \\mathcal{O},\\, \\mathcal{S},\\, \\mathcal{R},\\, \\mathcal{A})\\).</p> <p>The five components are conceptually independent: each addresses a distinct design decision. In practice, certain combinations are more natural than others, and some methods couple components tightly. Making these couplings explicit is one of the framework's main contributions.</p> <p>A note on the role of \\(\\mathcal{R}\\) during search. The formulation above presents the ideal: the objective evaluates the quality of the representation, not of the raw selection. In practice, most combinatorial search methods (\\(\\mathcal{A}_\\text{comb}\\)) evaluate candidates by comparing the raw data of the selected periods against the full dataset, effectively bypassing \\(\\mathcal{R}\\) during the search and applying it only after the best selection has been identified. This is a pragmatic simplification: computing \\(\\mathcal{R}\\) for every candidate in an exhaustive search can be expensive, and for simple representation models like \\(\\mathcal{R}_\\text{equal}\\) the difference is negligible. Methods that jointly optimize selection and representation, such as \\(\\mathcal{A}_\\text{optim}\\) (MILP) or \\(\\mathcal{A}_\\text{construct}\\) (clustering, where the assignment is the representation), adhere more closely to the full formulation.</p> <p>The following subsections define each component, its variants, and their trade-offs.</p>"},{"location":"unified_framework/#22-component-1-feature-space-mathcalf-how-we-see-the-data","title":"2.2 Component 1: Feature Space (\\(\\mathcal{F}\\)) \u2014 How We See the Data","text":"<p>Before periods can be compared, grouped, or evaluated, they must be represented as mathematical objects. The feature space \\(\\mathcal{F}\\) defines this representation:</p> \\[\\mathcal{F}: D \\;\\to\\; \\{z_1, \\ldots, z_N\\}, \\quad z_i \\in \\mathbb{R}^p\\] <p>where \\(z_i\\) is the feature vector for period \\(i\\), and \\(p\\) is the dimensionality of the feature space.</p> <p>The choice of \\(\\mathcal{F}\\) is consequential: it defines what \"similar\" means. Two periods that are close in one feature space may be distant in another. The feature space shapes how the objective is computed and how the search algorithm operates.</p> <p>Variants:</p> Variant Description \\(\\mathcal{F}_\\text{direct}\\) Raw time-series vectors. For a period of \\(H\\) time steps and \\(V\\) variables (where \\(V\\) may include multiple regions): \\(z_i \\in \\mathbb{R}^{V \\times H}\\). Complete but high-dimensional. \\(\\mathcal{F}_\\text{stat}\\) Hand-crafted statistical summaries: means, standard deviations, quantiles, ramp rates, correlations, etc. Lower-dimensional and interpretable, but depends on the modeler's choice of features. \\(\\mathcal{F}_\\text{latent}\\) Learned low-dimensional representations via PCA, autoencoders, or other dimensionality reduction. Captures complex patterns automatically, including nonlinear structure (autoencoders). \\(\\mathcal{F}_\\text{model}\\) Features derived from running a simplified model (e.g., a dispatch model) for each period. Model outputs (generation mix, storage dispatch, marginal prices) are used as features, potentially combined with input data. This makes the feature space \"problem-aware\": periods are grouped by their operational impact, not just their statistical appearance. <p>\\(\\mathcal{F}_\\text{direct}\\) is the default when no explicit feature engineering is performed (e.g., standard k-means on raw hourly data). \\(\\mathcal{F}_\\text{stat}\\) and \\(\\mathcal{F}_\\text{latent}\\) trade information for computational efficiency and noise reduction. \\(\\mathcal{F}_\\text{model}\\) is the most advanced variant, requiring preliminary model runs but offering the strongest link between input representation and downstream model fidelity.</p> <p>These variants can be composed: for instance, \\(\\mathcal{F}_\\text{model}\\) features may be passed through an autoencoder to produce a \\(\\mathcal{F}_\\text{latent}\\) representation that encodes both input patterns and model responses.</p>"},{"location":"unified_framework/#feature-normalization-and-weighting","title":"Feature normalization and weighting","text":"<p>Two practical considerations arise when constructing \\(\\mathcal{F}\\):</p> <p>Normalization. Features often span vastly different scales. Demand variables may be in the range of thousands of MW, while capacity factors lie between 0 and 1. Without normalization, distance-based methods (clustering, Wasserstein distance, diversity metrics) are dominated by variables with the largest magnitude, effectively ignoring the rest. Common strategies include z-score standardization (subtract mean, divide by standard deviation) and min-max scaling. The choice of normalization is a design decision within \\(\\mathcal{F}\\) and should be made deliberately.</p> <p>Feature weighting. The modeler may have domain knowledge that certain variables or features matter more for the downstream application than others. For example, if the downstream model is demand-driven, load features should carry more weight than temperature features. Importance weights can be applied at the variable level (scaling entire time series before feature extraction) or at the feature level (scaling individual features in the constructed feature space). Either way, the weighting becomes part of \\(\\mathcal{F}\\) and shapes how \"similarity\" is defined for all downstream components.</p>"},{"location":"unified_framework/#the-curse-of-dimensionality","title":"The curse of dimensionality","text":"<p>The feature dimension \\(p\\) can grow rapidly. With \\(V\\) variables across \\(R\\) regions, the base data contains \\(V \\cdot R\\) time series. For \\(\\mathcal{F}_\\text{direct}\\), each series contributes \\(H\\) features (one per time step), yielding \\(p = V \\cdot R \\cdot H\\), which can easily reach thousands. For \\(\\mathcal{F}_\\text{stat}\\), each series produces multiple summary statistics (mean, standard deviation, quantiles, ramp rates, correlations), so \\(p\\) scales with the number of statistics chosen. In either case, the resulting feature space can have hundreds or thousands of dimensions, while the number of candidate periods \\(N\\) may be only 12 (months) to 52 (weeks).</p> <p>When \\(p\\) is large relative to \\(N\\), distances concentrate: the ratio of the maximum to the minimum pairwise distance approaches 1, and all periods begin to look equally (dis)similar. Clustering degrades, diversity metrics lose discriminative power, and the selection process becomes unreliable.</p> <p>Practical diagnostics:</p> <ul> <li>Feature-to-sample ratio. Compare \\(p\\) to \\(N\\). If \\(p \\gg N\\), dimensionality reduction is strongly advisable.</li> <li>PCA explained variance. Inspect the cumulative explained variance curve. If a few principal components capture most of the variance (e.g., 3 components explain 90%), the effective dimensionality is manageable.</li> <li>Use \\(\\mathcal{F}_\\text{latent}\\) (PCA, autoencoders) to project the feature space down to a tractable number of dimensions before running the search.</li> </ul>"},{"location":"unified_framework/#23-component-2-objective-mathcalo-what-we-want-to-preserve","title":"2.3 Component 2: Objective (\\(\\mathcal{O}\\)) \u2014 What We Want to Preserve","text":"<p>The objective defines the quality measure that the selection should optimize. This is the most consequential choice in the framework, as it determines what \"representative\" means for a given application.</p> \\[\\mathcal{O}: \\mathcal{R}(x, D) \\;\\to\\; \\mathbb{R} \\quad (\\text{or } \\mathbb{R}^m \\text{ in the multi-objective case})\\] <p>At the highest level, objectives fall into two categories:</p> Variant Description \\(\\mathcal{O}_\\text{stat}\\) Statistical fidelity. Preserve statistical properties of the input data. This is a proxy for the true goal. \\(\\mathcal{O}_\\text{model}\\) Model outcome fidelity. Preserve the results of the downstream optimization model (total cost, capacity mix, emissions). This is the true goal, but typically requires running the model during the selection process. <p>Most practical methods use \\(\\mathcal{O}_\\text{stat}\\) because evaluating \\(\\mathcal{O}_\\text{model}\\) during the selection process is computationally expensive. The fundamental challenge of RPS is that the relationship between statistical fidelity and model outcome fidelity is complex and nonlinear: low statistical error does not guarantee accurate model results.</p>"},{"location":"unified_framework/#statistical-objectives-in-practice","title":"Statistical objectives in practice","text":"<p>Statistical fidelity is not a single concept. In this context, it decomposes into three distinct dimensions, each capturing a different aspect of how well the selection preserves the original data.</p> <p>1. Marginal distribution fidelity. The weighted selection should reproduce the overall distribution of each variable independently: annual means, load duration curves, quantile structures. This is the most commonly targeted fidelity dimension. When the downstream model question concerns aggregate outcomes (annualized system cost, total generation mix), marginal distributions are the primary concern. Metrics include:</p> <ul> <li>Wasserstein distance: how far apart are the full and selected marginal distributions?</li> <li>Duration curve NRMSE: how well does the weighted selection reconstruct the sorted load/generation profiles?</li> <li>Mean preservation: does the weighted selection reproduce the annual mean of each variable?</li> </ul> <p>2. Temporal pattern fidelity. Energy systems are sensitive not just to what values occur, but to when and how fast things change. Temporal pattern fidelity captures intra-period dynamics: diurnal shapes (solar noon peaks, evening demand ramps), ramp rates (the rate of change between consecutive time steps), and autocorrelation structure. A selection that matches the marginal distribution perfectly can still fail if it misses the characteristic temporal shapes, for instance by selecting periods with flat profiles when the full dataset contains sharp morning ramps. Metrics include:</p> <ul> <li>Diurnal profile MSE: does the selection reproduce the mean hourly shape of each variable?</li> <li>DTW distance: how similar are the temporal shapes (allowing for slight time shifts)?</li> <li>Ramp-rate statistics: does the selection preserve the distribution of hour-to-hour changes?</li> </ul> <p>3. Cross-variable dependency fidelity. Variables in energy systems do not evolve independently. Wind and solar output are often negatively correlated; demand tends to peak when solar generation ramps down; price spikes coincide with low VRES availability. Preserving these relationships is critical for models that involve cross-sectoral coupling, storage dispatch, or market dynamics. This dimension covers:</p> <ul> <li>Static correlations: does the selection preserve the overall correlation matrix across variables (Frobenius norm of the correlation matrix difference)?</li> <li>Joint temporal dynamics: does the selection preserve co-movement patterns, such as solar generation ramping up while demand ramps down, or wind output dropping during peak price hours? This goes beyond static correlations to capture the temporal co-evolution of variables. One natural measure is the correlation matrix of first-differences (ramp correlations), which quantifies how the rates of change across variables relate to each other.</li> </ul> <p>These three dimensions can be addressed both in \\(\\mathcal{F}\\) (by including ramp-rate statistics, cross-correlations, or derivative-based features in the feature space) and in \\(\\mathcal{O}\\) (by including score components that explicitly measure each fidelity type). In practice, a selection pipeline can address all three, either through the choice of features, the choice of objective components, or both.</p> <p>State-space coverage. Complementary to fidelity, the selection should span the diversity of conditions that occur in the full dataset, capturing distinct system states such as \"high wind + low demand,\" \"low VRES + peak demand,\" or \"shoulder season with storage cycling.\" This matters most when the model question concerns system adequacy, resilience, or identifying binding constraints. Metrics include:</p> <ul> <li>Diversity (mean pairwise distance in feature space): are the selected periods sufficiently different from each other?</li> <li>Coverage balance (uniformity of representation responsibilities): does each selected period \"cover\" a roughly equal portion of the full dataset?</li> <li>Centroid balance (distance from selection centroid to global center): does the selection avoid systematic bias toward one region of the feature space?</li> </ul> <p>Diversity and coverage metrics evaluate properties of the selection itself in feature space, rather than how well the representation matches the full dataset. They complement fidelity metrics by ensuring the selection is well-spread and balanced.</p> <p>Relationship between state-space coverage and cross-variable dependency fidelity. These two concepts address the same underlying concern \u2014 the multivariate joint structure of the data \u2014 but from different angles. Cross-variable dependency fidelity is a fidelity metric: it compares a statistical property (e.g., the correlation matrix) of the selected subset against the full dataset and asks \"how well does the selection reproduce the original relationships between variables?\" State-space coverage is a diversity metric: it evaluates properties of the selection itself (spread, balance) and asks \"does the selection span the range of joint conditions that occur?\" In practice, they tend to reinforce each other: a selection that covers the full state space well will often preserve cross-variable dependencies, and vice versa. But they can also diverge. A selection optimized purely for correlation fidelity might cluster in the center of the distribution, matching the correlation matrix while missing rare but operationally important joint states. Conversely, a selection optimized purely for diversity might capture extreme corners of the state space but distort the overall dependency structure. Treating them as separate objectives allows the modeler to target both aspects explicitly and inspect their trade-off.</p> <p>Combined objectives. In many applications, both statistical fidelity and state-space coverage matter simultaneously. A common scenario is the requirement that the selected periods carry equal weight (see \\(\\mathcal{R}_\\text{equal}\\) in Section 2.5), meaning the selection must be intrinsically representative and cannot rely on non-uniform weights to correct for bias. The selection must then:</p> <ol> <li>land close to the center of the data distribution (fidelity), and</li> <li>span a broad range of system states (coverage/diversity).</li> </ol> <p>These goals are in natural tension: optimizing for fidelity tends to select \"average\" periods, while optimizing for coverage tends to select \"extreme\" or \"boundary\" periods. Multi-objective optimization resolves this tension by computing the trade-off frontier (Pareto front) explicitly, letting the modeler choose a preferred balance. This provides a systematic alternative to multi-stage hybrid approaches (see Section 2.6, \\(\\mathcal{A}_\\text{hybrid}\\)).</p>"},{"location":"unified_framework/#24-component-3-selection-space-mathcals-what-we-are-picking","title":"2.4 Component 3: Selection Space (\\(\\mathcal{S}\\)) \u2014 What We Are Picking","text":"<p>The selection space defines the structural form of the output, i.e. what kind of object \\(x\\) is.</p> Variant Description \\(\\mathcal{S}_\\text{subset}\\) Historical subset. \\(x \\subset \\{1, \\ldots, N\\}\\) with \\(\\lvert x \\rvert = k\\). The output is a set of \\(k\\) actual periods from the original data. \\(\\mathcal{S}_\\text{synthetic}\\) Synthetic archetypes. \\(x = \\{p_1, \\ldots, p_k\\}\\) where each \\(p_j \\in \\mathbb{R}^{V \\times H}\\) is an artificial period (e.g., a cluster centroid). These may not correspond to any historical period. \\(\\mathcal{S}_\\text{chrono}\\) Chronological segments. \\(x = \\{(t_1, l_1), \\ldots, (t_k, l_k)\\}\\) where \\((t_j, l_j)\\) defines a contiguous segment of variable length \\(l_j\\) starting at time \\(t_j\\). Segments collectively cover the full timeline. <p>\\(\\mathcal{S}_\\text{subset}\\) is the most common choice because it guarantees that each representative period is a physically realistic, historical pattern. It is the natural output of k-medoids clustering, combinatorial search, and greedy selection methods.</p> <p>\\(\\mathcal{S}_\\text{synthetic}\\) arises from methods that construct artificial representatives, such as k-means clustering, where centroids are computed as averages over cluster members. When clustering is performed in \\(\\mathcal{F}_\\text{direct}\\) (raw time-series space), centroids are themselves time series and can be used directly as synthetic periods, though they may produce physically unrealistic profiles (e.g., smoothed-out peaks). When clustering is performed in a reduced feature space (\\(\\mathcal{F}_\\text{stat}\\) or \\(\\mathcal{F}_\\text{latent}\\)), the centroid exists in feature space, not in time-series space. Recovering a synthetic time series then requires an inverse mapping, for instance a decoder in the case of \\(\\mathcal{F}_\\text{latent}\\) (autoencoders). For \\(\\mathcal{F}_\\text{stat}\\), no natural inverse exists, which is one reason k-medoids (\\(\\mathcal{S}_\\text{subset}\\)) is generally preferred over k-means (\\(\\mathcal{S}_\\text{synthetic}\\)) when working with engineered features.</p> <p>\\(\\mathcal{S}_\\text{chrono}\\) preserves the chronological ordering of the original data, which is critical for models with long-duration storage or seasonal dynamics. It is the output of Chronological Time-Period Clustering (CTPC) and related methods.</p> <p>The choice of \\(\\mathcal{S}\\) interacts with other components: \\(\\mathcal{S}_\\text{subset}\\) pairs naturally with \\(\\mathcal{A}_\\text{comb}\\) (combinatorial search) and \\(\\mathcal{A}_\\text{construct}\\) (clustering with medoid selection), while \\(\\mathcal{S}_\\text{chrono}\\) requires specialized constructive algorithms that enforce contiguity.</p>"},{"location":"unified_framework/#25-component-4-representation-model-mathcalr-how-the-selection-represents-the-whole","title":"2.5 Component 4: Representation Model (\\(\\mathcal{R}\\)) \u2014 How the Selection Represents the Whole","text":"<p>Given a selection \\(x\\) of \\(k\\) representatives, the representation model defines how the full dataset \\(D\\) is approximated for use in the downstream model. The output of \\(\\mathcal{R}\\) is the reduced input to the downstream model: a set of representative periods and their associated weights or reconstruction rules.</p> Variant Description \\(\\mathcal{R}_\\text{equal}\\) Equal weighting. Each selected period receives weight \\(1/k\\). No assignment of original periods to representatives is performed. \\(\\mathcal{R}_\\text{hard}\\) Hard assignment. Each of the \\(N\\) original periods is assigned to a single closest representative. The weight of representative \\(j\\) is proportional to the number of periods assigned to it. \\(\\mathcal{R}_\\text{soft}\\) Blended representation. Each original period \\(i\\) is approximated as a weighted combination of all \\(k\\) representatives: \\(d_i \\approx \\sum_{j=1}^k w_{ij} \\cdot x_j\\)."},{"location":"unified_framework/#mathcalr_textequal-equal-weighting","title":"\\(\\mathcal{R}_\\text{equal}\\): Equal weighting","text":"<p>The simplest representation model. Each of the \\(k\\) selected periods is treated identically in the downstream model, with weight \\(1/k\\).</p> <p>This choice is common in practice when the downstream model cannot accommodate non-uniform period weights, for instance when the model is formulated to run each representative period once and the results are simply averaged. It places the strongest requirements on the selection itself: since weights cannot compensate for a biased selection, the \\(k\\) periods must be intrinsically representative. The aggregate statistics of the equally-weighted selection must match those of the full dataset, while simultaneously covering the relevant state space.</p>"},{"location":"unified_framework/#mathcalr_texthard-hard-assignment","title":"\\(\\mathcal{R}_\\text{hard}\\): Hard assignment","text":"<p>Each original period is mapped to its single closest representative. The weight of representative \\(j\\) reflects the number of original periods it represents:</p> \\[w_j \\;=\\; \\frac{|\\{i : r(i) = j\\}|}{N}, \\quad \\text{where } r(i) = \\underset{j \\in x}{\\arg\\min}\\; \\|z_i - z_j\\|\\] <p>This is the standard output of clustering methods. The downstream model runs \\(k\\) periods, each weighted by \\(w_j\\), so that the weighted aggregate approximates the full-year result.</p> <p>The assignment function \\(r(\\cdot)\\) and distance metric can vary: Euclidean distance on features, DTW distance on raw series, RBF kernel similarity, or PCA-based k-medoids assignment. The choice of assignment method is a sub-decision within \\(\\mathcal{R}_\\text{hard}\\).</p> <p>Variants also exist where the weights are not simply cluster sizes but are optimized (e.g., via MILP) to minimize some reconstruction error, decoupling the weight computation from the cluster assignment.</p>"},{"location":"unified_framework/#mathcalr_textsoft-blended-representation","title":"\\(\\mathcal{R}_\\text{soft}\\): Blended representation","text":"<p>Each original period \\(i\\) is approximated as a weighted combination of all \\(k\\) representatives:</p> \\[d_i \\;\\approx\\; \\sum_{j=1}^k w_{ij} \\cdot x_j\\] <p>with constraints on the weight vectors \\(w_i = (w_{i1}, \\ldots, w_{ik})\\), typically:</p> <ul> <li>\\(w_{ij} \\geq 0\\) (non-negativity), and optionally</li> <li>\\(\\sum_j w_{ij} = 1\\) (convex combination) or \\(\\sum_j w_{ij}\\) unconstrained (conic combination).</li> </ul> <p>The blended representation provides a much richer approximation: rather than collapsing each original period to a single representative, it reconstructs each one from the full basis of representatives. This is the key idea in hull clustering, where the selected periods form the vertices of a polytope that spans the data.</p> <p>However, \\(\\mathcal{R}_\\text{soft}\\) requires the downstream model to handle blended inputs: the time-series parameters (load, VRES profiles) for each modeled period are themselves weighted sums of the representative profiles. This requires a different ESM formulation than the standard weighted-period approach.</p>"},{"location":"unified_framework/#duration-scaling","title":"Duration scaling","text":"<p>When periods have unequal durations (for instance, calendar months ranging from 28 to 31 days), the raw responsibility weights produced by any \\(\\mathcal{R}\\) variant should be adjusted to reflect the actual time span each representative covers. Without this adjustment, a representative month of 28 days and one of 31 days would receive the same weight despite covering different fractions of the year.</p> <p>Duration scaling is not a separate representation model but a practical post-processing refinement applicable to \\(\\mathcal{R}_\\text{equal}\\), \\(\\mathcal{R}_\\text{hard}\\), and \\(\\mathcal{R}_\\text{soft}\\) alike. The adjustment is straightforward: each weight \\(w_j\\) is multiplied by the duration \\(l_j\\) of period \\(j\\), and the result is renormalized so that the weights sum to the total time horizon. For \\(\\mathcal{R}_\\text{equal}\\) with monthly periods, this means a 31-day month receives slightly more weight than a 28-day month, ensuring that the weighted reconstruction accounts for the correct number of hours per period.</p>"},{"location":"unified_framework/#26-component-5-search-algorithm-mathcala-how-we-find-the-solution","title":"2.6 Component 5: Search Algorithm (\\(\\mathcal{A}\\)) \u2014 How We Find the Solution","text":"<p>The search algorithm is the computational procedure that finds \\(x^*\\) (or an approximation of it). Different algorithms impose different requirements on the other components and exhibit different computational trade-offs.</p> Variant Description \\(\\mathcal{A}_\\text{comb}\\) Combinatorial search. Enumerate or sample candidate selections from \\(\\mathcal{S}\\), evaluate each via \\(\\mathcal{O}\\), select the best. \\(\\mathcal{A}_\\text{construct}\\) Constructive algorithms. Build the solution incrementally: clustering (k-means, k-medoids, hierarchical) or greedy selection (forward selection, hull vertex identification). \\(\\mathcal{A}_\\text{optim}\\) Mathematical programming. Formulate the selection as an optimization problem (typically MILP) and solve it with a general-purpose solver. \\(\\mathcal{A}_\\text{hybrid}\\) Multi-stage or composite. Combine multiple algorithms or objectives, e.g., first select \"typical\" periods via clustering, then add \"extreme\" periods via optimization."},{"location":"unified_framework/#mathcala_textcomb-combinatorial-search","title":"\\(\\mathcal{A}_\\text{comb}\\): Combinatorial search","text":"<p>The most flexible approach. Candidate selections are generated (exhaustively or via metaheuristics such as genetic algorithms, simulated annealing, or random sampling) and evaluated against \\(\\mathcal{O}\\). This decouples the search from the objective: any \\(\\mathcal{O}\\) can be used, including multi-objective formulations.</p> <p>The main limitation is scalability. The number of possible \\(k\\)-subsets from \\(N\\) periods is \\(\\binom{N}{k}\\), which grows combinatorially. Exhaustive enumeration is feasible only for small problems (e.g., \\(\\binom{52}{8} \\approx 7.5 \\times 10^8\\) is already impractical without further constraints). Metaheuristics can handle larger problems but provide no optimality guarantees.</p> <p>A key advantage of \\(\\mathcal{A}_\\text{comb}\\) is its compatibility with multi-objective optimization: by evaluating each candidate on multiple objectives, it naturally produces Pareto fronts, enabling the modeler to inspect trade-offs explicitly.</p> <p>Selection policies. When the objective is multi-dimensional (\\(\\mathcal{O} \\to \\mathbb{R}^m\\)), the combinatorial search produces a table of \\(m\\) scores per candidate. A selection policy resolves this into a single winner. Common strategies include weighted-sum aggregation (simple but requires choosing weights a priori), utopia-distance methods (select the Pareto-optimal point closest to the ideal), and max-min fairness (select the Pareto-optimal point that maximizes the worst-performing objective). The choice of policy is a sub-decision within \\(\\mathcal{A}_\\text{comb}\\) that can significantly affect which selection is returned, even when the Pareto front is identical.</p> <p>Structured candidate generation. The scalability of \\(\\mathcal{A}_\\text{comb}\\) can be improved by constraining the search space at the generation stage. Beyond simple group quotas (e.g., \"one period per season\"), hierarchical generation enables evaluation at a finer granularity than the selection unit. For example, features may be computed at daily resolution while the selection operates at the monthly level: each candidate is a set of complete months, but the objective evaluates the daily data within those months. This hierarchical approach, combining group quotas with multi-resolution evaluation, dramatically reduces the combinatorial space while preserving the flexibility of the generate-and-test paradigm.</p>"},{"location":"unified_framework/#mathcala_textconstruct-constructive-algorithms","title":"\\(\\mathcal{A}_\\text{construct}\\): Constructive algorithms","text":"<p>These algorithms build the solution incrementally rather than evaluating complete candidates.</p> <p>Clustering algorithms (k-means, k-medoids, hierarchical agglomerative) iteratively refine an assignment of periods to clusters. The output is a set of cluster representatives (centroids for k-means, medoids for k-medoids) and an implicit hard assignment. The objective is built into the algorithm: k-means minimizes within-cluster sum of squares; k-medoids minimizes within-cluster sum of distances.</p> <p>Note: k-medoids selects actual data points as representatives (\\(\\mathcal{S}_\\text{subset}\\)), while k-means produces centroids (\\(\\mathcal{S}_\\text{synthetic}\\)). Hierarchical methods can produce either, depending on how representatives are extracted from the dendrogram.</p> <p>Greedy algorithms build the selection one element at a time. At each step, the period that provides the greatest improvement to the objective is added. Hull clustering uses a greedy strategy to identify extreme points (hull vertices) that define the boundary of the data distribution.</p> <p>Constructive algorithms are typically fast and scalable, but they couple the search algorithm tightly with the objective. You cannot easily swap in a different \\(\\mathcal{O}\\) without changing the algorithm itself.</p>"},{"location":"unified_framework/#mathcala_textoptim-mathematical-programming","title":"\\(\\mathcal{A}_\\text{optim}\\): Mathematical programming","text":"<p>The selection problem is formulated as a mathematical optimization problem, typically a Mixed-Integer Linear Program (MILP), with binary decision variables \\(y_i \\in \\{0, 1\\}\\) indicating whether period \\(i\\) is selected:</p> \\[\\min \\;\\mathcal{O}(\\ldots) \\quad \\text{s.t.} \\quad \\sum_{i=1}^N y_i = k, \\quad \\text{and problem-specific constraints}\\] <p>This approach can provide global optimality guarantees (within solver tolerances) and naturally handles complex constraints. It is the standard approach for duration-curve-based selection and interregional optimization.</p> <p>Note that structural constraints such as \"select at least one period from each season\" do not necessarily require mathematical programming. They can also be enforced within \\(\\mathcal{A}_\\text{comb}\\) by constraining the candidate generation itself, for example by enumerating only combinations that satisfy group quotas.</p> <p>The limitation is that the objective must be expressible in a form compatible with the solver (linear or quadratic for LP/MILP). Complex, nonlinear objectives or multi-objective formulations may be difficult to encode.</p>"},{"location":"unified_framework/#mathcala_texthybrid-multi-stage-and-composite-approaches","title":"\\(\\mathcal{A}_\\text{hybrid}\\): Multi-stage and composite approaches","text":"<p>Hybrid approaches combine multiple algorithms or objectives in stages. A common pattern is:</p> <ol> <li>Select \\(k_1\\) \"typical\" periods using a clustering or combinatorial method.</li> <li>Select \\(k_2 = k - k_1\\) \"extreme\" or \"critical stress\" periods using optimization-based identification (e.g., identifying periods with the highest system stress via slack variables in a preliminary model run).</li> </ol> <p>This is pragmatic and widely used. Alternatively, the same goal (balancing aggregate fidelity with extreme-event coverage) can be pursued through multi-objective optimization within a single \\(\\mathcal{A}_\\text{comb}\\) framework, where both fidelity and coverage are explicit objectives. The multi-objective approach makes trade-offs transparent and lets the modeler retain full control over the balance, rather than committing to a fixed \\(k_1\\)/\\(k_2\\) split a priori.</p>"},{"location":"unified_framework/#3-methods-as-framework-instances","title":"3. Methods as Framework Instances","text":"<p>The framework's utility lies in its ability to decompose any RPS method into a specific \\((\\mathcal{F}, \\mathcal{O}, \\mathcal{S}, \\mathcal{R}, \\mathcal{A})\\) tuple, making implicit choices explicit and enabling direct comparison.</p>"},{"location":"unified_framework/#31-decomposition-table","title":"3.1 Decomposition Table","text":"<p>The table below classifies established methodologies. Each row is a specific instantiation of the five components.</p> Methodology \\(\\mathcal{F}\\) \\(\\mathcal{O}\\) \\(\\mathcal{S}\\) \\(\\mathcal{R}\\) \\(\\mathcal{A}\\) k-means \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): min. intra-cluster variance \\(\\mathcal{S}_\\text{synthetic}\\) (centroids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) k-medoids (PAM) \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): min. intra-cluster distance \\(\\mathcal{S}_\\text{subset}\\) (medoids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Hierarchical clustering \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): linkage criterion \\(\\mathcal{S}_\\text{subset}\\) or \\(\\mathcal{S}_\\text{synthetic}\\) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Duration curve MILP \\(\\mathcal{F}_\\text{direct}\\) \\(\\mathcal{O}_\\text{stat}\\): min. duration curve NRMSE \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) (optimized weights) \\(\\mathcal{A}_\\text{optim}\\) (MILP) Interregional MILP (NREL) \\(\\mathcal{F}_\\text{direct}\\) \\(\\mathcal{O}_\\text{stat}\\): min. regional errors \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) (optimized weights) \\(\\mathcal{A}_\\text{optim}\\) (MILP) Autoencoder (inputs only) \\(\\mathcal{F}_\\text{latent}\\) \\(\\mathcal{O}_\\text{stat}\\): min. latent distance \\(\\mathcal{S}_\\text{subset}\\) (medoids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Autoencoder (inputs + outputs) \\(\\mathcal{F}_\\text{model}\\) + \\(\\mathcal{F}_\\text{latent}\\) \\(\\mathcal{O}_\\text{stat}\\): min. latent distance \\(\\mathcal{S}_\\text{subset}\\) (medoids) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) DTW-based clustering \\(\\mathcal{F}_\\text{direct}\\) (DTW metric) \\(\\mathcal{O}_\\text{stat}\\): min. DTW intra-cluster \\(\\mathcal{S}_\\text{subset}\\) or \\(\\mathcal{S}_\\text{synthetic}\\) \\(\\mathcal{R}_\\text{hard}\\) (cluster size) \\(\\mathcal{A}_\\text{construct}\\) (clustering) Snippet algorithm \\(\\mathcal{F}_\\text{direct}\\) (subsequences) \\(\\mathcal{O}_\\text{stat}\\): min. subsequence distance \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{construct}\\) (greedy) CTPC \\(\\mathcal{F}_\\text{direct}\\) \\(\\mathcal{O}_\\text{stat}\\): min. intra-cluster distance \\(\\mathcal{S}_\\text{chrono}\\) \\(\\mathcal{R}_\\text{hard}\\) (implicit) \\(\\mathcal{A}_\\text{construct}\\) (hierarchical + contiguity) Hull clustering (blended) \\(\\mathcal{F}_\\text{direct}\\) or \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): min. projection error \\(\\mathcal{S}_\\text{subset}\\) (hull vertices) \\(\\mathcal{R}_\\text{soft}\\) (blended) \\(\\mathcal{A}_\\text{construct}\\) (greedy) Multi-objective Pareto \\(\\mathcal{F}_\\text{stat}\\) \\(\\mathcal{O}_\\text{stat}\\): multi-objective (Pareto) \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{equal}\\) or \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{comb}\\) (exhaustive / GA) Extreme event (slack vars) \\(\\mathcal{F}_\\text{model}\\) \\(\\mathcal{O}_\\text{model}\\): max. system stress \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{hybrid}\\) (model-in-loop) Hybrid (typical + extreme) mixed \\(\\mathcal{O}_\\text{stat}\\) + \\(\\mathcal{O}_\\text{model}\\) \\(\\mathcal{S}_\\text{subset}\\) \\(\\mathcal{R}_\\text{hard}\\) \\(\\mathcal{A}_\\text{hybrid}\\)"},{"location":"unified_framework/#32-observations","title":"3.2 Observations","text":"<p>Several insights emerge from the decomposition:</p> <ol> <li> <p>Most methods differ in only one or two components. Moving from k-medoids to autoencoder-based selection changes only \\(\\mathcal{F}\\) (from statistical to latent/model-informed). Moving from k-medoids to hull clustering changes \\(\\mathcal{R}\\) (from hard to soft) and \\(\\mathcal{A}\\) (from centroid-based to greedy). The other components remain the same. This makes trade-offs explicit and isolable.</p> </li> <li> <p>\\(\\mathcal{R}_\\text{soft}\\) is uncommon in the current literature. Almost all established methods use hard assignment or equal weighting. Blended representation (hull clustering) is a recent innovation that requires changes to the downstream model formulation.</p> </li> <li> <p>\\(\\mathcal{O}_\\text{model}\\) is the frontier. Most methods operate entirely on statistical objectives. Direct optimization for model outcome fidelity requires model-in-the-loop methods (extreme event identification via slack variables, autoencoder with model outputs), which are computationally expensive but represent the state of the art.</p> </li> <li> <p>Multi-objective optimization offers an alternative to hybrid approaches. The hybrid approach (typical + extreme) is pragmatic and widely used for balancing aggregate fidelity with extreme-event coverage. Multi-objective formulations provide an alternative: they compute the full trade-off frontier and let the modeler choose, rather than committing to a fixed split.</p> </li> <li> <p>The choice of \\(\\mathcal{A}\\) often dominates the method's identity. Methods are typically named after their search algorithm (k-means, MILP, genetic algorithm), but the other components, particularly \\(\\mathcal{F}\\) and \\(\\mathcal{O}\\), often have a greater impact on the quality of the result. The framework redirects attention from how the search is conducted to what is being optimized and how quality is measured.</p> </li> </ol>"},{"location":"unified_framework/#4-discussion","title":"4. Discussion","text":""},{"location":"unified_framework/#41-component-interactions-and-coupling","title":"4.1 Component Interactions and Coupling","text":"<p>While the five components are conceptually independent, certain combinations are tightly coupled in practice:</p> <ul> <li>\\(\\mathcal{A}_\\text{construct}\\) couples \\(\\mathcal{A}\\) with \\(\\mathcal{O}\\): clustering algorithms have their objective function built in (e.g., k-means minimizes within-cluster variance). You cannot freely swap \\(\\mathcal{O}\\) without changing \\(\\mathcal{A}\\).</li> <li>\\(\\mathcal{R}_\\text{soft}\\) couples \\(\\mathcal{R}\\) with the downstream model: the ESM must be formulated to accept blended inputs, which is a non-trivial modeling change.</li> <li>\\(\\mathcal{F}_\\text{model}\\) couples \\(\\mathcal{F}\\) with a preliminary model run: model-informed features require access to a simplified ESM, creating a dependency between the feature engineering pipeline and the modeling workflow.</li> </ul> <p>The framework makes these couplings visible, helping practitioners understand which components they can modify independently and which require coordinated changes.</p>"},{"location":"unified_framework/#42-practical-guidance","title":"4.2 Practical Guidance","text":"<p>The choice of components should be guided by the modeling question:</p> <ul> <li>If aggregate cost accuracy is the priority and non-uniform period weights are acceptable: \\(\\mathcal{O}_\\text{stat}\\) with distributional metrics, \\(\\mathcal{R}_\\text{hard}\\) with optimized weights, \\(\\mathcal{A}_\\text{optim}\\) (duration curve MILP).</li> <li>If equal-weight periods are required: \\(\\mathcal{R}_\\text{equal}\\), combined with a multi-objective \\(\\mathcal{O}\\) that balances aggregate fidelity with coverage/diversity, and \\(\\mathcal{A}_\\text{comb}\\) for explicit trade-off analysis.</li> <li>If the model has significant storage or temporal coupling: \\(\\mathcal{F}\\) should capture temporal dynamics (DTW, ramp features), or \\(\\mathcal{S}_\\text{chrono}\\) should be used to preserve chronology.</li> <li>If the problem space is very large (e.g., selecting 10 periods from 365 days): \\(\\mathcal{A}_\\text{construct}\\) (clustering) or \\(\\mathcal{A}_\\text{optim}\\) (MILP) for scalability; exhaustive \\(\\mathcal{A}_\\text{comb}\\) is infeasible.</li> <li>If model outcome fidelity is critical and computational budget allows: \\(\\mathcal{F}_\\text{model}\\) with autoencoder, or \\(\\mathcal{A}_\\text{hybrid}\\) with slack-variable-based extreme identification.</li> </ul>"},{"location":"unified_framework/#43-open-questions","title":"4.3 Open Questions","text":"<p>Several aspects of the framework invite further investigation:</p> <ol> <li>Systematic benchmarking. The framework enables, and calls for, controlled experiments comparing different component combinations on the same datasets and downstream models.</li> <li>Better proxies for \\(\\mathcal{O}_\\text{model}\\). Developing statistical objectives that are stronger predictors of model outcome fidelity, without requiring model-in-the-loop evaluation, remains a major research gap.</li> <li>Automated component selection. Can the best \\((\\mathcal{F}, \\mathcal{O}, \\mathcal{S}, \\mathcal{R}, \\mathcal{A})\\) tuple be selected automatically based on problem characteristics?</li> <li>\\(\\mathcal{R}_\\text{soft}\\) adoption. Blended representations promise higher fidelity but require ESM reformulation. Quantifying the fidelity gain is important for motivating this effort.</li> </ol>"},{"location":"unified_framework/#5-conclusion","title":"5. Conclusion","text":"<p>The field of representative period selection for energy time series has produced a rich and growing body of methods, each designed with care for specific use cases. However, these methods are typically presented as monolithic procedures, which obscures their shared structure and makes systematic comparison difficult.</p> <p>The five-component framework proposed in this paper (Feature Space, Objective, Selection Space, Representation Model, and Search Algorithm) provides a common structure for understanding any RPS method. By decomposing methods into their fundamental choices, the framework:</p> <ul> <li>makes implicit assumptions explicit,</li> <li>enables direct, component-level comparison between methods,</li> <li>guides practitioners in assembling custom pipelines suited to their specific modeling questions, and</li> <li>provides an architectural blueprint for modular software design.</li> </ul> <p>The framework does not claim that one component combination is universally superior. It provides the vocabulary and structure needed to make informed, transparent choices, trying to move from ad-hoc method selection toward systematic, principled design of representative period selection pipelines.</p>"},{"location":"unified_framework/#references","title":"References","text":"<ol> <li>Warren B. Powell (2017). A unified framework for stochastic optimization. European Journal of Operational Research, 275(3)</li> <li>Hoffmann et al. (2020). A Review on Time Series Aggregation Methods for Energy System Models. Energies, 13(3), 641.</li> <li>Teichgraeber &amp; Brandt (2022). Time-series aggregation for the optimization of energy systems: Goals, challenges, approaches, and opportunities. Renewable and Sustainable Energy Reviews, 157, 111984.</li> <li>Nahmmacher et al. (2016). Carpe diem: A novel approach to select representative days for long-term power system modeling. Energy, 112, 430--442.</li> <li>Barbar &amp; Mallapragada (2022). Representative period selection for power system planning using autoencoder-based dimensionality reduction. arXiv:2204.13608.</li> <li>Brown et al. (2025). An Interregional Optimization Approach for Time Series Aggregation in Continent-Scale Electricity System Models. Energy, 324, 135830.</li> <li>Anderson et al. (2024). On the Selection of Intermediate Length Representative Periods for Capacity Expansion. arXiv:2401.02888.</li> <li>Neustroev et al. (2025). Hull Clustering with Blended Representative Periods for Energy System Optimization Models. arXiv:2508.21641.</li> <li>Kotzur et al. (2018). Impact of different time series aggregation methods on optimal energy system design. Renewable Energy, 117, 474--487.</li> <li>Bahl et al. (2018). Typical Periods for Two-Stage Synthesis by Time-Series Aggregation with Bounded Error in Objective Function. Frontiers in Energy Research, 5, 35.</li> <li>Poncelet et al. (2017). Selecting Representative Days for Capturing the Implications of Integrating Intermittent Renewables in Generation Expansion Planning Problems. IEEE Transactions on Power Systems, 32(3), 1936--1948.</li> </ol>"},{"location":"workflow/","title":"Workflow Types","text":""},{"location":"workflow/#the-three-generalized-workflows","title":"The Three Generalized Workflows","text":"<p>Here are the three primary types of workflows that energy-repset supports. Each uses the five core modules in a slightly different sequence and with different emphasis.</p>"},{"location":"workflow/#workflow-1-generate-and-test","title":"Workflow 1: Generate-and-Test","text":"<p>This is the classic combinatorial search approach. Its philosophy is to create many candidate solutions, evaluate each one thoroughly, and then use a clear policy to select the best.</p> <ul> <li>Status: Fully implemented in the current software.</li> <li>Examples: Exhaustive (brute-force) search, Genetic Algorithms.</li> <li>How the Modules are Used:</li> <li>SearchAlgorithm (A): Its main role is to generate candidate subsets. This is often delegated to a CombinationGenerator (which can handle constraints like \"one per season\").</li> <li>ObjectiveSet (O): Used intensively. It is called to evaluate every single candidate subset generated by the search algorithm.</li> <li>SelectionPolicy (Pi): Used at the very end. After the search is complete, it receives a full table of all candidates and their scores, and applies its rule (e.g., WeightedSum) to pick the winner.</li> <li>Summary: There is a clear, linear sequence: the SearchAlgorithm generates, the ObjectiveSet evaluates, and the Policy decides.</li> </ul> <p>Key Modules:</p> Role Implementation Import Search <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <code>energy_repset.search_algorithms</code> Candidates <code>ExhaustiveCombiGen</code>, <code>GroupQuotaCombiGen</code>, hierarchical variants <code>energy_repset.combi_gens</code> Evaluation <code>ObjectiveSet</code> with <code>ScoreComponent</code> instances <code>energy_repset.objectives</code> Decision <code>WeightedSumPolicy</code>, <code>ParetoMaxMinStrategy</code>, <code>ParetoUtopiaPolicy</code> <code>energy_repset.selection_policies</code> <p>Examples: Ex1: Getting Started, Ex2: Feature Space, Ex3: Hierarchical Selection, Ex4: Representation Models, Ex5: Multi-Objective</p>"},{"location":"workflow/#workflow-2-constructive","title":"Workflow 2: Constructive","text":"<p>This approach builds a solution directly and iteratively, rather than testing pre-made combinations. It's generally much faster than the Generate-and-Test workflow.</p> <ul> <li>Status: Implemented. Three constructive algorithms are available.</li> <li>Examples: Hull Clustering, CTPC (Chronological Time-Period Clustering), Snippet Algorithm.</li> <li>How the Modules are Used:</li> <li>SearchAlgorithm (A): This is the star of the show. It contains the entire complex logic of the method (e.g., the k-medoids algorithm). It has its own internal objective (e.g., minimize intra-cluster distance) that guides its construction process.</li> <li>ObjectiveSet (O): Not used during the search. Its role shifts to post-hoc evaluation. After the SearchAlgorithm has constructed a final solution, the ObjectiveSet is used to give it a standardized score. This is crucial for comparing its result against the results from other workflow types.</li> <li>SelectionPolicy (Pi): Bypassed entirely. The constructive nature of the algorithm is its own policy; the final result is the direct output of the construction process, with no separate decision step needed.</li> <li>Summary: The SearchAlgorithm does all the heavy lifting to directly build a solution. The ObjectiveSet is only used for final validation.</li> </ul> <p>Key Modules:</p> Role Implementation Import Search <code>HullClusteringSearch</code> <code>energy_repset.search_algorithms</code> Search <code>CTPCSearch</code> <code>energy_repset.search_algorithms</code> Search <code>SnippetSearch</code> <code>energy_repset.search_algorithms</code> Protocol <code>SearchAlgorithm</code> (structural typing) <code>energy_repset.search_algorithms</code> Evaluation <code>ObjectiveSet</code> (post-hoc only) <code>energy_repset.objectives</code> <p>Examples: Ex6: K-Medoids Clustering and Ex7: Constructive Algorithms</p> <p>For algorithm details, see Constructive Algorithms.</p>"},{"location":"workflow/#workflow-3-direct-optimization","title":"Workflow 3: Direct Optimization","text":"<p>This is the most sophisticated workflow. It formulates the entire selection problem as a single, large-scale mathematical optimization problem and uses a dedicated solver to find the globally optimal solution.</p> <ul> <li>Status: Not yet implemented in the current software.</li> <li>Examples: Mixed-Integer Linear Programming (MILP) to select periods that best reconstruct an annual duration curve.</li> <li>How the Modules are Used:</li> <li>SearchAlgorithm (A): Its main job is to act as a bridge to a mathematical programming solver (e.g., Gurobi, HiGHS). It translates the user's problem into the strict mathematical language of the solver.</li> <li>ObjectiveSet (O): Its concepts (e.g., \"minimize NRMSE\") are translated into the formal mathematical objective function of the optimization model. It is deeply embedded within the SearchAlgorithm's formulation.</li> <li>SelectionPolicy (Pi): Bypassed entirely. The solver's goal is to find the single optimal solution that minimizes the mathematical objective. The decision is inherent in the optimization process.</li> <li>Summary: The problem is handed off to a powerful external solver. The main work is in the formulation of the problem within the SearchAlgorithm module.</li> </ul> <p>Key Modules:</p> Role Implementation Status Search MILP-based <code>SearchAlgorithm</code> Planned Protocol <code>SearchAlgorithm</code> (structural typing) <code>energy_repset.search_algorithms</code> Solver External (Gurobi, HiGHS, etc.) N/A <p>Planned algorithms: MILP-based duration curve reconstruction, MILP with temporal coupling constraints.</p>"},{"location":"workflow/#the-workflow-dataclass","title":"The Workflow Dataclass","text":"<p>All three workflow types share the same assembly pattern. The <code>Workflow</code> dataclass bundles the three runtime components -- a <code>FeatureEngineer</code>, a <code>SearchAlgorithm</code>, and a <code>RepresentationModel</code> -- into a single object that <code>RepSetExperiment</code> can execute:</p> <pre><code>import energy_repset as rep\n\nworkflow = rep.Workflow(\n    feature_engineer=feature_engineer,   # F: transforms raw data into features\n    search_algorithm=search_algorithm,   # A: finds optimal selection (uses O internally)\n    representation_model=representation_model,  # R: calculates weights\n)\n\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n</code></pre> <p>The <code>Workflow</code> is intentionally thin: it holds references to components, not logic. The orchestration logic lives in <code>RepSetExperiment.run()</code>, which calls the components in sequence. This keeps each component independently testable and swappable.</p> <p>Practical note on features. Feature normalization and variable weighting are handled within the <code>FeatureEngineer</code> component (F). The choice of F determines what distances and similarities mean for the downstream search: two periods that appear similar under one feature space may look very different under another. See the Configuration Advisor for guidance on normalization and weighting.</p> <p>For the full component catalog, see Modules &amp; Components.</p>"},{"location":"api/","title":"API Reference","text":"Module Description Context &amp; Slicing <code>ProblemContext</code> data container and <code>TimeSlicer</code> Workflow &amp; Experiment <code>Workflow</code>, <code>RepSetExperiment</code>, and <code>RepSetResult</code> Feature Engineering Feature engineer protocol and implementations Objectives <code>ObjectiveSet</code>, <code>ObjectiveSpec</code>, and <code>ScoreComponent</code> base Score Components All concrete score component implementations Combination Generators <code>CombinationGenerator</code> protocol and implementations Selection Policies <code>SelectionPolicy</code> protocol and implementations Search Algorithms <code>SearchAlgorithm</code> protocol and implementations Representation <code>RepresentationModel</code> protocol and implementations Diagnostics Visualization classes for feature space, scores, and results"},{"location":"api/combi_gens/","title":"Combination Generators","text":""},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator","title":"CombinationGenerator","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for generating and counting combinations of candidate slices.</p> <p>This protocol defines the interface for combination generators used in Generate-and-Test workflows. Implementations determine which k-element subsets of candidate slices should be evaluated.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>Number of elements in each combination to generate.</p> Source code in <code>energy_repset/combi_gens/combination_generator.py</code> <pre><code>class CombinationGenerator(Protocol):\n    \"\"\"Protocol for generating and counting combinations of candidate slices.\n\n    This protocol defines the interface for combination generators used in\n    Generate-and-Test workflows. Implementations determine which k-element\n    subsets of candidate slices should be evaluated.\n\n    Attributes:\n        k: Number of elements in each combination to generate.\n    \"\"\"\n\n    k: int\n\n    def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n        \"\"\"Generate k-combinations from the candidate slices.\n\n        Args:\n            unique_slices: Sequence of candidate slice labels.\n\n        Yields:\n            Tuples of length k representing candidate selections.\n        \"\"\"\n        ...\n\n    def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n        \"\"\"Count the total number of combinations that will be generated.\n\n        Args:\n            unique_slices: Sequence of candidate slice labels.\n\n        Returns:\n            Total number of k-combinations.\n        \"\"\"\n        ...\n\n    def combination_is_valid(self, combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool:\n        \"\"\"Check if a combination is valid according to generator constraints.\n\n        Args:\n            combination: Tuple of slice labels to validate.\n            unique_slices: Sequence of candidate slice labels.\n\n        Returns:\n            True if the combination satisfies the generator's constraints.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate k-combinations from the candidate slices.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples of length k representing candidate selections.</p> Source code in <code>energy_repset/combi_gens/combination_generator.py</code> <pre><code>def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n    \"\"\"Generate k-combinations from the candidate slices.\n\n    Args:\n        unique_slices: Sequence of candidate slice labels.\n\n    Yields:\n        Tuples of length k representing candidate selections.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count the total number of combinations that will be generated.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of k-combinations.</p> Source code in <code>energy_repset/combi_gens/combination_generator.py</code> <pre><code>def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n    \"\"\"Count the total number of combinations that will be generated.\n\n    Args:\n        unique_slices: Sequence of candidate slice labels.\n\n    Returns:\n        Total number of k-combinations.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.CombinationGenerator.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if a combination is valid according to generator constraints.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the combination satisfies the generator's constraints.</p> Source code in <code>energy_repset/combi_gens/combination_generator.py</code> <pre><code>def combination_is_valid(self, combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool:\n    \"\"\"Check if a combination is valid according to generator constraints.\n\n    Args:\n        combination: Tuple of slice labels to validate.\n        unique_slices: Sequence of candidate slice labels.\n\n    Returns:\n        True if the combination satisfies the generator's constraints.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen","title":"ExhaustiveCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate all k-combinations of the candidate slices.</p> <p>This generator produces every possible k-element subset using itertools.combinations. It is suitable for small problem sizes where the total number of combinations (n choose k) is computationally feasible.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of elements in each combination.</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of elements per combination.</p> Note <p>The count is computed via binomial coefficient (n choose k) and matches the number of yielded combinations exactly.</p> <p>Examples:</p> <p>Generate all 3-month combinations from a year:</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; months = [pd.Period('2024-01', 'M'), pd.Period('2024-02', 'M'),\n...           pd.Period('2024-03', 'M'), pd.Period('2024-04', 'M')]\n&gt;&gt;&gt; generator = ExhaustiveCombiGen(k=3)\n&gt;&gt;&gt; generator.count(months)  # 4 choose 3\n    4\n&gt;&gt;&gt; list(generator.generate(months))\n    [(Period('2024-01', 'M'), Period('2024-02', 'M'), Period('2024-03', 'M')),\n     (Period('2024-01', 'M'), Period('2024-02', 'M'), Period('2024-04', 'M')),\n     (Period('2024-01', 'M'), Period('2024-03', 'M'), Period('2024-04', 'M')),\n     (Period('2024-02', 'M'), Period('2024-03', 'M'), Period('2024-04', 'M'))]\n</code></pre> Source code in <code>energy_repset/combi_gens/simple_exhaustive.py</code> <pre><code>class ExhaustiveCombiGen(CombinationGenerator):\n    \"\"\"Generate all k-combinations of the candidate slices.\n\n    This generator produces every possible k-element subset using\n    itertools.combinations. It is suitable for small problem sizes where\n    the total number of combinations (n choose k) is computationally feasible.\n\n    Args:\n        k: Number of elements in each combination.\n\n    Attributes:\n        k: Number of elements per combination.\n\n    Note:\n        The count is computed via binomial coefficient (n choose k) and matches\n        the number of yielded combinations exactly.\n\n    Examples:\n        Generate all 3-month combinations from a year:\n\n        &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; months = [pd.Period('2024-01', 'M'), pd.Period('2024-02', 'M'),\n        ...           pd.Period('2024-03', 'M'), pd.Period('2024-04', 'M')]\n        &gt;&gt;&gt; generator = ExhaustiveCombiGen(k=3)\n        &gt;&gt;&gt; generator.count(months)  # 4 choose 3\n            4\n        &gt;&gt;&gt; list(generator.generate(months))\n            [(Period('2024-01', 'M'), Period('2024-02', 'M'), Period('2024-03', 'M')),\n             (Period('2024-01', 'M'), Period('2024-02', 'M'), Period('2024-04', 'M')),\n             (Period('2024-01', 'M'), Period('2024-03', 'M'), Period('2024-04', 'M')),\n             (Period('2024-02', 'M'), Period('2024-03', 'M'), Period('2024-04', 'M'))]\n    \"\"\"\n\n    def __init__(self, k: int) -&gt; None:\n        \"\"\"Initialize exhaustive generator with target combination size.\n\n        Args:\n            k: Number of elements in each combination.\n        \"\"\"\n        self.k = k\n\n    def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n        \"\"\"Generate all k-combinations using itertools.combinations.\n\n        Args:\n            unique_slices: Sequence of candidate slice labels.\n\n        Yields:\n            All possible k-element tuples from unique_slices.\n        \"\"\"\n        yield from itertools.combinations(unique_slices, self.k)\n\n    def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n        \"\"\"Count total combinations using binomial coefficient.\n\n        Args:\n            unique_slices: Sequence of candidate slice labels.\n\n        Returns:\n            n choose k, where n is the number of unique slices.\n        \"\"\"\n        return math.comb(len(unique_slices), self.k)\n\n    def combination_is_valid(self, combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool:\n        \"\"\"Check if combination has exactly k elements from unique_slices.\n\n        Args:\n            combination: Tuple of slice labels to validate.\n            unique_slices: Sequence of candidate slice labels.\n\n        Returns:\n            True if combination has k elements all in unique_slices.\n        \"\"\"\n        if len(combination) != self.k:\n            return False\n        if any(s not in unique_slices for s in combination):\n            return False\n        return True\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(k: int) -&gt; None\n</code></pre> <p>Initialize exhaustive generator with target combination size.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of elements in each combination.</p> required Source code in <code>energy_repset/combi_gens/simple_exhaustive.py</code> <pre><code>def __init__(self, k: int) -&gt; None:\n    \"\"\"Initialize exhaustive generator with target combination size.\n\n    Args:\n        k: Number of elements in each combination.\n    \"\"\"\n    self.k = k\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate all k-combinations using itertools.combinations.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>All possible k-element tuples from unique_slices.</p> Source code in <code>energy_repset/combi_gens/simple_exhaustive.py</code> <pre><code>def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n    \"\"\"Generate all k-combinations using itertools.combinations.\n\n    Args:\n        unique_slices: Sequence of candidate slice labels.\n\n    Yields:\n        All possible k-element tuples from unique_slices.\n    \"\"\"\n    yield from itertools.combinations(unique_slices, self.k)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total combinations using binomial coefficient.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>n choose k, where n is the number of unique slices.</p> Source code in <code>energy_repset/combi_gens/simple_exhaustive.py</code> <pre><code>def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n    \"\"\"Count total combinations using binomial coefficient.\n\n    Args:\n        unique_slices: Sequence of candidate slice labels.\n\n    Returns:\n        n choose k, where n is the number of unique slices.\n    \"\"\"\n    return math.comb(len(unique_slices), self.k)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination has exactly k elements from unique_slices.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination has k elements all in unique_slices.</p> Source code in <code>energy_repset/combi_gens/simple_exhaustive.py</code> <pre><code>def combination_is_valid(self, combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool:\n    \"\"\"Check if combination has exactly k elements from unique_slices.\n\n    Args:\n        combination: Tuple of slice labels to validate.\n        unique_slices: Sequence of candidate slice labels.\n\n    Returns:\n        True if combination has k elements all in unique_slices.\n    \"\"\"\n    if len(combination) != self.k:\n        return False\n    if any(s not in unique_slices for s in combination):\n        return False\n    return True\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen","title":"GroupQuotaCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate combinations that respect exact quotas per group.</p> <p>This generator enforces that selections contain a specific number of elements from each group. It is useful for ensuring balanced representation across categories (e.g., seasons, must-have periods, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Total number of elements in each combination. Must equal sum of group quotas.</p> required <code>slice_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from each candidate slice to its group label.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Mapping from group label to the required count in the selection.</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of elements per combination.</p> <code>group_of</code> <p>Mapping from slice to group.</p> <code>group_quota</code> <p>Required count per group.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal k.</p> Note <p>Use this to enforce constraints like \"exactly one month per season\" or \"2 must-have periods plus 2 optional periods\".</p> <p>Examples:</p> <p>Example 1 - Seasonal constraints (one month per season):</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define months and their seasons\n&gt;&gt;&gt; months = [pd.Period(f'2024-{i:02d}', 'M') for i in range(1, 13)]\n&gt;&gt;&gt; season_map = {}\n&gt;&gt;&gt; for month in months:\n...     if month.month in [12, 1, 2]: season_map[month] = 'winter'\n...     elif month.month in [3, 4, 5]: season_map[month] = 'spring'\n...     elif month.month in [6, 7, 8]: season_map[month] = 'summer'\n...     else: season_map[month] = 'fall'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Select 4 months, one per season\n&gt;&gt;&gt; generator = GroupQuotaCombiGen(\n...     k=4,\n...     slice_to_group_mapping=season_map,\n...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n... )\n&gt;&gt;&gt; generator.count(months)  # 3 * 3 * 3 * 3 = 81 combinations\n    81\n</code></pre> <p>Example 2 - Optional and must-have categories:</p> <pre><code>&gt;&gt;&gt; # Force specific periods to be included\n&gt;&gt;&gt; months = [pd.Period(f'2024-{i:02d}', 'M') for i in range(1, 13)]\n&gt;&gt;&gt; group_mapping = {p: 'optional' for p in months}\n&gt;&gt;&gt; group_mapping[pd.Period('2024-01', 'M')] = 'must'\n&gt;&gt;&gt; group_mapping[pd.Period('2024-12', 'M')] = 'must'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Select 4 total: 2 must-have + 2 optional\n&gt;&gt;&gt; generator = GroupQuotaCombiGen(\n...     k=4,\n...     slice_to_group_mapping=group_mapping,\n...     group_quota={'optional': 2, 'must': 2}\n... )\n&gt;&gt;&gt; # All combinations will include Jan and Dec plus 2 from the other 10\n&gt;&gt;&gt; generator.count(months)  # 1 * 45 = 45 combinations\n45\n</code></pre> Source code in <code>energy_repset/combi_gens/simple_group_quota.py</code> <pre><code>class GroupQuotaCombiGen(CombinationGenerator):\n    \"\"\"Generate combinations that respect exact quotas per group.\n\n    This generator enforces that selections contain a specific number of elements\n    from each group. It is useful for ensuring balanced representation across\n    categories (e.g., seasons, must-have periods, etc.).\n\n    Args:\n        k: Total number of elements in each combination. Must equal sum of group quotas.\n        slice_to_group_mapping: Mapping from each candidate slice to its group label.\n        group_quota: Mapping from group label to the required count in the selection.\n\n    Attributes:\n        k: Number of elements per combination.\n        group_of: Mapping from slice to group.\n        group_quota: Required count per group.\n\n    Raises:\n        ValueError: If sum of group quotas does not equal k.\n\n    Note:\n        Use this to enforce constraints like \"exactly one month per season\" or\n        \"2 must-have periods plus 2 optional periods\".\n\n    Examples:\n        Example 1 - Seasonal constraints (one month per season):\n\n        &gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaCombiGen\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define months and their seasons\n        &gt;&gt;&gt; months = [pd.Period(f'2024-{i:02d}', 'M') for i in range(1, 13)]\n        &gt;&gt;&gt; season_map = {}\n        &gt;&gt;&gt; for month in months:\n        ...     if month.month in [12, 1, 2]: season_map[month] = 'winter'\n        ...     elif month.month in [3, 4, 5]: season_map[month] = 'spring'\n        ...     elif month.month in [6, 7, 8]: season_map[month] = 'summer'\n        ...     else: season_map[month] = 'fall'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Select 4 months, one per season\n        &gt;&gt;&gt; generator = GroupQuotaCombiGen(\n        ...     k=4,\n        ...     slice_to_group_mapping=season_map,\n        ...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n        ... )\n        &gt;&gt;&gt; generator.count(months)  # 3 * 3 * 3 * 3 = 81 combinations\n            81\n\n        Example 2 - Optional and must-have categories:\n\n        &gt;&gt;&gt; # Force specific periods to be included\n        &gt;&gt;&gt; months = [pd.Period(f'2024-{i:02d}', 'M') for i in range(1, 13)]\n        &gt;&gt;&gt; group_mapping = {p: 'optional' for p in months}\n        &gt;&gt;&gt; group_mapping[pd.Period('2024-01', 'M')] = 'must'\n        &gt;&gt;&gt; group_mapping[pd.Period('2024-12', 'M')] = 'must'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Select 4 total: 2 must-have + 2 optional\n        &gt;&gt;&gt; generator = GroupQuotaCombiGen(\n        ...     k=4,\n        ...     slice_to_group_mapping=group_mapping,\n        ...     group_quota={'optional': 2, 'must': 2}\n        ... )\n        &gt;&gt;&gt; # All combinations will include Jan and Dec plus 2 from the other 10\n        &gt;&gt;&gt; generator.count(months)  # 1 * 45 = 45 combinations\n        45\n    \"\"\"\n\n    def __init__(\n            self,\n            k: int,\n            slice_to_group_mapping: Dict[Hashable, Hashable],\n            group_quota: Dict[Hashable, int]\n    ) -&gt; None:\n        \"\"\"Initialize generator with group quotas.\n\n        Args:\n            k: Total number of elements in each combination.\n            slice_to_group_mapping: Mapping from slice to its group label.\n            group_quota: Required count per group.\n\n        Raises:\n            ValueError: If sum of group quotas does not equal k.\n        \"\"\"\n        self.k = k\n        self.group_of = slice_to_group_mapping\n        self.group_quota = group_quota\n\n        # Validate that quotas sum to k\n        if sum(group_quota.values()) != k:\n            raise ValueError(f\"Sum of group quotas ({sum(group_quota.values())}) must equal k ({k})\")\n\n    def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n        \"\"\"Generate combinations respecting group quotas.\n\n        Args:\n            unique_slices: Sequence of candidate slice labels.\n\n        Yields:\n            Tuples of length k where each group contributes exactly its quota.\n        \"\"\"\n        groups: Dict[Hashable, List[Hashable]] = {}\n        for c in unique_slices:\n            g = self.group_of[c]\n            groups.setdefault(g, []).append(c)\n        per_group_lists = []\n        for g, q in self.group_quota.items():\n            per_group_lists.append(list(itertools.combinations(groups.get(g, []), q)))\n        for tpl in itertools.product(*per_group_lists):\n            flat = tuple(itertools.chain.from_iterable(tpl))\n            if len(flat) == self.k:\n                yield flat\n\n    def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n        \"\"\"Count total combinations respecting group quotas.\n\n        Args:\n            unique_slices: Sequence of candidate slice labels.\n\n        Returns:\n            Product of binomial coefficients across all groups. For each group\n            with n members and quota q, contributes C(n, q) to the product.\n        \"\"\"\n        groups: Dict[Hashable, List[Hashable]] = {}\n        for c in unique_slices:\n            g = self.group_of[c]\n            groups.setdefault(g, []).append(c)\n        total = 1\n        for g, q in self.group_quota.items():\n            n = len(groups.get(g, []))\n            total *= math.comb(n, q)\n        return total\n\n    def combination_is_valid(self, combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool:\n        \"\"\"Check if combination satisfies group quotas.\n\n        Args:\n            combination: Tuple of slice labels to validate.\n            unique_slices: Sequence of candidate slice labels.\n\n        Returns:\n            True if combination has exactly k elements with each group contributing\n            its required quota.\n        \"\"\"\n        if len(combination) != self.k:\n            return False\n        if any(s not in unique_slices for s in combination):\n            return False\n\n        group_count = {group_name: 0 for group_name in self.group_quota.keys()}\n        for slice in combination:\n            group_count[self.group_of[slice]] +=1\n        if not all(group_count[g] == self.group_quota[g] for g in self.group_quota.keys()):\n            return False\n\n        return True\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(k: int, slice_to_group_mapping: dict[Hashable, Hashable], group_quota: dict[Hashable, int]) -&gt; None\n</code></pre> <p>Initialize generator with group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Total number of elements in each combination.</p> required <code>slice_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from slice to its group label.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count per group.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal k.</p> Source code in <code>energy_repset/combi_gens/simple_group_quota.py</code> <pre><code>def __init__(\n        self,\n        k: int,\n        slice_to_group_mapping: Dict[Hashable, Hashable],\n        group_quota: Dict[Hashable, int]\n) -&gt; None:\n    \"\"\"Initialize generator with group quotas.\n\n    Args:\n        k: Total number of elements in each combination.\n        slice_to_group_mapping: Mapping from slice to its group label.\n        group_quota: Required count per group.\n\n    Raises:\n        ValueError: If sum of group quotas does not equal k.\n    \"\"\"\n    self.k = k\n    self.group_of = slice_to_group_mapping\n    self.group_quota = group_quota\n\n    # Validate that quotas sum to k\n    if sum(group_quota.values()) != k:\n        raise ValueError(f\"Sum of group quotas ({sum(group_quota.values())}) must equal k ({k})\")\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate combinations respecting group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples of length k where each group contributes exactly its quota.</p> Source code in <code>energy_repset/combi_gens/simple_group_quota.py</code> <pre><code>def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n    \"\"\"Generate combinations respecting group quotas.\n\n    Args:\n        unique_slices: Sequence of candidate slice labels.\n\n    Yields:\n        Tuples of length k where each group contributes exactly its quota.\n    \"\"\"\n    groups: Dict[Hashable, List[Hashable]] = {}\n    for c in unique_slices:\n        g = self.group_of[c]\n        groups.setdefault(g, []).append(c)\n    per_group_lists = []\n    for g, q in self.group_quota.items():\n        per_group_lists.append(list(itertools.combinations(groups.get(g, []), q)))\n    for tpl in itertools.product(*per_group_lists):\n        flat = tuple(itertools.chain.from_iterable(tpl))\n        if len(flat) == self.k:\n            yield flat\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total combinations respecting group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Product of binomial coefficients across all groups. For each group</p> <code>int</code> <p>with n members and quota q, contributes C(n, q) to the product.</p> Source code in <code>energy_repset/combi_gens/simple_group_quota.py</code> <pre><code>def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n    \"\"\"Count total combinations respecting group quotas.\n\n    Args:\n        unique_slices: Sequence of candidate slice labels.\n\n    Returns:\n        Product of binomial coefficients across all groups. For each group\n        with n members and quota q, contributes C(n, q) to the product.\n    \"\"\"\n    groups: Dict[Hashable, List[Hashable]] = {}\n    for c in unique_slices:\n        g = self.group_of[c]\n        groups.setdefault(g, []).append(c)\n    total = 1\n    for g, q in self.group_quota.items():\n        n = len(groups.get(g, []))\n        total *= math.comb(n, q)\n    return total\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination satisfies group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of candidate slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination has exactly k elements with each group contributing</p> <code>bool</code> <p>its required quota.</p> Source code in <code>energy_repset/combi_gens/simple_group_quota.py</code> <pre><code>def combination_is_valid(self, combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool:\n    \"\"\"Check if combination satisfies group quotas.\n\n    Args:\n        combination: Tuple of slice labels to validate.\n        unique_slices: Sequence of candidate slice labels.\n\n    Returns:\n        True if combination has exactly k elements with each group contributing\n        its required quota.\n    \"\"\"\n    if len(combination) != self.k:\n        return False\n    if any(s not in unique_slices for s in combination):\n        return False\n\n    group_count = {group_name: 0 for group_name in self.group_quota.keys()}\n    for slice in combination:\n        group_count[self.group_of[slice]] +=1\n    if not all(group_count[g] == self.group_quota[g] for g in self.group_quota.keys()):\n        return False\n\n    return True\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen","title":"ExhaustiveHierarchicalCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate combinations where child slices are selected in complete parent groups.</p> <p>This generator enforces hierarchical selection: child slices (e.g., days) can only be selected as complete parent groups (e.g., months). It enables high-resolution features (e.g. per-day) while enforcing structural constraints at the parent level (e.g. months).</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Number of parent groups to select.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from each child slice to its parent group. Example: {Period('2024-01-01', 'D'): Period('2024-01', 'M'), ...}</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of parent groups per combination (same as parent_k for protocol compliance).</p> <code>parent_k</code> <p>Number of parent groups per combination.</p> <code>slice_to_parent</code> <p>Child to parent mapping.</p> Note <p>The <code>generate()</code> method yields flattened tuples of child slices, but internally enforces parent-level constraints. Use the factory method <code>from_slicers()</code> for automatic parent grouping based on TimeSlicer objects.</p> <p>Examples:</p> <p>Manual construction with custom grouping:</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define child-to-parent mapping\n&gt;&gt;&gt; slice_to_parent = {\n...     pd.Period('2024-01-01', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-01-02', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-02-01', 'D'): pd.Period('2024-02', 'M'),\n...     pd.Period('2024-02-02', 'D'): pd.Period('2024-02', 'M'),\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Select 2 months, but combinations contain days\n&gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen(\n...     parent_k=2,\n...     slice_to_parent_mapping=slice_to_parent\n... )\n&gt;&gt;&gt; gen.count(list(slice_to_parent.keys()))  # C(2, 2) = 1\n    1\n</code></pre> <p>Using factory method with TimeSlicer:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n...     parent_k=3,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     parent_slicer=parent_slicer\n... )\n&gt;&gt;&gt; unique_days = child_slicer.unique_slices(dates)\n&gt;&gt;&gt; gen.count(unique_days)  # C(12, 3) = 220 combinations of months\n    220\n</code></pre> Source code in <code>energy_repset/combi_gens/hierarchical_exhaustive.py</code> <pre><code>class ExhaustiveHierarchicalCombiGen(CombinationGenerator):\n    \"\"\"Generate combinations where child slices are selected in complete parent groups.\n\n    This generator enforces hierarchical selection: child slices (e.g., days) can only\n    be selected as complete parent groups (e.g., months). It enables high-resolution\n    features (e.g. per-day) while enforcing structural constraints at the parent level (e.g. months).\n\n    Args:\n        parent_k: Number of parent groups to select.\n        slice_to_parent_mapping: Mapping from each child slice to its parent group.\n            Example: {Period('2024-01-01', 'D'): Period('2024-01', 'M'), ...}\n\n    Attributes:\n        k: Number of parent groups per combination (same as parent_k for protocol compliance).\n        parent_k: Number of parent groups per combination.\n        slice_to_parent: Child to parent mapping.\n\n    Note:\n        The `generate()` method yields flattened tuples of child slices, but internally\n        enforces parent-level constraints. Use the factory method `from_slicers()`\n        for automatic parent grouping based on TimeSlicer objects.\n\n    Examples:\n        Manual construction with custom grouping:\n\n        &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define child-to-parent mapping\n        &gt;&gt;&gt; slice_to_parent = {\n        ...     pd.Period('2024-01-01', 'D'): pd.Period('2024-01', 'M'),\n        ...     pd.Period('2024-01-02', 'D'): pd.Period('2024-01', 'M'),\n        ...     pd.Period('2024-02-01', 'D'): pd.Period('2024-02', 'M'),\n        ...     pd.Period('2024-02-02', 'D'): pd.Period('2024-02', 'M'),\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Select 2 months, but combinations contain days\n        &gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen(\n        ...     parent_k=2,\n        ...     slice_to_parent_mapping=slice_to_parent\n        ... )\n        &gt;&gt;&gt; gen.count(list(slice_to_parent.keys()))  # C(2, 2) = 1\n            1\n\n        Using factory method with TimeSlicer:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n        &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n        &gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n        ...     parent_k=3,\n        ...     dt_index=dates,\n        ...     child_slicer=child_slicer,\n        ...     parent_slicer=parent_slicer\n        ... )\n        &gt;&gt;&gt; unique_days = child_slicer.unique_slices(dates)\n        &gt;&gt;&gt; gen.count(unique_days)  # C(12, 3) = 220 combinations of months\n            220\n    \"\"\"\n\n    def __init__(\n        self,\n        parent_k: int,\n        slice_to_parent_mapping: Dict[Hashable, Hashable]\n    ) -&gt; None:\n        \"\"\"Initialize hierarchical generator with child-to-parent mapping.\n\n        Args:\n            parent_k: Number of parent groups to select.\n            slice_to_parent_mapping: Dict mapping each child slice to its parent.\n        \"\"\"\n        self.parent_k = parent_k\n        self.k = parent_k  # For CombinationGenerator protocol compliance\n        self.slice_to_parent = slice_to_parent_mapping\n\n    @classmethod\n    def from_slicers(\n        cls,\n        parent_k: int,\n        dt_index: pd.DatetimeIndex,\n        child_slicer: TimeSlicer,\n        parent_slicer: TimeSlicer\n    ) -&gt; ExhaustiveHierarchicalCombiGen:\n        \"\"\"Factory method to create generator from child and parent TimeSlicer objects.\n\n        Args:\n            parent_k: Number of parent groups to select.\n            dt_index: DatetimeIndex of the time series data.\n            child_slicer: TimeSlicer defining child slice granularity (e.g., daily).\n            parent_slicer: TimeSlicer defining parent slice granularity (e.g., monthly).\n\n        Returns:\n            ExhaustiveHierarchicalCombinationGenerator with auto-constructed mappings.\n\n        Examples:\n            Select 4 months from a year of daily data:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n            &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n            &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n            &gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n            ...     parent_k=4,\n            ...     dt_index=dates,\n            ...     child_slicer=child_slicer,\n            ...     parent_slicer=parent_slicer\n            ... )\n            &gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # C(12, 4) = 495\n            495\n        \"\"\"\n        child_labels = child_slicer.labels_for_index(dt_index)\n        parent_labels = parent_slicer.labels_for_index(dt_index)\n\n        slice_to_parent = {}\n        for child, parent in zip(child_labels, parent_labels):\n            slice_to_parent[child] = parent\n\n        unique_slice_to_parent = {child: slice_to_parent[child] for child in child_labels.unique()}\n\n        return cls(parent_k=parent_k, slice_to_parent_mapping=unique_slice_to_parent)\n\n    def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n        \"\"\"Generate combinations of k parent groups, yielding flattened child slices.\n\n        Args:\n            unique_slices: Sequence of child slice labels.\n\n        Yields:\n            Tuples containing all child slices from k selected parent groups.\n        \"\"\"\n        parent_to_children: Dict[Hashable, list] = {}\n        for child in unique_slices:\n            parent = self.slice_to_parent[child]\n            parent_to_children.setdefault(parent, []).append(child)\n\n        parent_ids = sorted(parent_to_children.keys())\n        for parent_combi in itertools.combinations(parent_ids, self.parent_k):\n            child_slices = []\n            for parent_id in sorted(parent_combi):\n                child_slices.extend(parent_to_children[parent_id])\n            yield tuple(child_slices)\n\n    def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n        \"\"\"Count total number of parent-level combinations.\n\n        Args:\n            unique_slices: Sequence of child slice labels.\n\n        Returns:\n            C(n_parents, parent_k) where n_parents is the number of unique parent groups.\n        \"\"\"\n        unique_parents = set(self.slice_to_parent[s] for s in unique_slices)\n        n_parents = len(unique_parents)\n        return math.comb(n_parents, self.parent_k)\n\n    def combination_is_valid(\n        self,\n        combination: SliceCombination,\n        unique_slices: Sequence[Hashable]\n    ) -&gt; bool:\n        \"\"\"Check if combination represents exactly parent_k complete parent groups.\n\n        Args:\n            combination: Tuple of child slice labels to validate.\n            unique_slices: Sequence of all valid child slice labels.\n\n        Returns:\n            True if combination contains all children from exactly parent_k parent groups.\n        \"\"\"\n        if not all(c in unique_slices for c in combination):\n            return False\n\n        parent_to_children: Dict[Hashable, set] = {}\n        for child in unique_slices:\n            parent = self.slice_to_parent[child]\n            parent_to_children.setdefault(parent, set()).add(child)\n\n        parents_in_combi = set()\n        for child in combination:\n            if child not in self.slice_to_parent:\n                return False\n            parents_in_combi.add(self.slice_to_parent[child])\n\n        if len(parents_in_combi) != self.parent_k:\n            return False\n\n        expected_children = set()\n        for parent in parents_in_combi:\n            expected_children.update(parent_to_children[parent])\n\n        return set(combination) == expected_children\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(parent_k: int, slice_to_parent_mapping: dict[Hashable, Hashable]) -&gt; None\n</code></pre> <p>Initialize hierarchical generator with child-to-parent mapping.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Number of parent groups to select.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Dict mapping each child slice to its parent.</p> required Source code in <code>energy_repset/combi_gens/hierarchical_exhaustive.py</code> <pre><code>def __init__(\n    self,\n    parent_k: int,\n    slice_to_parent_mapping: Dict[Hashable, Hashable]\n) -&gt; None:\n    \"\"\"Initialize hierarchical generator with child-to-parent mapping.\n\n    Args:\n        parent_k: Number of parent groups to select.\n        slice_to_parent_mapping: Dict mapping each child slice to its parent.\n    \"\"\"\n    self.parent_k = parent_k\n    self.k = parent_k  # For CombinationGenerator protocol compliance\n    self.slice_to_parent = slice_to_parent_mapping\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.from_slicers","title":"from_slicers  <code>classmethod</code>","text":"<pre><code>from_slicers(parent_k: int, dt_index: DatetimeIndex, child_slicer: TimeSlicer, parent_slicer: TimeSlicer) -&gt; ExhaustiveHierarchicalCombiGen\n</code></pre> <p>Factory method to create generator from child and parent TimeSlicer objects.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Number of parent groups to select.</p> required <code>dt_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex of the time series data.</p> required <code>child_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining child slice granularity (e.g., daily).</p> required <code>parent_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining parent slice granularity (e.g., monthly).</p> required <p>Returns:</p> Type Description <code>ExhaustiveHierarchicalCombiGen</code> <p>ExhaustiveHierarchicalCombinationGenerator with auto-constructed mappings.</p> <p>Examples:</p> <p>Select 4 months from a year of daily data:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n...     parent_k=4,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     parent_slicer=parent_slicer\n... )\n&gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # C(12, 4) = 495\n495\n</code></pre> Source code in <code>energy_repset/combi_gens/hierarchical_exhaustive.py</code> <pre><code>@classmethod\ndef from_slicers(\n    cls,\n    parent_k: int,\n    dt_index: pd.DatetimeIndex,\n    child_slicer: TimeSlicer,\n    parent_slicer: TimeSlicer\n) -&gt; ExhaustiveHierarchicalCombiGen:\n    \"\"\"Factory method to create generator from child and parent TimeSlicer objects.\n\n    Args:\n        parent_k: Number of parent groups to select.\n        dt_index: DatetimeIndex of the time series data.\n        child_slicer: TimeSlicer defining child slice granularity (e.g., daily).\n        parent_slicer: TimeSlicer defining parent slice granularity (e.g., monthly).\n\n    Returns:\n        ExhaustiveHierarchicalCombinationGenerator with auto-constructed mappings.\n\n    Examples:\n        Select 4 months from a year of daily data:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveHierarchicalCombiGen\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n        &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n        &gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; gen = ExhaustiveHierarchicalCombiGen.from_slicers(\n        ...     parent_k=4,\n        ...     dt_index=dates,\n        ...     child_slicer=child_slicer,\n        ...     parent_slicer=parent_slicer\n        ... )\n        &gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # C(12, 4) = 495\n        495\n    \"\"\"\n    child_labels = child_slicer.labels_for_index(dt_index)\n    parent_labels = parent_slicer.labels_for_index(dt_index)\n\n    slice_to_parent = {}\n    for child, parent in zip(child_labels, parent_labels):\n        slice_to_parent[child] = parent\n\n    unique_slice_to_parent = {child: slice_to_parent[child] for child in child_labels.unique()}\n\n    return cls(parent_k=parent_k, slice_to_parent_mapping=unique_slice_to_parent)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate combinations of k parent groups, yielding flattened child slices.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples containing all child slices from k selected parent groups.</p> Source code in <code>energy_repset/combi_gens/hierarchical_exhaustive.py</code> <pre><code>def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n    \"\"\"Generate combinations of k parent groups, yielding flattened child slices.\n\n    Args:\n        unique_slices: Sequence of child slice labels.\n\n    Yields:\n        Tuples containing all child slices from k selected parent groups.\n    \"\"\"\n    parent_to_children: Dict[Hashable, list] = {}\n    for child in unique_slices:\n        parent = self.slice_to_parent[child]\n        parent_to_children.setdefault(parent, []).append(child)\n\n    parent_ids = sorted(parent_to_children.keys())\n    for parent_combi in itertools.combinations(parent_ids, self.parent_k):\n        child_slices = []\n        for parent_id in sorted(parent_combi):\n            child_slices.extend(parent_to_children[parent_id])\n        yield tuple(child_slices)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total number of parent-level combinations.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>C(n_parents, parent_k) where n_parents is the number of unique parent groups.</p> Source code in <code>energy_repset/combi_gens/hierarchical_exhaustive.py</code> <pre><code>def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n    \"\"\"Count total number of parent-level combinations.\n\n    Args:\n        unique_slices: Sequence of child slice labels.\n\n    Returns:\n        C(n_parents, parent_k) where n_parents is the number of unique parent groups.\n    \"\"\"\n    unique_parents = set(self.slice_to_parent[s] for s in unique_slices)\n    n_parents = len(unique_parents)\n    return math.comb(n_parents, self.parent_k)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.ExhaustiveHierarchicalCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination represents exactly parent_k complete parent groups.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of child slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of all valid child slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination contains all children from exactly parent_k parent groups.</p> Source code in <code>energy_repset/combi_gens/hierarchical_exhaustive.py</code> <pre><code>def combination_is_valid(\n    self,\n    combination: SliceCombination,\n    unique_slices: Sequence[Hashable]\n) -&gt; bool:\n    \"\"\"Check if combination represents exactly parent_k complete parent groups.\n\n    Args:\n        combination: Tuple of child slice labels to validate.\n        unique_slices: Sequence of all valid child slice labels.\n\n    Returns:\n        True if combination contains all children from exactly parent_k parent groups.\n    \"\"\"\n    if not all(c in unique_slices for c in combination):\n        return False\n\n    parent_to_children: Dict[Hashable, set] = {}\n    for child in unique_slices:\n        parent = self.slice_to_parent[child]\n        parent_to_children.setdefault(parent, set()).add(child)\n\n    parents_in_combi = set()\n    for child in combination:\n        if child not in self.slice_to_parent:\n            return False\n        parents_in_combi.add(self.slice_to_parent[child])\n\n    if len(parents_in_combi) != self.parent_k:\n        return False\n\n    expected_children = set()\n    for parent in parents_in_combi:\n        expected_children.update(parent_to_children[parent])\n\n    return set(combination) == expected_children\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen","title":"GroupQuotaHierarchicalCombiGen","text":"<p>               Bases: <code>CombinationGenerator</code></p> <p>Generate combinations respecting quotas per parent-level group.</p> <p>This generator combines hierarchical selection (child slices selected in complete parent groups) with group quotas (e.g., exactly 1 month per season). It enables high-resolution features (e.g. per-day) while enforcing structural constraints at the parent level (e.g. months).</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select. Must equal sum of group quotas.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from each child slice to its parent group.</p> required <code>parent_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from parent ID to its group label (e.g., season).</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count of parents per group.</p> required <p>Attributes:</p> Name Type Description <code>k</code> <p>Number of parent groups per combination (same as parent_k for protocol compliance).</p> <code>slice_to_parent</code> <p>Child to parent mapping.</p> <code>parent_to_group</code> <p>Parent to group label mapping.</p> <code>group_quota</code> <p>Required count per group.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal parent_k.</p> Note <p>Use factory methods <code>from_slicers()</code> for automatic parent mapping and <code>from_slicers_with_seasons()</code> for automatic seasonal grouping.</p> <p>Examples:</p> <p>Manual construction for seasonal month selection:</p> <pre><code>&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; slice_to_parent = {\n...     pd.Period('2024-01-01', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-01-02', 'D'): pd.Period('2024-01', 'M'),\n...     pd.Period('2024-07-01', 'D'): pd.Period('2024-07', 'M'),\n...     pd.Period('2024-07-02', 'D'): pd.Period('2024-07', 'M'),\n... }\n&gt;&gt;&gt; parent_to_group = {\n...     pd.Period('2024-01', 'M'): 'winter',\n...     pd.Period('2024-07', 'M'): 'summer',\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen(\n...     parent_k=2,\n...     slice_to_parent_mapping=slice_to_parent,\n...     parent_to_group_mapping=parent_to_group,\n...     group_quota={'winter': 1, 'summer': 1}\n... )\n&gt;&gt;&gt; gen.count(list(slice_to_parent.keys()))  # 1 * 1 = 1\n    1\n</code></pre> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>class GroupQuotaHierarchicalCombiGen(CombinationGenerator):\n    \"\"\"Generate combinations respecting quotas per parent-level group.\n\n    This generator combines hierarchical selection (child slices selected in complete\n    parent groups) with group quotas (e.g., exactly 1 month per season). It enables\n    high-resolution features (e.g. per-day) while enforcing structural constraints\n    at the parent level (e.g. months).\n\n    Args:\n        parent_k: Total number of parent groups to select. Must equal sum of group quotas.\n        slice_to_parent_mapping: Mapping from each child slice to its parent group.\n        parent_to_group_mapping: Mapping from parent ID to its group label (e.g., season).\n        group_quota: Required count of parents per group.\n\n    Attributes:\n        k: Number of parent groups per combination (same as parent_k for protocol compliance).\n        slice_to_parent: Child to parent mapping.\n        parent_to_group: Parent to group label mapping.\n        group_quota: Required count per group.\n\n    Raises:\n        ValueError: If sum of group quotas does not equal parent_k.\n\n    Note:\n        Use factory methods `from_slicers()` for automatic parent mapping and\n        `from_slicers_with_seasons()` for automatic seasonal grouping.\n\n    Examples:\n        Manual construction for seasonal month selection:\n\n        &gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; slice_to_parent = {\n        ...     pd.Period('2024-01-01', 'D'): pd.Period('2024-01', 'M'),\n        ...     pd.Period('2024-01-02', 'D'): pd.Period('2024-01', 'M'),\n        ...     pd.Period('2024-07-01', 'D'): pd.Period('2024-07', 'M'),\n        ...     pd.Period('2024-07-02', 'D'): pd.Period('2024-07', 'M'),\n        ... }\n        &gt;&gt;&gt; parent_to_group = {\n        ...     pd.Period('2024-01', 'M'): 'winter',\n        ...     pd.Period('2024-07', 'M'): 'summer',\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen(\n        ...     parent_k=2,\n        ...     slice_to_parent_mapping=slice_to_parent,\n        ...     parent_to_group_mapping=parent_to_group,\n        ...     group_quota={'winter': 1, 'summer': 1}\n        ... )\n        &gt;&gt;&gt; gen.count(list(slice_to_parent.keys()))  # 1 * 1 = 1\n            1\n    \"\"\"\n\n    def __init__(\n        self,\n        parent_k: int,\n        slice_to_parent_mapping: Dict[Hashable, Hashable],\n        parent_to_group_mapping: Dict[Hashable, Hashable],\n        group_quota: Dict[Hashable, int]\n    ) -&gt; None:\n        \"\"\"Initialize hierarchical quota generator.\n\n        Args:\n            parent_k: Total number of parent groups to select.\n            slice_to_parent_mapping: Dict mapping each child slice to its parent.\n            parent_to_group_mapping: Mapping from parent ID to group label.\n            group_quota: Required count per group.\n\n        Raises:\n            ValueError: If sum of group quotas does not equal parent_k, or if any\n                parent in slice_to_parent_mapping is missing from parent_to_group_mapping.\n        \"\"\"\n        self.k = parent_k  # For CombinationGenerator protocol compliance\n        self.slice_to_parent = slice_to_parent_mapping\n        self.parent_to_group = parent_to_group_mapping\n        self.group_quota = group_quota\n\n        self._validate_configuration(parent_k, slice_to_parent_mapping, parent_to_group_mapping, group_quota)\n\n    @classmethod\n    def from_slicers(\n        cls,\n        parent_k: int,\n        dt_index: pd.DatetimeIndex,\n        child_slicer: TimeSlicer,\n        parent_slicer: TimeSlicer,\n        parent_to_group_mapping: Dict[Hashable, Hashable],\n        group_quota: Dict[Hashable, int]\n    ) -&gt; GroupQuotaHierarchicalCombiGen:\n        \"\"\"Factory method to create generator from slicers with custom group mapping.\n\n        Args:\n            parent_k: Total number of parent groups to select.\n            dt_index: DatetimeIndex of the time series data.\n            child_slicer: TimeSlicer defining child slice granularity (e.g., daily).\n            parent_slicer: TimeSlicer defining parent slice granularity (e.g., monthly).\n            parent_to_group_mapping: Dict mapping parent IDs to group labels.\n            group_quota: Required count per group.\n\n        Returns:\n            GroupQuotaHierarchicalCombinationGenerator with auto-constructed child-to-parent mapping.\n\n        Raises:\n            ValueError: If quotas invalid.\n\n        Examples:\n            Custom grouping of months into seasons:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n            &gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n            &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n            &gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; parent_to_group = {\n            ...     pd.Period('2024-01', 'M'): 'winter',\n            ...     pd.Period('2024-02', 'M'): 'winter',\n            ...     # ... define for all 12 months\n            ... }\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers(\n            ...     parent_k=4,\n            ...     dt_index=dates,\n            ...     child_slicer=child_slicer,\n            ...     parent_slicer=parent_slicer,\n            ...     parent_to_group_mapping=parent_to_group,\n            ...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n            ... )\n        \"\"\"\n        child_labels = child_slicer.labels_for_index(dt_index)\n        parent_labels = parent_slicer.labels_for_index(dt_index)\n\n        slice_to_parent = {}\n        for child, parent in zip(child_labels, parent_labels):\n            slice_to_parent[child] = parent\n\n        unique_slice_to_parent = {child: slice_to_parent[child] for child in child_labels.unique()}\n\n        return cls(\n            parent_k=parent_k,\n            slice_to_parent_mapping=unique_slice_to_parent,\n            parent_to_group_mapping=parent_to_group_mapping,\n            group_quota=group_quota\n        )\n\n    @classmethod\n    def from_slicers_with_seasons(\n        cls,\n        parent_k: int,\n        dt_index: pd.DatetimeIndex,\n        child_slicer: TimeSlicer,\n        group_quota: Dict[Literal['winter', 'spring', 'summer', 'fall'], int]\n    ) -&gt; GroupQuotaHierarchicalCombiGen:\n        \"\"\"Factory method with automatic seasonal grouping of months.\n\n        Args:\n            parent_k: Total number of parent groups to select (must equal sum of quotas).\n            dt_index: DatetimeIndex of the time series data.\n            child_slicer: TimeSlicer defining child slice granularity (e.g., daily).\n            group_quota: Required count per season. Keys must be subset of\n                {'winter', 'spring', 'summer', 'fall'}.\n\n        Returns:\n            GroupQuotaHierarchicalCombinationGenerator with seasonal parent grouping.\n\n        Raises:\n            ValueError: If quotas invalid.\n\n        Note:\n            This factory method uses monthly parents regardless of child slicer.\n            Seasons are assigned as: winter (Dec/Jan/Feb), spring (Mar/Apr/May),\n            summer (Jun/Jul/Aug), fall (Sep/Oct/Nov).\n\n        Examples:\n            Select 4 months (1 per season) from daily data:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n            &gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n            &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n            ...     parent_k=4,\n            ...     dt_index=dates,\n            ...     child_slicer=child_slicer,\n            ...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n            ... )\n            &gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # 3 * 3 * 3 * 3 = 81\n                81\n        \"\"\"\n        child_labels = child_slicer.labels_for_index(dt_index)\n        parent_slicer = TimeSlicer(unit='month')\n        parent_labels = parent_slicer.labels_for_index(dt_index)\n\n        slice_to_parent = {}\n        all_parents = set()\n        for child, parent in zip(child_labels, parent_labels):\n            slice_to_parent[child] = parent\n            all_parents.add(parent)\n\n        unique_slice_to_parent = {child: slice_to_parent[child] for child in child_labels.unique()}\n        parent_to_group = cls._assign_seasons(list(all_parents))\n\n        return cls(\n            parent_k=parent_k,\n            slice_to_parent_mapping=unique_slice_to_parent,\n            parent_to_group_mapping=parent_to_group,\n            group_quota=group_quota\n        )\n\n    @staticmethod\n    def _validate_configuration(\n        parent_k: int,\n        slice_to_parent_mapping: Dict[Hashable, Hashable],\n        parent_to_group_mapping: Dict[Hashable, Hashable],\n        group_quota: Dict[Hashable, int]\n    ) -&gt; None:\n        \"\"\"Validate the configuration of mappings and quotas.\n\n        Args:\n            parent_k: Total number of parent groups to select.\n            slice_to_parent_mapping: Dict mapping each child slice to its parent.\n            parent_to_group_mapping: Mapping from parent ID to group label.\n            group_quota: Required count per group.\n\n        Raises:\n            ValueError: If sum of group quotas does not equal parent_k, or if any\n                parent in slice_to_parent_mapping is missing from parent_to_group_mapping.\n        \"\"\"\n        if sum(group_quota.values()) != parent_k:\n            raise ValueError(\n                f\"Sum of group quotas ({sum(group_quota.values())}) must equal parent_k ({parent_k})\"\n            )\n\n        unique_parents = set(slice_to_parent_mapping.values())\n        missing_parents = unique_parents - set(parent_to_group_mapping.keys())\n        if missing_parents:\n            raise ValueError(\n                f\"All parents in slice_to_parent_mapping must have a group mapping. \"\n                f\"Missing group mappings for parents: {sorted(missing_parents)}\"\n            )\n\n    @staticmethod\n    def _assign_seasons(months: list[pd.Period]) -&gt; Dict[pd.Period, str]:\n        \"\"\"Assign meteorological seasons to month Period objects.\n\n        Args:\n            months: List of monthly Period objects.\n\n        Returns:\n            Dict mapping each month to its season: winter (Dec/Jan/Feb),\n            spring (Mar/Apr/May), summer (Jun/Jul/Aug), fall (Sep/Oct/Nov).\n        \"\"\"\n        season_map = {}\n        for month in months:\n            m = month.month\n            if m in [12, 1, 2]:\n                season_map[month] = 'winter'\n            elif m in [3, 4, 5]:\n                season_map[month] = 'spring'\n            elif m in [6, 7, 8]:\n                season_map[month] = 'summer'\n            else:\n                season_map[month] = 'fall'\n        return season_map\n\n    def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n        \"\"\"Generate combinations respecting group quotas, yielding flattened child slices.\n\n        Args:\n            unique_slices: Sequence of child slice labels.\n\n        Yields:\n            Tuples containing all child slices from parent_k parent groups satisfying quotas.\n        \"\"\"\n        parent_to_children: Dict[Hashable, list] = {}\n        for child in unique_slices:\n            parent = self.slice_to_parent[child]\n            parent_to_children.setdefault(parent, []).append(child)\n\n        unique_parents = set(self.slice_to_parent[s] for s in unique_slices)\n        groups_of_parents: Dict[Hashable, list] = {}\n        for parent in unique_parents:\n            group = self.parent_to_group[parent]\n            groups_of_parents.setdefault(group, []).append(parent)\n\n        per_group_combis = []\n        for group, quota in self.group_quota.items():\n            parents_in_group = groups_of_parents.get(group, [])\n            per_group_combis.append(list(itertools.combinations(parents_in_group, quota)))\n\n        for parent_combi_tuple in itertools.product(*per_group_combis):\n            all_selected_parents = list(itertools.chain.from_iterable(parent_combi_tuple))\n            child_slices = []\n            for parent in sorted(all_selected_parents):\n                child_slices.extend(parent_to_children[parent])\n            yield tuple(child_slices)\n\n    def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n        \"\"\"Count total combinations respecting group quotas.\n\n        Args:\n            unique_slices: Sequence of child slice labels.\n\n        Returns:\n            Product of C(n_parents_in_group, quota) across all groups.\n        \"\"\"\n        unique_parents = set(self.slice_to_parent[s] for s in unique_slices)\n        groups_of_parents: Dict[Hashable, list] = {}\n        for parent in unique_parents:\n            group = self.parent_to_group[parent]\n            groups_of_parents.setdefault(group, []).append(parent)\n\n        total = 1\n        for group, quota in self.group_quota.items():\n            n_parents = len(groups_of_parents.get(group, []))\n            total *= math.comb(n_parents, quota)\n        return total\n\n    def combination_is_valid(\n        self,\n        combination: SliceCombination,\n        unique_slices: Sequence[Hashable]\n    ) -&gt; bool:\n        \"\"\"Check if combination satisfies group quotas and completeness.\n\n        Args:\n            combination: Tuple of child slice labels to validate.\n            unique_slices: Sequence of all valid child slice labels.\n\n        Returns:\n            True if combination contains complete parent groups satisfying quotas.\n        \"\"\"\n        if not all(c in unique_slices for c in combination):\n            return False\n\n        parent_to_children: Dict[Hashable, set] = {}\n        for child in unique_slices:\n            parent = self.slice_to_parent[child]\n            parent_to_children.setdefault(parent, set()).add(child)\n\n        parents_in_combi = set()\n        for child in combination:\n            if child not in self.slice_to_parent:\n                return False\n            parents_in_combi.add(self.slice_to_parent[child])\n\n        if len(parents_in_combi) != self.k:\n            return False\n\n        expected_children = set()\n        for parent in parents_in_combi:\n            expected_children.update(parent_to_children[parent])\n        if set(combination) != expected_children:\n            return False\n\n        group_count = {group: 0 for group in self.group_quota.keys()}\n        for parent in parents_in_combi:\n            group = self.parent_to_group[parent]\n            group_count[group] += 1\n\n        return all(group_count[g] == self.group_quota[g] for g in self.group_quota.keys())\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.__init__","title":"__init__","text":"<pre><code>__init__(parent_k: int, slice_to_parent_mapping: dict[Hashable, Hashable], parent_to_group_mapping: dict[Hashable, Hashable], group_quota: dict[Hashable, int]) -&gt; None\n</code></pre> <p>Initialize hierarchical quota generator.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select.</p> required <code>slice_to_parent_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Dict mapping each child slice to its parent.</p> required <code>parent_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Mapping from parent ID to group label.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count per group.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If sum of group quotas does not equal parent_k, or if any parent in slice_to_parent_mapping is missing from parent_to_group_mapping.</p> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>def __init__(\n    self,\n    parent_k: int,\n    slice_to_parent_mapping: Dict[Hashable, Hashable],\n    parent_to_group_mapping: Dict[Hashable, Hashable],\n    group_quota: Dict[Hashable, int]\n) -&gt; None:\n    \"\"\"Initialize hierarchical quota generator.\n\n    Args:\n        parent_k: Total number of parent groups to select.\n        slice_to_parent_mapping: Dict mapping each child slice to its parent.\n        parent_to_group_mapping: Mapping from parent ID to group label.\n        group_quota: Required count per group.\n\n    Raises:\n        ValueError: If sum of group quotas does not equal parent_k, or if any\n            parent in slice_to_parent_mapping is missing from parent_to_group_mapping.\n    \"\"\"\n    self.k = parent_k  # For CombinationGenerator protocol compliance\n    self.slice_to_parent = slice_to_parent_mapping\n    self.parent_to_group = parent_to_group_mapping\n    self.group_quota = group_quota\n\n    self._validate_configuration(parent_k, slice_to_parent_mapping, parent_to_group_mapping, group_quota)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.from_slicers","title":"from_slicers  <code>classmethod</code>","text":"<pre><code>from_slicers(parent_k: int, dt_index: DatetimeIndex, child_slicer: TimeSlicer, parent_slicer: TimeSlicer, parent_to_group_mapping: dict[Hashable, Hashable], group_quota: dict[Hashable, int]) -&gt; GroupQuotaHierarchicalCombiGen\n</code></pre> <p>Factory method to create generator from slicers with custom group mapping.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select.</p> required <code>dt_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex of the time series data.</p> required <code>child_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining child slice granularity (e.g., daily).</p> required <code>parent_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining parent slice granularity (e.g., monthly).</p> required <code>parent_to_group_mapping</code> <code>dict[Hashable, Hashable]</code> <p>Dict mapping parent IDs to group labels.</p> required <code>group_quota</code> <code>dict[Hashable, int]</code> <p>Required count per group.</p> required <p>Returns:</p> Type Description <code>GroupQuotaHierarchicalCombiGen</code> <p>GroupQuotaHierarchicalCombinationGenerator with auto-constructed child-to-parent mapping.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If quotas invalid.</p> <p>Examples:</p> <p>Custom grouping of months into seasons:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt;\n&gt;&gt;&gt; parent_to_group = {\n...     pd.Period('2024-01', 'M'): 'winter',\n...     pd.Period('2024-02', 'M'): 'winter',\n...     # ... define for all 12 months\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers(\n...     parent_k=4,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     parent_slicer=parent_slicer,\n...     parent_to_group_mapping=parent_to_group,\n...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n... )\n</code></pre> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>@classmethod\ndef from_slicers(\n    cls,\n    parent_k: int,\n    dt_index: pd.DatetimeIndex,\n    child_slicer: TimeSlicer,\n    parent_slicer: TimeSlicer,\n    parent_to_group_mapping: Dict[Hashable, Hashable],\n    group_quota: Dict[Hashable, int]\n) -&gt; GroupQuotaHierarchicalCombiGen:\n    \"\"\"Factory method to create generator from slicers with custom group mapping.\n\n    Args:\n        parent_k: Total number of parent groups to select.\n        dt_index: DatetimeIndex of the time series data.\n        child_slicer: TimeSlicer defining child slice granularity (e.g., daily).\n        parent_slicer: TimeSlicer defining parent slice granularity (e.g., monthly).\n        parent_to_group_mapping: Dict mapping parent IDs to group labels.\n        group_quota: Required count per group.\n\n    Returns:\n        GroupQuotaHierarchicalCombinationGenerator with auto-constructed child-to-parent mapping.\n\n    Raises:\n        ValueError: If quotas invalid.\n\n    Examples:\n        Custom grouping of months into seasons:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n        &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n        &gt;&gt;&gt; parent_slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; parent_to_group = {\n        ...     pd.Period('2024-01', 'M'): 'winter',\n        ...     pd.Period('2024-02', 'M'): 'winter',\n        ...     # ... define for all 12 months\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers(\n        ...     parent_k=4,\n        ...     dt_index=dates,\n        ...     child_slicer=child_slicer,\n        ...     parent_slicer=parent_slicer,\n        ...     parent_to_group_mapping=parent_to_group,\n        ...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n        ... )\n    \"\"\"\n    child_labels = child_slicer.labels_for_index(dt_index)\n    parent_labels = parent_slicer.labels_for_index(dt_index)\n\n    slice_to_parent = {}\n    for child, parent in zip(child_labels, parent_labels):\n        slice_to_parent[child] = parent\n\n    unique_slice_to_parent = {child: slice_to_parent[child] for child in child_labels.unique()}\n\n    return cls(\n        parent_k=parent_k,\n        slice_to_parent_mapping=unique_slice_to_parent,\n        parent_to_group_mapping=parent_to_group_mapping,\n        group_quota=group_quota\n    )\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons","title":"from_slicers_with_seasons  <code>classmethod</code>","text":"<pre><code>from_slicers_with_seasons(parent_k: int, dt_index: DatetimeIndex, child_slicer: TimeSlicer, group_quota: dict[Literal['winter', 'spring', 'summer', 'fall'], int]) -&gt; GroupQuotaHierarchicalCombiGen\n</code></pre> <p>Factory method with automatic seasonal grouping of months.</p> <p>Parameters:</p> Name Type Description Default <code>parent_k</code> <code>int</code> <p>Total number of parent groups to select (must equal sum of quotas).</p> required <code>dt_index</code> <code>DatetimeIndex</code> <p>DatetimeIndex of the time series data.</p> required <code>child_slicer</code> <code>TimeSlicer</code> <p>TimeSlicer defining child slice granularity (e.g., daily).</p> required <code>group_quota</code> <code>dict[Literal['winter', 'spring', 'summer', 'fall'], int]</code> <p>Required count per season. Keys must be subset of {'winter', 'spring', 'summer', 'fall'}.</p> required <p>Returns:</p> Type Description <code>GroupQuotaHierarchicalCombiGen</code> <p>GroupQuotaHierarchicalCombinationGenerator with seasonal parent grouping.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If quotas invalid.</p> Note <p>This factory method uses monthly parents regardless of child slicer. Seasons are assigned as: winter (Dec/Jan/Feb), spring (Mar/Apr/May), summer (Jun/Jul/Aug), fall (Sep/Oct/Nov).</p> <p>Examples:</p> <p>Select 4 months (1 per season) from daily data:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n&gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt;\n&gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n...     parent_k=4,\n...     dt_index=dates,\n...     child_slicer=child_slicer,\n...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n... )\n&gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # 3 * 3 * 3 * 3 = 81\n    81\n</code></pre> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>@classmethod\ndef from_slicers_with_seasons(\n    cls,\n    parent_k: int,\n    dt_index: pd.DatetimeIndex,\n    child_slicer: TimeSlicer,\n    group_quota: Dict[Literal['winter', 'spring', 'summer', 'fall'], int]\n) -&gt; GroupQuotaHierarchicalCombiGen:\n    \"\"\"Factory method with automatic seasonal grouping of months.\n\n    Args:\n        parent_k: Total number of parent groups to select (must equal sum of quotas).\n        dt_index: DatetimeIndex of the time series data.\n        child_slicer: TimeSlicer defining child slice granularity (e.g., daily).\n        group_quota: Required count per season. Keys must be subset of\n            {'winter', 'spring', 'summer', 'fall'}.\n\n    Returns:\n        GroupQuotaHierarchicalCombinationGenerator with seasonal parent grouping.\n\n    Raises:\n        ValueError: If quotas invalid.\n\n    Note:\n        This factory method uses monthly parents regardless of child slicer.\n        Seasons are assigned as: winter (Dec/Jan/Feb), spring (Mar/Apr/May),\n        summer (Jun/Jul/Aug), fall (Sep/Oct/Nov).\n\n    Examples:\n        Select 4 months (1 per season) from daily data:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt; from energy_repset.combi_gens import GroupQuotaHierarchicalCombiGen\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=366, freq='D')\n        &gt;&gt;&gt; child_slicer = TimeSlicer(unit='day')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; gen = GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n        ...     parent_k=4,\n        ...     dt_index=dates,\n        ...     child_slicer=child_slicer,\n        ...     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n        ... )\n        &gt;&gt;&gt; gen.count(child_slicer.unique_slices(dates))  # 3 * 3 * 3 * 3 = 81\n            81\n    \"\"\"\n    child_labels = child_slicer.labels_for_index(dt_index)\n    parent_slicer = TimeSlicer(unit='month')\n    parent_labels = parent_slicer.labels_for_index(dt_index)\n\n    slice_to_parent = {}\n    all_parents = set()\n    for child, parent in zip(child_labels, parent_labels):\n        slice_to_parent[child] = parent\n        all_parents.add(parent)\n\n    unique_slice_to_parent = {child: slice_to_parent[child] for child in child_labels.unique()}\n    parent_to_group = cls._assign_seasons(list(all_parents))\n\n    return cls(\n        parent_k=parent_k,\n        slice_to_parent_mapping=unique_slice_to_parent,\n        parent_to_group_mapping=parent_to_group,\n        group_quota=group_quota\n    )\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.generate","title":"generate","text":"<pre><code>generate(unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]\n</code></pre> <p>Generate combinations respecting group quotas, yielding flattened child slices.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Yields:</p> Type Description <code>SliceCombination</code> <p>Tuples containing all child slices from parent_k parent groups satisfying quotas.</p> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>def generate(self, unique_slices: Sequence[Hashable]) -&gt; Iterator[SliceCombination]:\n    \"\"\"Generate combinations respecting group quotas, yielding flattened child slices.\n\n    Args:\n        unique_slices: Sequence of child slice labels.\n\n    Yields:\n        Tuples containing all child slices from parent_k parent groups satisfying quotas.\n    \"\"\"\n    parent_to_children: Dict[Hashable, list] = {}\n    for child in unique_slices:\n        parent = self.slice_to_parent[child]\n        parent_to_children.setdefault(parent, []).append(child)\n\n    unique_parents = set(self.slice_to_parent[s] for s in unique_slices)\n    groups_of_parents: Dict[Hashable, list] = {}\n    for parent in unique_parents:\n        group = self.parent_to_group[parent]\n        groups_of_parents.setdefault(group, []).append(parent)\n\n    per_group_combis = []\n    for group, quota in self.group_quota.items():\n        parents_in_group = groups_of_parents.get(group, [])\n        per_group_combis.append(list(itertools.combinations(parents_in_group, quota)))\n\n    for parent_combi_tuple in itertools.product(*per_group_combis):\n        all_selected_parents = list(itertools.chain.from_iterable(parent_combi_tuple))\n        child_slices = []\n        for parent in sorted(all_selected_parents):\n            child_slices.extend(parent_to_children[parent])\n        yield tuple(child_slices)\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.count","title":"count","text":"<pre><code>count(unique_slices: Sequence[Hashable]) -&gt; int\n</code></pre> <p>Count total combinations respecting group quotas.</p> <p>Parameters:</p> Name Type Description Default <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of child slice labels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Product of C(n_parents_in_group, quota) across all groups.</p> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>def count(self, unique_slices: Sequence[Hashable]) -&gt; int:\n    \"\"\"Count total combinations respecting group quotas.\n\n    Args:\n        unique_slices: Sequence of child slice labels.\n\n    Returns:\n        Product of C(n_parents_in_group, quota) across all groups.\n    \"\"\"\n    unique_parents = set(self.slice_to_parent[s] for s in unique_slices)\n    groups_of_parents: Dict[Hashable, list] = {}\n    for parent in unique_parents:\n        group = self.parent_to_group[parent]\n        groups_of_parents.setdefault(group, []).append(parent)\n\n    total = 1\n    for group, quota in self.group_quota.items():\n        n_parents = len(groups_of_parents.get(group, []))\n        total *= math.comb(n_parents, quota)\n    return total\n</code></pre>"},{"location":"api/combi_gens/#energy_repset.combi_gens.GroupQuotaHierarchicalCombiGen.combination_is_valid","title":"combination_is_valid","text":"<pre><code>combination_is_valid(combination: SliceCombination, unique_slices: Sequence[Hashable]) -&gt; bool\n</code></pre> <p>Check if combination satisfies group quotas and completeness.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of child slice labels to validate.</p> required <code>unique_slices</code> <code>Sequence[Hashable]</code> <p>Sequence of all valid child slice labels.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if combination contains complete parent groups satisfying quotas.</p> Source code in <code>energy_repset/combi_gens/hierarchical_group_quota.py</code> <pre><code>def combination_is_valid(\n    self,\n    combination: SliceCombination,\n    unique_slices: Sequence[Hashable]\n) -&gt; bool:\n    \"\"\"Check if combination satisfies group quotas and completeness.\n\n    Args:\n        combination: Tuple of child slice labels to validate.\n        unique_slices: Sequence of all valid child slice labels.\n\n    Returns:\n        True if combination contains complete parent groups satisfying quotas.\n    \"\"\"\n    if not all(c in unique_slices for c in combination):\n        return False\n\n    parent_to_children: Dict[Hashable, set] = {}\n    for child in unique_slices:\n        parent = self.slice_to_parent[child]\n        parent_to_children.setdefault(parent, set()).add(child)\n\n    parents_in_combi = set()\n    for child in combination:\n        if child not in self.slice_to_parent:\n            return False\n        parents_in_combi.add(self.slice_to_parent[child])\n\n    if len(parents_in_combi) != self.k:\n        return False\n\n    expected_children = set()\n    for parent in parents_in_combi:\n        expected_children.update(parent_to_children[parent])\n    if set(combination) != expected_children:\n        return False\n\n    group_count = {group: 0 for group in self.group_quota.keys()}\n    for parent in parents_in_combi:\n        group = self.parent_to_group[parent]\n        group_count[group] += 1\n\n    return all(group_count[g] == self.group_quota[g] for g in self.group_quota.keys())\n</code></pre>"},{"location":"api/context/","title":"Context &amp; Slicing","text":""},{"location":"api/context/#energy_repset.context.ProblemContext","title":"ProblemContext","text":"<p>A data container passed through the entire workflow.</p> <p>This class holds all data and metadata needed for representative subset selection. It is the central object passed between workflow stages (feature engineering, search algorithms, representation models).</p> <p>Parameters:</p> Name Type Description Default <code>df_raw</code> <code>DataFrame</code> <p>Raw time-series data with datetime index and variable columns.</p> required <code>slicer</code> <code>'TimeSlicer'</code> <p>TimeSlicer defining how the time index is divided into candidate periods.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dict for storing arbitrary user data (e.g., default weights, experiment configuration, notes, etc.). Not used by the framework itself, but available for user convenience and custom component implementations.</p> <code>None</code> <p>Examples:</p> <p>Create a context with monthly slicing:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.context import ProblemContext\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sample data\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'demand': np.random.rand(8760),\n...     'solar': np.random.rand(8760)\n... }, index=dates)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create context with metadata\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; context = ProblemContext(\n...     df_raw=df,\n...     slicer=slicer,\n...     metadata={\n...         'experiment_name': 'test_run_1',\n...         'default_weights': {'demand': 1.5, 'solar': 1.0},\n...         'notes': 'Testing seasonal selection'\n...     }\n... )\n&gt;&gt;&gt; len(context.get_unique_slices())  # 12 months\n    12\n&gt;&gt;&gt; context.metadata['experiment_name']  # 'test_run_1'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create context without metadata\n&gt;&gt;&gt; context2 = ProblemContext(df_raw=df, slicer=slicer)\n&gt;&gt;&gt; context2.metadata  # {}\n</code></pre> Source code in <code>energy_repset/context.py</code> <pre><code>class ProblemContext:\n    \"\"\"A data container passed through the entire workflow.\n\n    This class holds all data and metadata needed for representative subset selection.\n    It is the central object passed between workflow stages (feature engineering,\n    search algorithms, representation models).\n\n    Args:\n        df_raw: Raw time-series data with datetime index and variable columns.\n        slicer: TimeSlicer defining how the time index is divided into candidate periods.\n        metadata: Optional dict for storing arbitrary user data (e.g., default weights,\n            experiment configuration, notes, etc.). Not used by the framework itself,\n            but available for user convenience and custom component implementations.\n\n    Examples:\n        Create a context with monthly slicing:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.context import ProblemContext\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create sample data\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'demand': np.random.rand(8760),\n        ...     'solar': np.random.rand(8760)\n        ... }, index=dates)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create context with metadata\n        &gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt; context = ProblemContext(\n        ...     df_raw=df,\n        ...     slicer=slicer,\n        ...     metadata={\n        ...         'experiment_name': 'test_run_1',\n        ...         'default_weights': {'demand': 1.5, 'solar': 1.0},\n        ...         'notes': 'Testing seasonal selection'\n        ...     }\n        ... )\n        &gt;&gt;&gt; len(context.get_unique_slices())  # 12 months\n            12\n        &gt;&gt;&gt; context.metadata['experiment_name']  # 'test_run_1'\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create context without metadata\n        &gt;&gt;&gt; context2 = ProblemContext(df_raw=df, slicer=slicer)\n        &gt;&gt;&gt; context2.metadata  # {}\n    \"\"\"\n\n    def __init__(\n        self,\n        df_raw: pd.DataFrame,\n        slicer: 'TimeSlicer',\n        metadata: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Initialize a ProblemContext.\n\n        Args:\n            df_raw: Raw time-series data with datetime index and variable columns.\n            slicer: TimeSlicer defining how the time index is divided into candidate periods.\n            metadata: Optional dict for storing arbitrary user data. Not used by the\n                framework itself.\n        \"\"\"\n        self.df_raw = df_raw\n        self.slicer = slicer\n        self.metadata = metadata if metadata is not None else {}\n        self._df_features: Optional[pd.DataFrame] = None\n\n    def copy(self) -&gt; 'ProblemContext':\n        \"\"\"Create a deep copy of this ProblemContext instance.\n\n        Returns:\n            A new, independent instance of the context with all data copied.\n        \"\"\"\n        return copy.deepcopy(self)\n\n    def get_sliced_data(self) -&gt; Dict[Hashable, pd.DataFrame]:\n        \"\"\"Generate sliced raw data on demand.\n\n        Returns:\n            Dictionary mapping slice labels to their corresponding DataFrame chunks.\n\n        Raises:\n            NotImplementedError: This method is not yet implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def df_features(self) -&gt; pd.DataFrame:\n        \"\"\"Get the computed feature matrix.\n\n        Returns:\n            DataFrame with slice labels as index and engineered features as columns.\n\n        Raises:\n            ValueError: If features have not been computed yet. Use a FeatureEngineer\n                to populate this field first.\n        \"\"\"\n        if self._df_features is None:\n            raise ValueError(\n                f'You tried to retrieve df_features before assigning it. Please set first using a FeatureEngineer.'\n            )\n        return self._df_features\n\n    @df_features.setter\n    def df_features(self, df_features: pd.DataFrame):\n        \"\"\"Set the feature matrix.\n\n        Args:\n            df_features: DataFrame with slice labels as index and features as columns.\n\n        Raises:\n            ValueError: If df_features index does not contain all expected slices\n                from the slicer.\n        \"\"\"\n        self._validate_all_slices_present_in_features_df(df_features)\n\n        self._df_features = df_features\n\n    def _validate_all_slices_present_in_features_df(self, df_features):\n        expected_slices = set(self.get_unique_slices())\n        actual_slices = set(df_features.index)\n        if not expected_slices.issubset(actual_slices):\n            missing_slices = expected_slices - actual_slices\n            raise ValueError(\n                f\"df_features is missing {len(missing_slices)} slice(s). \"\n                f\"Expected all slices from slicer but missing: {sorted(list(missing_slices)[:5])}\"\n                f\"{'...' if len(missing_slices) &gt; 5 else ''}\"\n            )\n\n    def get_unique_slices(self) -&gt; List[Hashable]:\n        \"\"\"Get list of all unique slice labels from the time index.\n\n        Returns:\n            List of slice labels (e.g., Period objects for monthly slicing).\n        \"\"\"\n        return self.slicer.unique_slices(self.df_raw.index)\n</code></pre>"},{"location":"api/context/#energy_repset.context.ProblemContext.df_features","title":"df_features  <code>property</code> <code>writable</code>","text":"<pre><code>df_features: DataFrame\n</code></pre> <p>Get the computed feature matrix.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with slice labels as index and engineered features as columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If features have not been computed yet. Use a FeatureEngineer to populate this field first.</p>"},{"location":"api/context/#energy_repset.context.ProblemContext.__init__","title":"__init__","text":"<pre><code>__init__(df_raw: DataFrame, slicer: 'TimeSlicer', metadata: dict[str, Any] | None = None)\n</code></pre> <p>Initialize a ProblemContext.</p> <p>Parameters:</p> Name Type Description Default <code>df_raw</code> <code>DataFrame</code> <p>Raw time-series data with datetime index and variable columns.</p> required <code>slicer</code> <code>'TimeSlicer'</code> <p>TimeSlicer defining how the time index is divided into candidate periods.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dict for storing arbitrary user data. Not used by the framework itself.</p> <code>None</code> Source code in <code>energy_repset/context.py</code> <pre><code>def __init__(\n    self,\n    df_raw: pd.DataFrame,\n    slicer: 'TimeSlicer',\n    metadata: Optional[Dict[str, Any]] = None\n):\n    \"\"\"Initialize a ProblemContext.\n\n    Args:\n        df_raw: Raw time-series data with datetime index and variable columns.\n        slicer: TimeSlicer defining how the time index is divided into candidate periods.\n        metadata: Optional dict for storing arbitrary user data. Not used by the\n            framework itself.\n    \"\"\"\n    self.df_raw = df_raw\n    self.slicer = slicer\n    self.metadata = metadata if metadata is not None else {}\n    self._df_features: Optional[pd.DataFrame] = None\n</code></pre>"},{"location":"api/context/#energy_repset.context.ProblemContext.copy","title":"copy","text":"<pre><code>copy() -&gt; 'ProblemContext'\n</code></pre> <p>Create a deep copy of this ProblemContext instance.</p> <p>Returns:</p> Type Description <code>'ProblemContext'</code> <p>A new, independent instance of the context with all data copied.</p> Source code in <code>energy_repset/context.py</code> <pre><code>def copy(self) -&gt; 'ProblemContext':\n    \"\"\"Create a deep copy of this ProblemContext instance.\n\n    Returns:\n        A new, independent instance of the context with all data copied.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/context/#energy_repset.context.ProblemContext.get_sliced_data","title":"get_sliced_data","text":"<pre><code>get_sliced_data() -&gt; dict[Hashable, DataFrame]\n</code></pre> <p>Generate sliced raw data on demand.</p> <p>Returns:</p> Type Description <code>dict[Hashable, DataFrame]</code> <p>Dictionary mapping slice labels to their corresponding DataFrame chunks.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not yet implemented.</p> Source code in <code>energy_repset/context.py</code> <pre><code>def get_sliced_data(self) -&gt; Dict[Hashable, pd.DataFrame]:\n    \"\"\"Generate sliced raw data on demand.\n\n    Returns:\n        Dictionary mapping slice labels to their corresponding DataFrame chunks.\n\n    Raises:\n        NotImplementedError: This method is not yet implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/context/#energy_repset.context.ProblemContext.get_unique_slices","title":"get_unique_slices","text":"<pre><code>get_unique_slices() -&gt; list[Hashable]\n</code></pre> <p>Get list of all unique slice labels from the time index.</p> <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>List of slice labels (e.g., Period objects for monthly slicing).</p> Source code in <code>energy_repset/context.py</code> <pre><code>def get_unique_slices(self) -&gt; List[Hashable]:\n    \"\"\"Get list of all unique slice labels from the time index.\n\n    Returns:\n        List of slice labels (e.g., Period objects for monthly slicing).\n    \"\"\"\n    return self.slicer.unique_slices(self.df_raw.index)\n</code></pre>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer","title":"TimeSlicer","text":"<p>Convert a DatetimeIndex into labeled time slices.</p> <p>This class defines how the time index is divided into candidate periods for representative subset selection. It converts timestamps into Period objects or floored timestamps based on the specified temporal granularity.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>SliceUnit</code> <p>Temporal granularity of the slices. One of \"year\", \"month\", \"week\", \"day\", or \"hour\".</p> required <p>Attributes:</p> Name Type Description <code>unit</code> <p>The temporal granularity used for slicing.</p> Note <p>The labels are hashable and suitable for set membership and grouping. Period objects are used for year, month, week, and day. Naive timestamps (floored to hour) are used for hourly slicing.</p> <p>Examples:</p> <p>Create a slicer for monthly periods:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; labels = slicer.labels_for_index(dates)\n&gt;&gt;&gt; unique_months = slicer.unique_slices(dates)\n&gt;&gt;&gt; len(unique_months)  # 12 months in a year\n    12\n&gt;&gt;&gt; unique_months[0]  # First month\n    Period('2024-01', 'M')\n</code></pre> <p>Weekly slicing:</p> <pre><code>&gt;&gt;&gt; slicer = TimeSlicer(unit='week')\n&gt;&gt;&gt; unique_weeks = slicer.unique_slices(dates)\n&gt;&gt;&gt; len(unique_weeks)  # ~52 weeks in a year\n    53\n</code></pre> Source code in <code>energy_repset/time_slicer.py</code> <pre><code>class TimeSlicer:\n    \"\"\"Convert a DatetimeIndex into labeled time slices.\n\n    This class defines how the time index is divided into candidate periods\n    for representative subset selection. It converts timestamps into Period\n    objects or floored timestamps based on the specified temporal granularity.\n\n    Args:\n        unit: Temporal granularity of the slices. One of \"year\", \"month\",\n            \"week\", \"day\", or \"hour\".\n\n    Attributes:\n        unit: The temporal granularity used for slicing.\n\n    Note:\n        The labels are hashable and suitable for set membership and grouping.\n        Period objects are used for year, month, week, and day. Naive\n        timestamps (floored to hour) are used for hourly slicing.\n\n    Examples:\n        Create a slicer for monthly periods:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n        &gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt; labels = slicer.labels_for_index(dates)\n        &gt;&gt;&gt; unique_months = slicer.unique_slices(dates)\n        &gt;&gt;&gt; len(unique_months)  # 12 months in a year\n            12\n        &gt;&gt;&gt; unique_months[0]  # First month\n            Period('2024-01', 'M')\n\n        Weekly slicing:\n\n        &gt;&gt;&gt; slicer = TimeSlicer(unit='week')\n        &gt;&gt;&gt; unique_weeks = slicer.unique_slices(dates)\n        &gt;&gt;&gt; len(unique_weeks)  # ~52 weeks in a year\n            53\n    \"\"\"\n\n    def __init__(self, unit: SliceUnit) -&gt; None:\n        \"\"\"Initialize TimeSlicer with specified temporal granularity.\n\n        Args:\n            unit: One of \"year\", \"month\", \"week\", \"day\", or \"hour\".\n        \"\"\"\n        self.unit = unit\n\n    def labels_for_index(self, index: pd.DatetimeIndex) -&gt; pd.Index:\n        \"\"\"Return a vector of slice labels aligned to the given index.\n\n        Args:\n            index: DatetimeIndex for the input data.\n\n        Returns:\n            Index of slice labels matching the input index length. Each timestamp\n            is mapped to its corresponding period or floored hour.\n\n        Raises:\n            ValueError: If unit is not one of the supported values.\n        \"\"\"\n        if self.unit == \"year\":\n            return index.to_period(\"Y\")\n        if self.unit == \"month\":\n            return index.to_period(\"M\")\n        if self.unit == \"week\":\n            return index.to_period(\"W\")\n        if self.unit == \"day\":\n            return index.to_period(\"D\")\n        if self.unit == \"hour\":\n            return pd.Index(index.floor(\"H\"))\n        raise ValueError(\"Unsupported unit\")\n\n    def unique_slices(self, index: pd.DatetimeIndex) -&gt; List[Hashable]:\n        \"\"\"Return the sorted list of unique slice labels present in the index.\n\n        Args:\n            index: DatetimeIndex for the input data.\n\n        Returns:\n            Sorted list of unique slice labels. The sort order follows the natural\n            ordering of Period objects or timestamps.\n        \"\"\"\n        labels = self.labels_for_index(index)\n        unique = pd.Index(labels).unique().tolist()\n        unique.sort()\n        return unique\n\n    def get_indices_for_slice_combi(\n        self,\n        index: pd.DatetimeIndex,\n        selection: Union[Hashable, SliceCombination],\n    ) -&gt; pd.Index:\n        \"\"\"Return the index positions for timestamps belonging to the given slice(s).\n\n        Args:\n            index: DatetimeIndex for the input data.\n            selection: Either a single slice label or a tuple of slice labels\n                (SliceCombination) to extract indices for.\n\n        Returns:\n            Index of timestamps that belong to the specified slice(s). If selection\n            is a tuple, returns the union of all timestamps from all slices.\n\n        Examples:\n            Get indices for a single month:\n\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n            &gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n            &gt;&gt;&gt; jan_slice = slicer.unique_slices(dates)[0]  # Period('2024-01', 'M')\n            &gt;&gt;&gt; jan_indices = slicer.get_indices_for_slice_combi(dates, jan_slice)\n            &gt;&gt;&gt; len(jan_indices)  # 744 hours in January 2024\n                744\n\n            Get indices for multiple months (selection):\n\n            &gt;&gt;&gt; selection = (Period('2024-01', 'M'), Period('2024-06', 'M'))\n            &gt;&gt;&gt; selected_indices = slicer.get_indices_for_slice_combi(dates, selection)\n            &gt;&gt;&gt; len(selected_indices)  # Jan (744) + Jun (720) = 1464\n                1464\n        \"\"\"\n        labels = self.labels_for_index(index)\n\n        # Convert single slice to tuple for uniform handling\n        if isinstance(selection, tuple):\n            slice_set = set(selection)\n        else:\n            slice_set = {selection}\n\n        # Create boolean mask for timestamps in any of the selected slices\n        mask = labels.isin(slice_set)\n\n        # Return the index positions where mask is True\n        return index[mask]\n</code></pre>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.__init__","title":"__init__","text":"<pre><code>__init__(unit: SliceUnit) -&gt; None\n</code></pre> <p>Initialize TimeSlicer with specified temporal granularity.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>SliceUnit</code> <p>One of \"year\", \"month\", \"week\", \"day\", or \"hour\".</p> required Source code in <code>energy_repset/time_slicer.py</code> <pre><code>def __init__(self, unit: SliceUnit) -&gt; None:\n    \"\"\"Initialize TimeSlicer with specified temporal granularity.\n\n    Args:\n        unit: One of \"year\", \"month\", \"week\", \"day\", or \"hour\".\n    \"\"\"\n    self.unit = unit\n</code></pre>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.labels_for_index","title":"labels_for_index","text":"<pre><code>labels_for_index(index: DatetimeIndex) -&gt; Index\n</code></pre> <p>Return a vector of slice labels aligned to the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>DatetimeIndex</code> <p>DatetimeIndex for the input data.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>Index of slice labels matching the input index length. Each timestamp</p> <code>Index</code> <p>is mapped to its corresponding period or floored hour.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unit is not one of the supported values.</p> Source code in <code>energy_repset/time_slicer.py</code> <pre><code>def labels_for_index(self, index: pd.DatetimeIndex) -&gt; pd.Index:\n    \"\"\"Return a vector of slice labels aligned to the given index.\n\n    Args:\n        index: DatetimeIndex for the input data.\n\n    Returns:\n        Index of slice labels matching the input index length. Each timestamp\n        is mapped to its corresponding period or floored hour.\n\n    Raises:\n        ValueError: If unit is not one of the supported values.\n    \"\"\"\n    if self.unit == \"year\":\n        return index.to_period(\"Y\")\n    if self.unit == \"month\":\n        return index.to_period(\"M\")\n    if self.unit == \"week\":\n        return index.to_period(\"W\")\n    if self.unit == \"day\":\n        return index.to_period(\"D\")\n    if self.unit == \"hour\":\n        return pd.Index(index.floor(\"H\"))\n    raise ValueError(\"Unsupported unit\")\n</code></pre>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.unique_slices","title":"unique_slices","text":"<pre><code>unique_slices(index: DatetimeIndex) -&gt; list[Hashable]\n</code></pre> <p>Return the sorted list of unique slice labels present in the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>DatetimeIndex</code> <p>DatetimeIndex for the input data.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>Sorted list of unique slice labels. The sort order follows the natural</p> <code>list[Hashable]</code> <p>ordering of Period objects or timestamps.</p> Source code in <code>energy_repset/time_slicer.py</code> <pre><code>def unique_slices(self, index: pd.DatetimeIndex) -&gt; List[Hashable]:\n    \"\"\"Return the sorted list of unique slice labels present in the index.\n\n    Args:\n        index: DatetimeIndex for the input data.\n\n    Returns:\n        Sorted list of unique slice labels. The sort order follows the natural\n        ordering of Period objects or timestamps.\n    \"\"\"\n    labels = self.labels_for_index(index)\n    unique = pd.Index(labels).unique().tolist()\n    unique.sort()\n    return unique\n</code></pre>"},{"location":"api/context/#energy_repset.time_slicer.TimeSlicer.get_indices_for_slice_combi","title":"get_indices_for_slice_combi","text":"<pre><code>get_indices_for_slice_combi(index: DatetimeIndex, selection: Hashable | SliceCombination) -&gt; Index\n</code></pre> <p>Return the index positions for timestamps belonging to the given slice(s).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>DatetimeIndex</code> <p>DatetimeIndex for the input data.</p> required <code>selection</code> <code>Hashable | SliceCombination</code> <p>Either a single slice label or a tuple of slice labels (SliceCombination) to extract indices for.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>Index of timestamps that belong to the specified slice(s). If selection</p> <code>Index</code> <p>is a tuple, returns the union of all timestamps from all slices.</p> <p>Examples:</p> <p>Get indices for a single month:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt;\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; jan_slice = slicer.unique_slices(dates)[0]  # Period('2024-01', 'M')\n&gt;&gt;&gt; jan_indices = slicer.get_indices_for_slice_combi(dates, jan_slice)\n&gt;&gt;&gt; len(jan_indices)  # 744 hours in January 2024\n    744\n</code></pre> <p>Get indices for multiple months (selection):</p> <pre><code>&gt;&gt;&gt; selection = (Period('2024-01', 'M'), Period('2024-06', 'M'))\n&gt;&gt;&gt; selected_indices = slicer.get_indices_for_slice_combi(dates, selection)\n&gt;&gt;&gt; len(selected_indices)  # Jan (744) + Jun (720) = 1464\n    1464\n</code></pre> Source code in <code>energy_repset/time_slicer.py</code> <pre><code>def get_indices_for_slice_combi(\n    self,\n    index: pd.DatetimeIndex,\n    selection: Union[Hashable, SliceCombination],\n) -&gt; pd.Index:\n    \"\"\"Return the index positions for timestamps belonging to the given slice(s).\n\n    Args:\n        index: DatetimeIndex for the input data.\n        selection: Either a single slice label or a tuple of slice labels\n            (SliceCombination) to extract indices for.\n\n    Returns:\n        Index of timestamps that belong to the specified slice(s). If selection\n        is a tuple, returns the union of all timestamps from all slices.\n\n    Examples:\n        Get indices for a single month:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n        &gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt; jan_slice = slicer.unique_slices(dates)[0]  # Period('2024-01', 'M')\n        &gt;&gt;&gt; jan_indices = slicer.get_indices_for_slice_combi(dates, jan_slice)\n        &gt;&gt;&gt; len(jan_indices)  # 744 hours in January 2024\n            744\n\n        Get indices for multiple months (selection):\n\n        &gt;&gt;&gt; selection = (Period('2024-01', 'M'), Period('2024-06', 'M'))\n        &gt;&gt;&gt; selected_indices = slicer.get_indices_for_slice_combi(dates, selection)\n        &gt;&gt;&gt; len(selected_indices)  # Jan (744) + Jun (720) = 1464\n            1464\n    \"\"\"\n    labels = self.labels_for_index(index)\n\n    # Convert single slice to tuple for uniform handling\n    if isinstance(selection, tuple):\n        slice_set = set(selection)\n    else:\n        slice_set = {selection}\n\n    # Create boolean mask for timestamps in any of the selected slices\n    mask = labels.isin(slice_set)\n\n    # Return the index positions where mask is True\n    return index[mask]\n</code></pre>"},{"location":"api/diagnostics/","title":"Diagnostics","text":""},{"location":"api/diagnostics/#feature-space","title":"Feature Space","text":""},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter2D","title":"FeatureSpaceScatter2D","text":"<p>2D scatter plot for visualizing feature space.</p> <p>Creates an interactive scatter plot of any two features from df_features. Can highlight a specific selection of slices. Works with any feature columns including PCA components ('pc_0', 'pc_1'), statistical features ('mean__wind'), or mixed features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize PCA space\n&gt;&gt;&gt; scatter = FeatureSpaceScatter2D()\n&gt;&gt;&gt; fig = scatter.plot(context.df_features, x='pc_0', y='pc_1')\n&gt;&gt;&gt; fig.update_layout(title='PCA Feature Space')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Visualize with selection highlighted\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='mean__demand',\n...     y='pc_0',\n...     selection=('2024-01', '2024-04', '2024-07')\n... )\n\n&gt;&gt;&gt; # Color by another feature\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     color='std__wind'\n... )\n</code></pre> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter.py</code> <pre><code>class FeatureSpaceScatter2D:\n    \"\"\"2D scatter plot for visualizing feature space.\n\n    Creates an interactive scatter plot of any two features from df_features.\n    Can highlight a specific selection of slices. Works with any feature columns\n    including PCA components ('pc_0', 'pc_1'), statistical features ('mean__wind'),\n    or mixed features.\n\n    Examples:\n\n        &gt;&gt;&gt; # Visualize PCA space\n        &gt;&gt;&gt; scatter = FeatureSpaceScatter2D()\n        &gt;&gt;&gt; fig = scatter.plot(context.df_features, x='pc_0', y='pc_1')\n        &gt;&gt;&gt; fig.update_layout(title='PCA Feature Space')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Visualize with selection highlighted\n        &gt;&gt;&gt; fig = scatter.plot(\n        ...     context.df_features,\n        ...     x='mean__demand',\n        ...     y='pc_0',\n        ...     selection=('2024-01', '2024-04', '2024-07')\n        ... )\n\n        &gt;&gt;&gt; # Color by another feature\n        &gt;&gt;&gt; fig = scatter.plot(\n        ...     context.df_features,\n        ...     x='pc_0',\n        ...     y='pc_1',\n        ...     color='std__wind'\n        ... )\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the scatter plot diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_features: pd.DataFrame,\n        x: str,\n        y: str,\n        selection: SliceCombination = None,\n        color: str = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create a 2D scatter plot of feature space.\n\n        Args:\n            df_features: Feature matrix with slices as rows, features as columns.\n            x: Column name for x-axis.\n            y: Column name for y-axis.\n            selection: Optional tuple of slice identifiers to highlight.\n            color: Optional column name to use for color mapping.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            KeyError: If x, y, or color columns are not in df_features.\n        \"\"\"\n        # Validate columns\n        if x not in df_features.columns:\n            raise KeyError(f\"Column '{x}' not found in df_features\")\n        if y not in df_features.columns:\n            raise KeyError(f\"Column '{y}' not found in df_features\")\n        if color is not None and color not in df_features.columns:\n            raise KeyError(f\"Column '{color}' not found in df_features\")\n\n        # Prepare data\n        plot_df = df_features.copy()\n        plot_df['slice_label'] = plot_df.index.astype(str)\n\n        # Add selection indicator\n        if selection is not None:\n            selection_set = set(selection)\n            plot_df['is_selected'] = plot_df.index.isin(selection_set)\n        else:\n            plot_df['is_selected'] = False\n\n        # Create scatter plot\n        if color is not None:\n            # Color by feature value\n            fig = px.scatter(\n                plot_df,\n                x=x,\n                y=y,\n                color=color,\n                hover_data=['slice_label'],\n                symbol='is_selected' if selection is not None else None,\n                symbol_map={True: 'star', False: 'circle'} if selection is not None else None,\n            )\n        else:\n            # Color by selection status\n            if selection is not None:\n                fig = px.scatter(\n                    plot_df,\n                    x=x,\n                    y=y,\n                    color='is_selected',\n                    hover_data=['slice_label'],\n                    color_discrete_map={True: 'red', False: 'lightgray'},\n                )\n            else:\n                fig = px.scatter(\n                    plot_df,\n                    x=x,\n                    y=y,\n                    hover_data=['slice_label'],\n                )\n\n        # Update layout for better readability\n        fig.update_layout(\n            xaxis_title=x,\n            yaxis_title=y,\n            hovermode='closest',\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter2D.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the scatter plot diagnostic.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the scatter plot diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter2D.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, x: str, y: str, selection: SliceCombination = None, color: str = None) -&gt; Figure\n</code></pre> <p>Create a 2D scatter plot of feature space.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>x</code> <code>str</code> <p>Column name for x-axis.</p> required <code>y</code> <code>str</code> <p>Column name for y-axis.</p> required <code>selection</code> <code>SliceCombination</code> <p>Optional tuple of slice identifiers to highlight.</p> <code>None</code> <code>color</code> <code>str</code> <p>Optional column name to use for color mapping.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If x, y, or color columns are not in df_features.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter.py</code> <pre><code>def plot(\n    self,\n    df_features: pd.DataFrame,\n    x: str,\n    y: str,\n    selection: SliceCombination = None,\n    color: str = None,\n) -&gt; go.Figure:\n    \"\"\"Create a 2D scatter plot of feature space.\n\n    Args:\n        df_features: Feature matrix with slices as rows, features as columns.\n        x: Column name for x-axis.\n        y: Column name for y-axis.\n        selection: Optional tuple of slice identifiers to highlight.\n        color: Optional column name to use for color mapping.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        KeyError: If x, y, or color columns are not in df_features.\n    \"\"\"\n    # Validate columns\n    if x not in df_features.columns:\n        raise KeyError(f\"Column '{x}' not found in df_features\")\n    if y not in df_features.columns:\n        raise KeyError(f\"Column '{y}' not found in df_features\")\n    if color is not None and color not in df_features.columns:\n        raise KeyError(f\"Column '{color}' not found in df_features\")\n\n    # Prepare data\n    plot_df = df_features.copy()\n    plot_df['slice_label'] = plot_df.index.astype(str)\n\n    # Add selection indicator\n    if selection is not None:\n        selection_set = set(selection)\n        plot_df['is_selected'] = plot_df.index.isin(selection_set)\n    else:\n        plot_df['is_selected'] = False\n\n    # Create scatter plot\n    if color is not None:\n        # Color by feature value\n        fig = px.scatter(\n            plot_df,\n            x=x,\n            y=y,\n            color=color,\n            hover_data=['slice_label'],\n            symbol='is_selected' if selection is not None else None,\n            symbol_map={True: 'star', False: 'circle'} if selection is not None else None,\n        )\n    else:\n        # Color by selection status\n        if selection is not None:\n            fig = px.scatter(\n                plot_df,\n                x=x,\n                y=y,\n                color='is_selected',\n                hover_data=['slice_label'],\n                color_discrete_map={True: 'red', False: 'lightgray'},\n            )\n        else:\n            fig = px.scatter(\n                plot_df,\n                x=x,\n                y=y,\n                hover_data=['slice_label'],\n            )\n\n    # Update layout for better readability\n    fig.update_layout(\n        xaxis_title=x,\n        yaxis_title=y,\n        hovermode='closest',\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter3D","title":"FeatureSpaceScatter3D","text":"<p>3D scatter plot for visualizing feature space.</p> <p>Creates an interactive 3D scatter plot of any three features from df_features. Can highlight a specific selection of slices. Works with any feature columns including PCA components or statistical features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize 3D PCA space\n&gt;&gt;&gt; scatter = FeatureSpaceScatter3D()\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     z='pc_2'\n... )\n&gt;&gt;&gt; fig.update_layout(title='3D PCA Space')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Highlight selection\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     z='pc_2',\n...     selection=('2024-01', '2024-04')\n... )\n\n&gt;&gt;&gt; # Color by feature value\n&gt;&gt;&gt; fig = scatter.plot(\n...     context.df_features,\n...     x='pc_0',\n...     y='pc_1',\n...     z='pc_2',\n...     color='mean__demand'\n... )\n</code></pre> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter.py</code> <pre><code>class FeatureSpaceScatter3D:\n    \"\"\"3D scatter plot for visualizing feature space.\n\n    Creates an interactive 3D scatter plot of any three features from df_features.\n    Can highlight a specific selection of slices. Works with any feature columns\n    including PCA components or statistical features.\n\n    Examples:\n\n        &gt;&gt;&gt; # Visualize 3D PCA space\n        &gt;&gt;&gt; scatter = FeatureSpaceScatter3D()\n        &gt;&gt;&gt; fig = scatter.plot(\n        ...     context.df_features,\n        ...     x='pc_0',\n        ...     y='pc_1',\n        ...     z='pc_2'\n        ... )\n        &gt;&gt;&gt; fig.update_layout(title='3D PCA Space')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Highlight selection\n        &gt;&gt;&gt; fig = scatter.plot(\n        ...     context.df_features,\n        ...     x='pc_0',\n        ...     y='pc_1',\n        ...     z='pc_2',\n        ...     selection=('2024-01', '2024-04')\n        ... )\n\n        &gt;&gt;&gt; # Color by feature value\n        &gt;&gt;&gt; fig = scatter.plot(\n        ...     context.df_features,\n        ...     x='pc_0',\n        ...     y='pc_1',\n        ...     z='pc_2',\n        ...     color='mean__demand'\n        ... )\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the 3D scatter plot diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_features: pd.DataFrame,\n        x: str,\n        y: str,\n        z: str,\n        selection: SliceCombination = None,\n        color: str = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create a 3D scatter plot of feature space.\n\n        Args:\n            df_features: Feature matrix with slices as rows, features as columns.\n            x: Column name for x-axis.\n            y: Column name for y-axis.\n            z: Column name for z-axis.\n            selection: Optional tuple of slice identifiers to highlight.\n            color: Optional column name to use for color mapping.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            KeyError: If x, y, z, or color columns are not in df_features.\n        \"\"\"\n        # Validate columns\n        if x not in df_features.columns:\n            raise KeyError(f\"Column '{x}' not found in df_features\")\n        if y not in df_features.columns:\n            raise KeyError(f\"Column '{y}' not found in df_features\")\n        if z not in df_features.columns:\n            raise KeyError(f\"Column '{z}' not found in df_features\")\n        if color is not None and color not in df_features.columns:\n            raise KeyError(f\"Column '{color}' not found in df_features\")\n\n        # Prepare data\n        plot_df = df_features.copy()\n        plot_df['slice_label'] = plot_df.index.astype(str)\n\n        # Add selection indicator\n        if selection is not None:\n            selection_set = set(selection)\n            plot_df['is_selected'] = plot_df.index.isin(selection_set)\n        else:\n            plot_df['is_selected'] = False\n\n        # Create 3D scatter plot\n        if color is not None:\n            # Color by feature value\n            fig = px.scatter_3d(\n                plot_df,\n                x=x,\n                y=y,\n                z=z,\n                color=color,\n                hover_data=['slice_label'],\n                symbol='is_selected' if selection is not None else None,\n                symbol_map={True: 'diamond', False: 'circle'} if selection is not None else None,\n            )\n        else:\n            # Color by selection status\n            if selection is not None:\n                fig = px.scatter_3d(\n                    plot_df,\n                    x=x,\n                    y=y,\n                    z=z,\n                    color='is_selected',\n                    hover_data=['slice_label'],\n                    color_discrete_map={True: 'red', False: 'lightgray'},\n                )\n            else:\n                fig = px.scatter_3d(\n                    plot_df,\n                    x=x,\n                    y=y,\n                    z=z,\n                    hover_data=['slice_label'],\n                )\n\n        # Update layout for better readability\n        fig.update_layout(\n            scene=dict(\n                xaxis_title=x,\n                yaxis_title=y,\n                zaxis_title=z,\n            ),\n            hovermode='closest',\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter3D.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the 3D scatter plot diagnostic.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the 3D scatter plot diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatter3D.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, x: str, y: str, z: str, selection: SliceCombination = None, color: str = None) -&gt; Figure\n</code></pre> <p>Create a 3D scatter plot of feature space.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>x</code> <code>str</code> <p>Column name for x-axis.</p> required <code>y</code> <code>str</code> <p>Column name for y-axis.</p> required <code>z</code> <code>str</code> <p>Column name for z-axis.</p> required <code>selection</code> <code>SliceCombination</code> <p>Optional tuple of slice identifiers to highlight.</p> <code>None</code> <code>color</code> <code>str</code> <p>Optional column name to use for color mapping.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If x, y, z, or color columns are not in df_features.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter.py</code> <pre><code>def plot(\n    self,\n    df_features: pd.DataFrame,\n    x: str,\n    y: str,\n    z: str,\n    selection: SliceCombination = None,\n    color: str = None,\n) -&gt; go.Figure:\n    \"\"\"Create a 3D scatter plot of feature space.\n\n    Args:\n        df_features: Feature matrix with slices as rows, features as columns.\n        x: Column name for x-axis.\n        y: Column name for y-axis.\n        z: Column name for z-axis.\n        selection: Optional tuple of slice identifiers to highlight.\n        color: Optional column name to use for color mapping.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        KeyError: If x, y, z, or color columns are not in df_features.\n    \"\"\"\n    # Validate columns\n    if x not in df_features.columns:\n        raise KeyError(f\"Column '{x}' not found in df_features\")\n    if y not in df_features.columns:\n        raise KeyError(f\"Column '{y}' not found in df_features\")\n    if z not in df_features.columns:\n        raise KeyError(f\"Column '{z}' not found in df_features\")\n    if color is not None and color not in df_features.columns:\n        raise KeyError(f\"Column '{color}' not found in df_features\")\n\n    # Prepare data\n    plot_df = df_features.copy()\n    plot_df['slice_label'] = plot_df.index.astype(str)\n\n    # Add selection indicator\n    if selection is not None:\n        selection_set = set(selection)\n        plot_df['is_selected'] = plot_df.index.isin(selection_set)\n    else:\n        plot_df['is_selected'] = False\n\n    # Create 3D scatter plot\n    if color is not None:\n        # Color by feature value\n        fig = px.scatter_3d(\n            plot_df,\n            x=x,\n            y=y,\n            z=z,\n            color=color,\n            hover_data=['slice_label'],\n            symbol='is_selected' if selection is not None else None,\n            symbol_map={True: 'diamond', False: 'circle'} if selection is not None else None,\n        )\n    else:\n        # Color by selection status\n        if selection is not None:\n            fig = px.scatter_3d(\n                plot_df,\n                x=x,\n                y=y,\n                z=z,\n                color='is_selected',\n                hover_data=['slice_label'],\n                color_discrete_map={True: 'red', False: 'lightgray'},\n            )\n        else:\n            fig = px.scatter_3d(\n                plot_df,\n                x=x,\n                y=y,\n                z=z,\n                hover_data=['slice_label'],\n            )\n\n    # Update layout for better readability\n    fig.update_layout(\n        scene=dict(\n            xaxis_title=x,\n            yaxis_title=y,\n            zaxis_title=z,\n        ),\n        hovermode='closest',\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatterMatrix","title":"FeatureSpaceScatterMatrix","text":"<p>Scatter matrix (SPLOM) for visualizing relationships between multiple features.</p> <p>Creates an interactive scatter plot matrix showing pairwise relationships between all specified features. Can highlight a specific selection of slices. Useful for exploring multi-dimensional feature spaces and identifying feature correlations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize PCA components\n&gt;&gt;&gt; scatter_matrix = FeatureSpaceScatterMatrix()\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     context.df_features,\n...     dimensions=['pc_0', 'pc_1', 'pc_2']\n... )\n&gt;&gt;&gt; fig.update_layout(title='PCA Component Relationships')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Visualize statistical features with selection\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     context.df_features,\n...     dimensions=['mean__demand', 'std__demand', 'max__wind'],\n...     selection=('2024-01', '2024-04', '2024-07')\n... )\n\n&gt;&gt;&gt; # Color by a feature value\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     context.df_features,\n...     dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3'],\n...     color='mean__demand'\n... )\n\n&gt;&gt;&gt; # All features\n&gt;&gt;&gt; fig = scatter_matrix.plot(context.df_features)\n</code></pre> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter_matrix.py</code> <pre><code>class FeatureSpaceScatterMatrix:\n    \"\"\"Scatter matrix (SPLOM) for visualizing relationships between multiple features.\n\n    Creates an interactive scatter plot matrix showing pairwise relationships between\n    all specified features. Can highlight a specific selection of slices. Useful for\n    exploring multi-dimensional feature spaces and identifying feature correlations.\n\n    Examples:\n\n        &gt;&gt;&gt; # Visualize PCA components\n        &gt;&gt;&gt; scatter_matrix = FeatureSpaceScatterMatrix()\n        &gt;&gt;&gt; fig = scatter_matrix.plot(\n        ...     context.df_features,\n        ...     dimensions=['pc_0', 'pc_1', 'pc_2']\n        ... )\n        &gt;&gt;&gt; fig.update_layout(title='PCA Component Relationships')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Visualize statistical features with selection\n        &gt;&gt;&gt; fig = scatter_matrix.plot(\n        ...     context.df_features,\n        ...     dimensions=['mean__demand', 'std__demand', 'max__wind'],\n        ...     selection=('2024-01', '2024-04', '2024-07')\n        ... )\n\n        &gt;&gt;&gt; # Color by a feature value\n        &gt;&gt;&gt; fig = scatter_matrix.plot(\n        ...     context.df_features,\n        ...     dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3'],\n        ...     color='mean__demand'\n        ... )\n\n        &gt;&gt;&gt; # All features\n        &gt;&gt;&gt; fig = scatter_matrix.plot(context.df_features)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the scatter matrix diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_features: pd.DataFrame,\n        dimensions: list[str] = None,\n        selection: SliceCombination = None,\n        color: str = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create a scatter plot matrix of feature space.\n\n        Args:\n            df_features: Feature matrix with slices as rows, features as columns.\n            dimensions: List of column names to include in the matrix. If None,\n                uses all columns (may be slow for many features).\n            selection: Optional tuple of slice identifiers to highlight.\n            color: Optional column name to use for color mapping. If None and\n                selection is provided, colors by selection status.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            KeyError: If any dimension or color column is not in df_features.\n            ValueError: If dimensions list is empty.\n        \"\"\"\n        # Handle dimensions default\n        if dimensions is None:\n            dimensions = list(df_features.columns)\n\n        if len(dimensions) == 0:\n            raise ValueError(\"dimensions list cannot be empty\")\n\n        # Validate columns\n        for dim in dimensions:\n            if dim not in df_features.columns:\n                raise KeyError(f\"Column '{dim}' not found in df_features\")\n        if color is not None and color not in df_features.columns:\n            raise KeyError(f\"Column '{color}' not found in df_features\")\n\n        # Prepare data\n        plot_df = df_features[dimensions].copy()\n        plot_df['slice_label'] = df_features.index.astype(str)\n\n        # Add selection indicator\n        if selection is not None:\n            selection_set = set(selection)\n            plot_df['is_selected'] = df_features.index.isin(selection_set)\n            # Order so selected points are drawn on top\n            plot_df = pd.concat([\n                plot_df[~plot_df['is_selected']],\n                plot_df[plot_df['is_selected']]\n            ], ignore_index=False)\n        else:\n            plot_df['is_selected'] = False\n\n        # Create scatter matrix\n        if color is not None:\n            # Color by feature value\n            fig = px.scatter_matrix(\n                plot_df,\n                dimensions=dimensions,\n                color=color,\n                hover_data=['slice_label'],\n                symbol='is_selected' if selection is not None else None,\n                symbol_map={True: 'star', False: 'circle'} if selection is not None else None,\n            )\n        else:\n            # Color by selection status\n            if selection is not None:\n                fig = px.scatter_matrix(\n                    plot_df,\n                    dimensions=dimensions,\n                    color='is_selected',\n                    hover_data=['slice_label'],\n                    color_discrete_map={True: 'red', False: 'lightgray'},\n                    symbol='is_selected',\n                    symbol_map={True: 'star', False: 'circle'},\n                )\n            else:\n                fig = px.scatter_matrix(\n                    plot_df,\n                    dimensions=dimensions,\n                    hover_data=['slice_label'],\n                )\n\n        # Update layout for better readability\n        fig.update_traces(\n            diagonal_visible=False,\n            showupperhalf=False,\n            marker=dict(size=4)\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatterMatrix.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the scatter matrix diagnostic.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter_matrix.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the scatter matrix diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureSpaceScatterMatrix.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, dimensions: list[str] = None, selection: SliceCombination = None, color: str = None) -&gt; Figure\n</code></pre> <p>Create a scatter plot matrix of feature space.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>dimensions</code> <code>list[str]</code> <p>List of column names to include in the matrix. If None, uses all columns (may be slow for many features).</p> <code>None</code> <code>selection</code> <code>SliceCombination</code> <p>Optional tuple of slice identifiers to highlight.</p> <code>None</code> <code>color</code> <code>str</code> <p>Optional column name to use for color mapping. If None and selection is provided, colors by selection status.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If any dimension or color column is not in df_features.</p> <code>ValueError</code> <p>If dimensions list is empty.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_space_scatter_matrix.py</code> <pre><code>def plot(\n    self,\n    df_features: pd.DataFrame,\n    dimensions: list[str] = None,\n    selection: SliceCombination = None,\n    color: str = None,\n) -&gt; go.Figure:\n    \"\"\"Create a scatter plot matrix of feature space.\n\n    Args:\n        df_features: Feature matrix with slices as rows, features as columns.\n        dimensions: List of column names to include in the matrix. If None,\n            uses all columns (may be slow for many features).\n        selection: Optional tuple of slice identifiers to highlight.\n        color: Optional column name to use for color mapping. If None and\n            selection is provided, colors by selection status.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        KeyError: If any dimension or color column is not in df_features.\n        ValueError: If dimensions list is empty.\n    \"\"\"\n    # Handle dimensions default\n    if dimensions is None:\n        dimensions = list(df_features.columns)\n\n    if len(dimensions) == 0:\n        raise ValueError(\"dimensions list cannot be empty\")\n\n    # Validate columns\n    for dim in dimensions:\n        if dim not in df_features.columns:\n            raise KeyError(f\"Column '{dim}' not found in df_features\")\n    if color is not None and color not in df_features.columns:\n        raise KeyError(f\"Column '{color}' not found in df_features\")\n\n    # Prepare data\n    plot_df = df_features[dimensions].copy()\n    plot_df['slice_label'] = df_features.index.astype(str)\n\n    # Add selection indicator\n    if selection is not None:\n        selection_set = set(selection)\n        plot_df['is_selected'] = df_features.index.isin(selection_set)\n        # Order so selected points are drawn on top\n        plot_df = pd.concat([\n            plot_df[~plot_df['is_selected']],\n            plot_df[plot_df['is_selected']]\n        ], ignore_index=False)\n    else:\n        plot_df['is_selected'] = False\n\n    # Create scatter matrix\n    if color is not None:\n        # Color by feature value\n        fig = px.scatter_matrix(\n            plot_df,\n            dimensions=dimensions,\n            color=color,\n            hover_data=['slice_label'],\n            symbol='is_selected' if selection is not None else None,\n            symbol_map={True: 'star', False: 'circle'} if selection is not None else None,\n        )\n    else:\n        # Color by selection status\n        if selection is not None:\n            fig = px.scatter_matrix(\n                plot_df,\n                dimensions=dimensions,\n                color='is_selected',\n                hover_data=['slice_label'],\n                color_discrete_map={True: 'red', False: 'lightgray'},\n                symbol='is_selected',\n                symbol_map={True: 'star', False: 'circle'},\n            )\n        else:\n            fig = px.scatter_matrix(\n                plot_df,\n                dimensions=dimensions,\n                hover_data=['slice_label'],\n            )\n\n    # Update layout for better readability\n    fig.update_traces(\n        diagonal_visible=False,\n        showupperhalf=False,\n        marker=dict(size=4)\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.PCAVarianceExplained","title":"PCAVarianceExplained","text":"<p>Visualize explained variance ratio for PCA components.</p> <p>Creates a bar chart showing the proportion of variance explained by each principal component, along with cumulative variance. Helps determine how many components are needed to capture most of the data's variance.</p> <p>This diagnostic requires the fitted PCAFeatureEngineer instance to access the explained_variance_ratio_ attribute.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get PCA engineer from pipeline\n&gt;&gt;&gt; pca_engineer = pipeline.engineers['pca']\n&gt;&gt;&gt; variance_plot = PCAVarianceExplained(pca_engineer)\n&gt;&gt;&gt; fig = variance_plot.plot()\n&gt;&gt;&gt; fig.update_layout(title='PCA Variance Explained')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # With custom number of components shown\n&gt;&gt;&gt; fig = variance_plot.plot(n_components=10)\n\n&gt;&gt;&gt; # After running workflow\n&gt;&gt;&gt; context_with_features = workflow.feature_engineer.run(context)\n&gt;&gt;&gt; pca_eng = workflow.feature_engineer.engineers['pca']\n&gt;&gt;&gt; variance_plot = PCAVarianceExplained(pca_eng)\n&gt;&gt;&gt; fig = variance_plot.plot()\n</code></pre> Source code in <code>energy_repset/diagnostics/feature_space/pca_variance_explained.py</code> <pre><code>class PCAVarianceExplained:\n    \"\"\"Visualize explained variance ratio for PCA components.\n\n    Creates a bar chart showing the proportion of variance explained by each\n    principal component, along with cumulative variance. Helps determine how\n    many components are needed to capture most of the data's variance.\n\n    This diagnostic requires the fitted PCAFeatureEngineer instance to access\n    the explained_variance_ratio_ attribute.\n\n    Examples:\n\n        &gt;&gt;&gt; # Get PCA engineer from pipeline\n        &gt;&gt;&gt; pca_engineer = pipeline.engineers['pca']\n        &gt;&gt;&gt; variance_plot = PCAVarianceExplained(pca_engineer)\n        &gt;&gt;&gt; fig = variance_plot.plot()\n        &gt;&gt;&gt; fig.update_layout(title='PCA Variance Explained')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # With custom number of components shown\n        &gt;&gt;&gt; fig = variance_plot.plot(n_components=10)\n\n        &gt;&gt;&gt; # After running workflow\n        &gt;&gt;&gt; context_with_features = workflow.feature_engineer.run(context)\n        &gt;&gt;&gt; pca_eng = workflow.feature_engineer.engineers['pca']\n        &gt;&gt;&gt; variance_plot = PCAVarianceExplained(pca_eng)\n        &gt;&gt;&gt; fig = variance_plot.plot()\n    \"\"\"\n\n    def __init__(self, pca_engineer: PCAFeatureEngineer):\n        \"\"\"Initialize the PCA variance explained diagnostic.\n\n        Args:\n            pca_engineer: A fitted PCAFeatureEngineer instance. Must have been\n                fitted on data (i.e., calc_and_get_features_df has been called).\n        \"\"\"\n        self.pca_engineer = pca_engineer\n\n    def plot(self, n_components: int = None, show_cumulative: bool = True) -&gt; go.Figure:\n        \"\"\"Create a bar chart of explained variance ratios.\n\n        Args:\n            n_components: Number of components to show. If None, shows all components.\n            show_cumulative: If True, adds a line showing cumulative variance explained.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            AttributeError: If the PCA engineer has not been fitted yet.\n        \"\"\"\n        # Get variance ratios\n        if not hasattr(self.pca_engineer, 'explained_variance_ratio_'):\n            raise AttributeError(\n                \"PCA engineer has not been fitted. Call calc_and_get_features_df() first.\"\n            )\n\n        variance_ratio = self.pca_engineer.explained_variance_ratio_\n\n        # Limit to requested number of components\n        if n_components is not None:\n            variance_ratio = variance_ratio[:n_components]\n\n        # Prepare data\n        n = len(variance_ratio)\n        component_labels = [f'PC{i}' for i in range(n)]\n        cumulative_variance = variance_ratio.cumsum()\n\n        # Create figure\n        fig = go.Figure()\n\n        # Add variance bars\n        fig.add_trace(go.Bar(\n            x=component_labels,\n            y=variance_ratio,\n            name='Individual',\n            marker_color='lightblue',\n            text=[f'{v:.1%}' for v in variance_ratio],\n            textposition='outside',\n        ))\n\n        # Add cumulative line if requested\n        if show_cumulative:\n            fig.add_trace(go.Scatter(\n                x=component_labels,\n                y=cumulative_variance,\n                name='Cumulative',\n                mode='lines+markers',\n                line=dict(color='red', width=2),\n                yaxis='y2',\n                text=[f'{v:.1%}' for v in cumulative_variance],\n                textposition='top center',\n            ))\n\n        # Update layout\n        layout_kwargs = dict(\n            xaxis_title='Principal Component',\n            yaxis_title='Explained Variance Ratio',\n            hovermode='x unified',\n            yaxis=dict(tickformat='.0%'),\n        )\n\n        if show_cumulative:\n            layout_kwargs['yaxis2'] = dict(\n                title='Cumulative Variance',\n                overlaying='y',\n                side='right',\n                tickformat='.0%',\n                range=[0, 1.05],\n            )\n\n        fig.update_layout(**layout_kwargs)\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.PCAVarianceExplained.__init__","title":"__init__","text":"<pre><code>__init__(pca_engineer: PCAFeatureEngineer)\n</code></pre> <p>Initialize the PCA variance explained diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>pca_engineer</code> <code>PCAFeatureEngineer</code> <p>A fitted PCAFeatureEngineer instance. Must have been fitted on data (i.e., calc_and_get_features_df has been called).</p> required Source code in <code>energy_repset/diagnostics/feature_space/pca_variance_explained.py</code> <pre><code>def __init__(self, pca_engineer: PCAFeatureEngineer):\n    \"\"\"Initialize the PCA variance explained diagnostic.\n\n    Args:\n        pca_engineer: A fitted PCAFeatureEngineer instance. Must have been\n            fitted on data (i.e., calc_and_get_features_df has been called).\n    \"\"\"\n    self.pca_engineer = pca_engineer\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.PCAVarianceExplained.plot","title":"plot","text":"<pre><code>plot(n_components: int = None, show_cumulative: bool = True) -&gt; Figure\n</code></pre> <p>Create a bar chart of explained variance ratios.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of components to show. If None, shows all components.</p> <code>None</code> <code>show_cumulative</code> <code>bool</code> <p>If True, adds a line showing cumulative variance explained.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the PCA engineer has not been fitted yet.</p> Source code in <code>energy_repset/diagnostics/feature_space/pca_variance_explained.py</code> <pre><code>def plot(self, n_components: int = None, show_cumulative: bool = True) -&gt; go.Figure:\n    \"\"\"Create a bar chart of explained variance ratios.\n\n    Args:\n        n_components: Number of components to show. If None, shows all components.\n        show_cumulative: If True, adds a line showing cumulative variance explained.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        AttributeError: If the PCA engineer has not been fitted yet.\n    \"\"\"\n    # Get variance ratios\n    if not hasattr(self.pca_engineer, 'explained_variance_ratio_'):\n        raise AttributeError(\n            \"PCA engineer has not been fitted. Call calc_and_get_features_df() first.\"\n        )\n\n    variance_ratio = self.pca_engineer.explained_variance_ratio_\n\n    # Limit to requested number of components\n    if n_components is not None:\n        variance_ratio = variance_ratio[:n_components]\n\n    # Prepare data\n    n = len(variance_ratio)\n    component_labels = [f'PC{i}' for i in range(n)]\n    cumulative_variance = variance_ratio.cumsum()\n\n    # Create figure\n    fig = go.Figure()\n\n    # Add variance bars\n    fig.add_trace(go.Bar(\n        x=component_labels,\n        y=variance_ratio,\n        name='Individual',\n        marker_color='lightblue',\n        text=[f'{v:.1%}' for v in variance_ratio],\n        textposition='outside',\n    ))\n\n    # Add cumulative line if requested\n    if show_cumulative:\n        fig.add_trace(go.Scatter(\n            x=component_labels,\n            y=cumulative_variance,\n            name='Cumulative',\n            mode='lines+markers',\n            line=dict(color='red', width=2),\n            yaxis='y2',\n            text=[f'{v:.1%}' for v in cumulative_variance],\n            textposition='top center',\n        ))\n\n    # Update layout\n    layout_kwargs = dict(\n        xaxis_title='Principal Component',\n        yaxis_title='Explained Variance Ratio',\n        hovermode='x unified',\n        yaxis=dict(tickformat='.0%'),\n    )\n\n    if show_cumulative:\n        layout_kwargs['yaxis2'] = dict(\n            title='Cumulative Variance',\n            overlaying='y',\n            side='right',\n            tickformat='.0%',\n            range=[0, 1.05],\n        )\n\n    fig.update_layout(**layout_kwargs)\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureCorrelationHeatmap","title":"FeatureCorrelationHeatmap","text":"<p>Visualize correlation matrix of features.</p> <p>Creates an interactive heatmap showing Pearson correlations between all features in the feature matrix. Helps identify redundant features and understand feature relationships. Can optionally show only the lower triangle to avoid redundancy.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize all feature correlations\n&gt;&gt;&gt; heatmap = FeatureCorrelationHeatmap()\n&gt;&gt;&gt; fig = heatmap.plot(context.df_features)\n&gt;&gt;&gt; fig.update_layout(title='Feature Correlation Matrix')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Show only lower triangle\n&gt;&gt;&gt; fig = heatmap.plot(context.df_features, show_lower_only=True)\n\n&gt;&gt;&gt; # Subset of features\n&gt;&gt;&gt; selected_features = context.df_features[['pc_0', 'pc_1', 'mean__demand']]\n&gt;&gt;&gt; fig = heatmap.plot(selected_features)\n</code></pre> Source code in <code>energy_repset/diagnostics/feature_space/feature_correlation_heatmap.py</code> <pre><code>class FeatureCorrelationHeatmap:\n    \"\"\"Visualize correlation matrix of features.\n\n    Creates an interactive heatmap showing Pearson correlations between all features\n    in the feature matrix. Helps identify redundant features and understand feature\n    relationships. Can optionally show only the lower triangle to avoid redundancy.\n\n    Examples:\n\n        &gt;&gt;&gt; # Visualize all feature correlations\n        &gt;&gt;&gt; heatmap = FeatureCorrelationHeatmap()\n        &gt;&gt;&gt; fig = heatmap.plot(context.df_features)\n        &gt;&gt;&gt; fig.update_layout(title='Feature Correlation Matrix')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Show only lower triangle\n        &gt;&gt;&gt; fig = heatmap.plot(context.df_features, show_lower_only=True)\n\n        &gt;&gt;&gt; # Subset of features\n        &gt;&gt;&gt; selected_features = context.df_features[['pc_0', 'pc_1', 'mean__demand']]\n        &gt;&gt;&gt; fig = heatmap.plot(selected_features)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the feature correlation heatmap diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_features: pd.DataFrame,\n        method: str = 'pearson',\n        show_lower_only: bool = False,\n    ) -&gt; go.Figure:\n        \"\"\"Create a heatmap of feature correlations.\n\n        Args:\n            df_features: Feature matrix with slices as rows, features as columns.\n            method: Correlation method ('pearson', 'spearman', or 'kendall').\n                Default is 'pearson'.\n            show_lower_only: If True, shows only the lower triangle of the\n                correlation matrix (removes redundant upper triangle and diagonal).\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            ValueError: If method is not one of the supported correlation methods.\n        \"\"\"\n        if method not in ['pearson', 'spearman', 'kendall']:\n            raise ValueError(\n                f\"method must be 'pearson', 'spearman', or 'kendall', got '{method}'\"\n            )\n\n        # Calculate correlation matrix\n        corr_matrix = df_features.corr(method=method)\n\n        # Mask upper triangle if requested\n        if show_lower_only:\n            mask = pd.DataFrame(\n                False,\n                index=corr_matrix.index,\n                columns=corr_matrix.columns\n            )\n            # Set upper triangle and diagonal to True (to be masked)\n            for i in range(len(corr_matrix)):\n                for j in range(i, len(corr_matrix)):\n                    mask.iloc[i, j] = True\n\n            # Apply mask by setting values to NaN\n            corr_matrix = corr_matrix.where(~mask)\n\n        # Create heatmap\n        fig = px.imshow(\n            corr_matrix,\n            x=corr_matrix.columns,\n            y=corr_matrix.index,\n            color_continuous_scale='RdBu_r',\n            color_continuous_midpoint=0,\n            zmin=-1,\n            zmax=1,\n            aspect='auto',\n        )\n\n        # Update layout for better readability\n        fig.update_layout(\n            xaxis_title='',\n            yaxis_title='',\n            coloraxis_colorbar=dict(title='Correlation'),\n        )\n\n        # Improve text readability\n        fig.update_traces(\n            text=corr_matrix.round(2).values,\n            texttemplate='%{text}',\n            textfont=dict(size=10),\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureCorrelationHeatmap.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the feature correlation heatmap diagnostic.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_correlation_heatmap.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the feature correlation heatmap diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureCorrelationHeatmap.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, method: str = 'pearson', show_lower_only: bool = False) -&gt; Figure\n</code></pre> <p>Create a heatmap of feature correlations.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>method</code> <code>str</code> <p>Correlation method ('pearson', 'spearman', or 'kendall'). Default is 'pearson'.</p> <code>'pearson'</code> <code>show_lower_only</code> <code>bool</code> <p>If True, shows only the lower triangle of the correlation matrix (removes redundant upper triangle and diagonal).</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is not one of the supported correlation methods.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_correlation_heatmap.py</code> <pre><code>def plot(\n    self,\n    df_features: pd.DataFrame,\n    method: str = 'pearson',\n    show_lower_only: bool = False,\n) -&gt; go.Figure:\n    \"\"\"Create a heatmap of feature correlations.\n\n    Args:\n        df_features: Feature matrix with slices as rows, features as columns.\n        method: Correlation method ('pearson', 'spearman', or 'kendall').\n            Default is 'pearson'.\n        show_lower_only: If True, shows only the lower triangle of the\n            correlation matrix (removes redundant upper triangle and diagonal).\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        ValueError: If method is not one of the supported correlation methods.\n    \"\"\"\n    if method not in ['pearson', 'spearman', 'kendall']:\n        raise ValueError(\n            f\"method must be 'pearson', 'spearman', or 'kendall', got '{method}'\"\n        )\n\n    # Calculate correlation matrix\n    corr_matrix = df_features.corr(method=method)\n\n    # Mask upper triangle if requested\n    if show_lower_only:\n        mask = pd.DataFrame(\n            False,\n            index=corr_matrix.index,\n            columns=corr_matrix.columns\n        )\n        # Set upper triangle and diagonal to True (to be masked)\n        for i in range(len(corr_matrix)):\n            for j in range(i, len(corr_matrix)):\n                mask.iloc[i, j] = True\n\n        # Apply mask by setting values to NaN\n        corr_matrix = corr_matrix.where(~mask)\n\n    # Create heatmap\n    fig = px.imshow(\n        corr_matrix,\n        x=corr_matrix.columns,\n        y=corr_matrix.index,\n        color_continuous_scale='RdBu_r',\n        color_continuous_midpoint=0,\n        zmin=-1,\n        zmax=1,\n        aspect='auto',\n    )\n\n    # Update layout for better readability\n    fig.update_layout(\n        xaxis_title='',\n        yaxis_title='',\n        coloraxis_colorbar=dict(title='Correlation'),\n    )\n\n    # Improve text readability\n    fig.update_traces(\n        text=corr_matrix.round(2).values,\n        texttemplate='%{text}',\n        textfont=dict(size=10),\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureDistributions","title":"FeatureDistributions","text":"<p>Visualize distributions of all features as histograms.</p> <p>Creates a grid of histograms showing the distribution of each feature across all slices. Helps identify feature scales, skewness, and potential outliers. Useful for understanding the feature space before selection.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Visualize all feature distributions\n&gt;&gt;&gt; dist_plot = FeatureDistributions()\n&gt;&gt;&gt; fig = dist_plot.plot(context.df_features)\n&gt;&gt;&gt; fig.update_layout(title='Feature Distributions')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Subset of features\n&gt;&gt;&gt; selected_features = context.df_features[['pc_0', 'pc_1', 'mean__demand']]\n&gt;&gt;&gt; fig = dist_plot.plot(selected_features)\n\n&gt;&gt;&gt; # With custom bin count\n&gt;&gt;&gt; fig = dist_plot.plot(context.df_features, nbins=30)\n</code></pre> Source code in <code>energy_repset/diagnostics/feature_space/feature_distributions.py</code> <pre><code>class FeatureDistributions:\n    \"\"\"Visualize distributions of all features as histograms.\n\n    Creates a grid of histograms showing the distribution of each feature across\n    all slices. Helps identify feature scales, skewness, and potential outliers.\n    Useful for understanding the feature space before selection.\n\n    Examples:\n\n        &gt;&gt;&gt; # Visualize all feature distributions\n        &gt;&gt;&gt; dist_plot = FeatureDistributions()\n        &gt;&gt;&gt; fig = dist_plot.plot(context.df_features)\n        &gt;&gt;&gt; fig.update_layout(title='Feature Distributions')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Subset of features\n        &gt;&gt;&gt; selected_features = context.df_features[['pc_0', 'pc_1', 'mean__demand']]\n        &gt;&gt;&gt; fig = dist_plot.plot(selected_features)\n\n        &gt;&gt;&gt; # With custom bin count\n        &gt;&gt;&gt; fig = dist_plot.plot(context.df_features, nbins=30)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the feature distributions diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_features: pd.DataFrame,\n        nbins: int = 20,\n        cols: int = 3,\n    ) -&gt; go.Figure:\n        \"\"\"Create a grid of histograms for all features.\n\n        Args:\n            df_features: Feature matrix with slices as rows, features as columns.\n            nbins: Number of bins for each histogram. Default is 20.\n            cols: Number of columns in the subplot grid. Default is 3.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            ValueError: If df_features is empty or nbins/cols are invalid.\n        \"\"\"\n        if df_features.empty:\n            raise ValueError(\"df_features cannot be empty\")\n        if nbins &lt;= 0:\n            raise ValueError(\"nbins must be positive\")\n        if cols &lt;= 0:\n            raise ValueError(\"cols must be positive\")\n\n        features = list(df_features.columns)\n        n_features = len(features)\n\n        # Calculate grid dimensions\n        rows = (n_features + cols - 1) // cols  # Ceiling division\n\n        # Create subplots\n        fig = make_subplots(\n            rows=rows,\n            cols=cols,\n            subplot_titles=features,\n            vertical_spacing=0.12 / rows if rows &gt; 1 else 0.1,\n            horizontal_spacing=0.1 / cols if cols &gt; 1 else 0.1,\n        )\n\n        # Add histogram for each feature\n        for idx, feature in enumerate(features):\n            row = idx // cols + 1\n            col = idx % cols + 1\n\n            fig.add_trace(\n                go.Histogram(\n                    x=df_features[feature],\n                    nbinsx=nbins,\n                    name=feature,\n                    showlegend=False,\n                    marker_color='lightblue',\n                ),\n                row=row,\n                col=col,\n            )\n\n            # Update axes labels\n            fig.update_xaxes(title_text=feature, row=row, col=col)\n            fig.update_yaxes(title_text='Count', row=row, col=col)\n\n        # Update overall layout\n        fig.update_layout(\n            height=300 * rows,\n            showlegend=False,\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureDistributions.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the feature distributions diagnostic.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_distributions.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the feature distributions diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.feature_space.FeatureDistributions.plot","title":"plot","text":"<pre><code>plot(df_features: DataFrame, nbins: int = 20, cols: int = 3) -&gt; Figure\n</code></pre> <p>Create a grid of histograms for all features.</p> <p>Parameters:</p> Name Type Description Default <code>df_features</code> <code>DataFrame</code> <p>Feature matrix with slices as rows, features as columns.</p> required <code>nbins</code> <code>int</code> <p>Number of bins for each histogram. Default is 20.</p> <code>20</code> <code>cols</code> <code>int</code> <p>Number of columns in the subplot grid. Default is 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If df_features is empty or nbins/cols are invalid.</p> Source code in <code>energy_repset/diagnostics/feature_space/feature_distributions.py</code> <pre><code>def plot(\n    self,\n    df_features: pd.DataFrame,\n    nbins: int = 20,\n    cols: int = 3,\n) -&gt; go.Figure:\n    \"\"\"Create a grid of histograms for all features.\n\n    Args:\n        df_features: Feature matrix with slices as rows, features as columns.\n        nbins: Number of bins for each histogram. Default is 20.\n        cols: Number of columns in the subplot grid. Default is 3.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        ValueError: If df_features is empty or nbins/cols are invalid.\n    \"\"\"\n    if df_features.empty:\n        raise ValueError(\"df_features cannot be empty\")\n    if nbins &lt;= 0:\n        raise ValueError(\"nbins must be positive\")\n    if cols &lt;= 0:\n        raise ValueError(\"cols must be positive\")\n\n    features = list(df_features.columns)\n    n_features = len(features)\n\n    # Calculate grid dimensions\n    rows = (n_features + cols - 1) // cols  # Ceiling division\n\n    # Create subplots\n    fig = make_subplots(\n        rows=rows,\n        cols=cols,\n        subplot_titles=features,\n        vertical_spacing=0.12 / rows if rows &gt; 1 else 0.1,\n        horizontal_spacing=0.1 / cols if cols &gt; 1 else 0.1,\n    )\n\n    # Add histogram for each feature\n    for idx, feature in enumerate(features):\n        row = idx // cols + 1\n        col = idx % cols + 1\n\n        fig.add_trace(\n            go.Histogram(\n                x=df_features[feature],\n                nbinsx=nbins,\n                name=feature,\n                showlegend=False,\n                marker_color='lightblue',\n            ),\n            row=row,\n            col=col,\n        )\n\n        # Update axes labels\n        fig.update_xaxes(title_text=feature, row=row, col=col)\n        fig.update_yaxes(title_text='Count', row=row, col=col)\n\n    # Update overall layout\n    fig.update_layout(\n        height=300 * rows,\n        showlegend=False,\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#score-components","title":"Score Components","text":""},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayECDF","title":"DistributionOverlayECDF","text":"<p>Overlay empirical cumulative distribution functions (ECDF) to compare distributions.</p> <p>Creates a plot showing the ECDF of a variable for both the full dataset and a selection. This helps visualize how well the selection represents the full distribution, which is what WassersteinFidelity measures.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare demand distribution\n&gt;&gt;&gt; ecdf_plot = DistributionOverlayECDF()\n&gt;&gt;&gt; full_data = context.df_raw['demand']\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, 'demand']\n&gt;&gt;&gt; fig = ecdf_plot.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Demand Distribution: Full vs Selected')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Alternative: using iloc\n&gt;&gt;&gt; selection_mask = context.df_raw.index.isin(selected_indices)\n&gt;&gt;&gt; fig = ecdf_plot.plot(\n...     context.df_raw['wind'],\n...     context.df_raw.loc[selection_mask, 'wind']\n... )\n</code></pre> Source code in <code>energy_repset/diagnostics/score_components/distribution_overlay.py</code> <pre><code>class DistributionOverlayECDF:\n    \"\"\"Overlay empirical cumulative distribution functions (ECDF) to compare distributions.\n\n    Creates a plot showing the ECDF of a variable for both the full dataset and\n    a selection. This helps visualize how well the selection represents the full\n    distribution, which is what WassersteinFidelity measures.\n\n    Examples:\n\n        &gt;&gt;&gt; # Compare demand distribution\n        &gt;&gt;&gt; ecdf_plot = DistributionOverlayECDF()\n        &gt;&gt;&gt; full_data = context.df_raw['demand']\n        &gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n        &gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, 'demand']\n        &gt;&gt;&gt; fig = ecdf_plot.plot(full_data, selected_data)\n        &gt;&gt;&gt; fig.update_layout(title='Demand Distribution: Full vs Selected')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Alternative: using iloc\n        &gt;&gt;&gt; selection_mask = context.df_raw.index.isin(selected_indices)\n        &gt;&gt;&gt; fig = ecdf_plot.plot(\n        ...     context.df_raw['wind'],\n        ...     context.df_raw.loc[selection_mask, 'wind']\n        ... )\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the ECDF overlay diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_full: pd.Series,\n        df_selection: pd.Series,\n        full_label: str = 'Full',\n        selection_label: str = 'Selection',\n    ) -&gt; go.Figure:\n        \"\"\"Create an ECDF overlay plot.\n\n        Args:\n            df_full: Series containing values for the full dataset.\n            df_selection: Series containing values for the selection.\n            full_label: Label for the full dataset in the legend. Default 'Full'.\n            selection_label: Label for the selection in the legend. Default 'Selection'.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n        \"\"\"\n        # Drop NaN values\n        full_values = df_full.dropna().values\n        selection_values = df_selection.dropna().values\n\n        # Calculate ECDF for full dataset\n        full_sorted = np.sort(full_values)\n        full_ecdf = np.arange(1, len(full_sorted) + 1) / len(full_sorted)\n\n        # Calculate ECDF for selection\n        selection_sorted = np.sort(selection_values)\n        selection_ecdf = np.arange(1, len(selection_sorted) + 1) / len(selection_sorted)\n\n        # Create figure\n        fig = go.Figure()\n\n        # Add full dataset ECDF\n        fig.add_trace(go.Scatter(\n            x=full_sorted,\n            y=full_ecdf,\n            mode='lines',\n            name=full_label,\n            line=dict(width=2),\n        ))\n\n        # Add selection ECDF\n        fig.add_trace(go.Scatter(\n            x=selection_sorted,\n            y=selection_ecdf,\n            mode='lines',\n            name=selection_label,\n            line=dict(width=2, dash='dash'),\n        ))\n\n        # Update layout\n        fig.update_layout(\n            xaxis_title=df_full.name or 'Value',\n            yaxis_title='Cumulative Probability',\n            hovermode='x unified',\n            yaxis=dict(tickformat='.0%', range=[0, 1]),\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayECDF.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the ECDF overlay diagnostic.</p> Source code in <code>energy_repset/diagnostics/score_components/distribution_overlay.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the ECDF overlay diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayECDF.plot","title":"plot","text":"<pre><code>plot(df_full: Series, df_selection: Series, full_label: str = 'Full', selection_label: str = 'Selection') -&gt; Figure\n</code></pre> <p>Create an ECDF overlay plot.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>Series</code> <p>Series containing values for the full dataset.</p> required <code>df_selection</code> <code>Series</code> <p>Series containing values for the selection.</p> required <code>full_label</code> <code>str</code> <p>Label for the full dataset in the legend. Default 'Full'.</p> <code>'Full'</code> <code>selection_label</code> <code>str</code> <p>Label for the selection in the legend. Default 'Selection'.</p> <code>'Selection'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> Source code in <code>energy_repset/diagnostics/score_components/distribution_overlay.py</code> <pre><code>def plot(\n    self,\n    df_full: pd.Series,\n    df_selection: pd.Series,\n    full_label: str = 'Full',\n    selection_label: str = 'Selection',\n) -&gt; go.Figure:\n    \"\"\"Create an ECDF overlay plot.\n\n    Args:\n        df_full: Series containing values for the full dataset.\n        df_selection: Series containing values for the selection.\n        full_label: Label for the full dataset in the legend. Default 'Full'.\n        selection_label: Label for the selection in the legend. Default 'Selection'.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n    \"\"\"\n    # Drop NaN values\n    full_values = df_full.dropna().values\n    selection_values = df_selection.dropna().values\n\n    # Calculate ECDF for full dataset\n    full_sorted = np.sort(full_values)\n    full_ecdf = np.arange(1, len(full_sorted) + 1) / len(full_sorted)\n\n    # Calculate ECDF for selection\n    selection_sorted = np.sort(selection_values)\n    selection_ecdf = np.arange(1, len(selection_sorted) + 1) / len(selection_sorted)\n\n    # Create figure\n    fig = go.Figure()\n\n    # Add full dataset ECDF\n    fig.add_trace(go.Scatter(\n        x=full_sorted,\n        y=full_ecdf,\n        mode='lines',\n        name=full_label,\n        line=dict(width=2),\n    ))\n\n    # Add selection ECDF\n    fig.add_trace(go.Scatter(\n        x=selection_sorted,\n        y=selection_ecdf,\n        mode='lines',\n        name=selection_label,\n        line=dict(width=2, dash='dash'),\n    ))\n\n    # Update layout\n    fig.update_layout(\n        xaxis_title=df_full.name or 'Value',\n        yaxis_title='Cumulative Probability',\n        hovermode='x unified',\n        yaxis=dict(tickformat='.0%', range=[0, 1]),\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayHistogram","title":"DistributionOverlayHistogram","text":"<p>Overlay histograms to compare distributions.</p> <p>Creates a plot showing normalized histograms of a variable for both the full dataset and a selection. Alternative to ECDF that may be more intuitive for some users. Shows probability density rather than cumulative probability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare demand distribution\n&gt;&gt;&gt; hist_plot = DistributionOverlayHistogram()\n&gt;&gt;&gt; full_data = context.df_raw['demand']\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, 'demand']\n&gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Demand Distribution: Full vs Selected')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # With custom bin count\n&gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data, nbins=50)\n\n&gt;&gt;&gt; # Using density mode\n&gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data, histnorm='probability density')\n</code></pre> Source code in <code>energy_repset/diagnostics/score_components/distribution_overlay.py</code> <pre><code>class DistributionOverlayHistogram:\n    \"\"\"Overlay histograms to compare distributions.\n\n    Creates a plot showing normalized histograms of a variable for both the\n    full dataset and a selection. Alternative to ECDF that may be more intuitive\n    for some users. Shows probability density rather than cumulative probability.\n\n    Examples:\n\n        &gt;&gt;&gt; # Compare demand distribution\n        &gt;&gt;&gt; hist_plot = DistributionOverlayHistogram()\n        &gt;&gt;&gt; full_data = context.df_raw['demand']\n        &gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n        &gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, 'demand']\n        &gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data)\n        &gt;&gt;&gt; fig.update_layout(title='Demand Distribution: Full vs Selected')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # With custom bin count\n        &gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data, nbins=50)\n\n        &gt;&gt;&gt; # Using density mode\n        &gt;&gt;&gt; fig = hist_plot.plot(full_data, selected_data, histnorm='probability density')\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the histogram overlay diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_full: pd.Series,\n        df_selection: pd.Series,\n        nbins: int = 30,\n        histnorm: str = 'probability',\n        full_label: str = 'Full',\n        selection_label: str = 'Selection',\n    ) -&gt; go.Figure:\n        \"\"\"Create a histogram overlay plot.\n\n        Args:\n            df_full: Series containing values for the full dataset.\n            df_selection: Series containing values for the selection.\n            nbins: Number of bins for the histogram. Default is 30.\n            histnorm: Histogram normalization mode. Options: 'probability',\n                'probability density', 'percent'. Default is 'probability'.\n            full_label: Label for the full dataset in the legend. Default 'Full'.\n            selection_label: Label for the selection in the legend. Default 'Selection'.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            ValueError: If histnorm is not a valid option.\n        \"\"\"\n        valid_histnorms = ['probability', 'probability density', 'percent', '']\n        if histnorm not in valid_histnorms:\n            raise ValueError(\n                f\"histnorm must be one of {valid_histnorms}, got '{histnorm}'\"\n            )\n\n        # Drop NaN values\n        full_values = df_full.dropna().values\n        selection_values = df_selection.dropna().values\n\n        # Create figure\n        fig = go.Figure()\n\n        # Add full dataset histogram\n        fig.add_trace(go.Histogram(\n            x=full_values,\n            name=full_label,\n            nbinsx=nbins,\n            histnorm=histnorm,\n            opacity=0.6,\n        ))\n\n        # Add selection histogram\n        fig.add_trace(go.Histogram(\n            x=selection_values,\n            name=selection_label,\n            nbinsx=nbins,\n            histnorm=histnorm,\n            opacity=0.6,\n        ))\n\n        # Update layout\n        yaxis_title = {\n            'probability': 'Probability',\n            'probability density': 'Probability Density',\n            'percent': 'Percent',\n            '': 'Count',\n        }.get(histnorm, 'Frequency')\n\n        fig.update_layout(\n            xaxis_title=df_full.name or 'Value',\n            yaxis_title=yaxis_title,\n            barmode='overlay',\n            hovermode='x unified',\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayHistogram.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the histogram overlay diagnostic.</p> Source code in <code>energy_repset/diagnostics/score_components/distribution_overlay.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the histogram overlay diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DistributionOverlayHistogram.plot","title":"plot","text":"<pre><code>plot(df_full: Series, df_selection: Series, nbins: int = 30, histnorm: str = 'probability', full_label: str = 'Full', selection_label: str = 'Selection') -&gt; Figure\n</code></pre> <p>Create a histogram overlay plot.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>Series</code> <p>Series containing values for the full dataset.</p> required <code>df_selection</code> <code>Series</code> <p>Series containing values for the selection.</p> required <code>nbins</code> <code>int</code> <p>Number of bins for the histogram. Default is 30.</p> <code>30</code> <code>histnorm</code> <code>str</code> <p>Histogram normalization mode. Options: 'probability', 'probability density', 'percent'. Default is 'probability'.</p> <code>'probability'</code> <code>full_label</code> <code>str</code> <p>Label for the full dataset in the legend. Default 'Full'.</p> <code>'Full'</code> <code>selection_label</code> <code>str</code> <p>Label for the selection in the legend. Default 'Selection'.</p> <code>'Selection'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If histnorm is not a valid option.</p> Source code in <code>energy_repset/diagnostics/score_components/distribution_overlay.py</code> <pre><code>def plot(\n    self,\n    df_full: pd.Series,\n    df_selection: pd.Series,\n    nbins: int = 30,\n    histnorm: str = 'probability',\n    full_label: str = 'Full',\n    selection_label: str = 'Selection',\n) -&gt; go.Figure:\n    \"\"\"Create a histogram overlay plot.\n\n    Args:\n        df_full: Series containing values for the full dataset.\n        df_selection: Series containing values for the selection.\n        nbins: Number of bins for the histogram. Default is 30.\n        histnorm: Histogram normalization mode. Options: 'probability',\n            'probability density', 'percent'. Default is 'probability'.\n        full_label: Label for the full dataset in the legend. Default 'Full'.\n        selection_label: Label for the selection in the legend. Default 'Selection'.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        ValueError: If histnorm is not a valid option.\n    \"\"\"\n    valid_histnorms = ['probability', 'probability density', 'percent', '']\n    if histnorm not in valid_histnorms:\n        raise ValueError(\n            f\"histnorm must be one of {valid_histnorms}, got '{histnorm}'\"\n        )\n\n    # Drop NaN values\n    full_values = df_full.dropna().values\n    selection_values = df_selection.dropna().values\n\n    # Create figure\n    fig = go.Figure()\n\n    # Add full dataset histogram\n    fig.add_trace(go.Histogram(\n        x=full_values,\n        name=full_label,\n        nbinsx=nbins,\n        histnorm=histnorm,\n        opacity=0.6,\n    ))\n\n    # Add selection histogram\n    fig.add_trace(go.Histogram(\n        x=selection_values,\n        name=selection_label,\n        nbinsx=nbins,\n        histnorm=histnorm,\n        opacity=0.6,\n    ))\n\n    # Update layout\n    yaxis_title = {\n        'probability': 'Probability',\n        'probability density': 'Probability Density',\n        'percent': 'Percent',\n        '': 'Count',\n    }.get(histnorm, 'Frequency')\n\n    fig.update_layout(\n        xaxis_title=df_full.name or 'Value',\n        yaxis_title=yaxis_title,\n        barmode='overlay',\n        hovermode='x unified',\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.CorrelationDifferenceHeatmap","title":"CorrelationDifferenceHeatmap","text":"<p>Visualize the difference between correlation matrices.</p> <p>Creates a heatmap showing the difference between the correlation matrix of the full dataset and the selection. This helps identify which variable relationships are well-preserved or poorly-preserved by the selection. Related to CorrelationFidelity score component.</p> <p>Positive values (red) indicate the selection has stronger correlation than the full dataset. Negative values (blue) indicate weaker correlation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare correlation structure\n&gt;&gt;&gt; corr_diff = CorrelationDifferenceHeatmap()\n&gt;&gt;&gt; full_data = context.df_raw[['demand', 'wind', 'solar']]\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, ['demand', 'wind', 'solar']]\n&gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Correlation Difference: Selection - Full')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # With Spearman correlation\n&gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data, method='spearman')\n\n&gt;&gt;&gt; # Show only lower triangle\n&gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data, show_lower_only=True)\n</code></pre> Source code in <code>energy_repset/diagnostics/score_components/correlation_difference_heatmap.py</code> <pre><code>class CorrelationDifferenceHeatmap:\n    \"\"\"Visualize the difference between correlation matrices.\n\n    Creates a heatmap showing the difference between the correlation matrix of\n    the full dataset and the selection. This helps identify which variable\n    relationships are well-preserved or poorly-preserved by the selection.\n    Related to CorrelationFidelity score component.\n\n    Positive values (red) indicate the selection has stronger correlation than\n    the full dataset. Negative values (blue) indicate weaker correlation.\n\n    Examples:\n\n        &gt;&gt;&gt; # Compare correlation structure\n        &gt;&gt;&gt; corr_diff = CorrelationDifferenceHeatmap()\n        &gt;&gt;&gt; full_data = context.df_raw[['demand', 'wind', 'solar']]\n        &gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n        &gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, ['demand', 'wind', 'solar']]\n        &gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data)\n        &gt;&gt;&gt; fig.update_layout(title='Correlation Difference: Selection - Full')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # With Spearman correlation\n        &gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data, method='spearman')\n\n        &gt;&gt;&gt; # Show only lower triangle\n        &gt;&gt;&gt; fig = corr_diff.plot(full_data, selected_data, show_lower_only=True)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the correlation difference heatmap diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_full: pd.DataFrame,\n        df_selection: pd.DataFrame,\n        method: str = 'pearson',\n        show_lower_only: bool = False,\n    ) -&gt; go.Figure:\n        \"\"\"Create a heatmap of correlation differences.\n\n        Args:\n            df_full: DataFrame containing variables for the full dataset.\n            df_selection: DataFrame containing variables for the selection.\n                Must have the same columns as df_full.\n            method: Correlation method ('pearson', 'spearman', or 'kendall').\n                Default is 'pearson'.\n            show_lower_only: If True, shows only the lower triangle of the\n                difference matrix (removes redundant upper triangle and diagonal).\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            ValueError: If method is invalid or columns don't match.\n        \"\"\"\n        if method not in ['pearson', 'spearman', 'kendall']:\n            raise ValueError(\n                f\"method must be 'pearson', 'spearman', or 'kendall', got '{method}'\"\n            )\n\n        if not df_full.columns.equals(df_selection.columns):\n            raise ValueError(\n                \"df_full and df_selection must have the same columns\"\n            )\n\n        # Calculate correlation matrices\n        corr_full = df_full.corr(method=method)\n        corr_selection = df_selection.corr(method=method)\n\n        # Calculate difference (selection - full)\n        corr_diff = corr_selection - corr_full\n\n        # Mask upper triangle if requested\n        if show_lower_only:\n            mask = pd.DataFrame(\n                False,\n                index=corr_diff.index,\n                columns=corr_diff.columns\n            )\n            # Set upper triangle and diagonal to True (to be masked)\n            for i in range(len(corr_diff)):\n                for j in range(i, len(corr_diff)):\n                    mask.iloc[i, j] = True\n\n            # Apply mask by setting values to NaN\n            corr_diff = corr_diff.where(~mask)\n\n        # Determine color scale range (symmetric around 0)\n        max_abs = max(abs(corr_diff.min().min()), abs(corr_diff.max().max()))\n        if pd.isna(max_abs):\n            max_abs = 1.0\n\n        # Create heatmap\n        fig = px.imshow(\n            corr_diff,\n            x=corr_diff.columns,\n            y=corr_diff.index,\n            color_continuous_scale='RdBu_r',\n            color_continuous_midpoint=0,\n            zmin=-max_abs,\n            zmax=max_abs,\n            aspect='auto',\n        )\n\n        # Update layout for better readability\n        fig.update_layout(\n            xaxis_title='',\n            yaxis_title='',\n            coloraxis_colorbar=dict(title='\u0394 Correlation&lt;br&gt;(Selection - Full)'),\n        )\n\n        # Improve text readability\n        fig.update_traces(\n            text=corr_diff.round(2).values,\n            texttemplate='%{text}',\n            textfont=dict(size=10),\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.CorrelationDifferenceHeatmap.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the correlation difference heatmap diagnostic.</p> Source code in <code>energy_repset/diagnostics/score_components/correlation_difference_heatmap.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the correlation difference heatmap diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.CorrelationDifferenceHeatmap.plot","title":"plot","text":"<pre><code>plot(df_full: DataFrame, df_selection: DataFrame, method: str = 'pearson', show_lower_only: bool = False) -&gt; Figure\n</code></pre> <p>Create a heatmap of correlation differences.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>DataFrame</code> <p>DataFrame containing variables for the full dataset.</p> required <code>df_selection</code> <code>DataFrame</code> <p>DataFrame containing variables for the selection. Must have the same columns as df_full.</p> required <code>method</code> <code>str</code> <p>Correlation method ('pearson', 'spearman', or 'kendall'). Default is 'pearson'.</p> <code>'pearson'</code> <code>show_lower_only</code> <code>bool</code> <p>If True, shows only the lower triangle of the difference matrix (removes redundant upper triangle and diagonal).</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is invalid or columns don't match.</p> Source code in <code>energy_repset/diagnostics/score_components/correlation_difference_heatmap.py</code> <pre><code>def plot(\n    self,\n    df_full: pd.DataFrame,\n    df_selection: pd.DataFrame,\n    method: str = 'pearson',\n    show_lower_only: bool = False,\n) -&gt; go.Figure:\n    \"\"\"Create a heatmap of correlation differences.\n\n    Args:\n        df_full: DataFrame containing variables for the full dataset.\n        df_selection: DataFrame containing variables for the selection.\n            Must have the same columns as df_full.\n        method: Correlation method ('pearson', 'spearman', or 'kendall').\n            Default is 'pearson'.\n        show_lower_only: If True, shows only the lower triangle of the\n            difference matrix (removes redundant upper triangle and diagonal).\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        ValueError: If method is invalid or columns don't match.\n    \"\"\"\n    if method not in ['pearson', 'spearman', 'kendall']:\n        raise ValueError(\n            f\"method must be 'pearson', 'spearman', or 'kendall', got '{method}'\"\n        )\n\n    if not df_full.columns.equals(df_selection.columns):\n        raise ValueError(\n            \"df_full and df_selection must have the same columns\"\n        )\n\n    # Calculate correlation matrices\n    corr_full = df_full.corr(method=method)\n    corr_selection = df_selection.corr(method=method)\n\n    # Calculate difference (selection - full)\n    corr_diff = corr_selection - corr_full\n\n    # Mask upper triangle if requested\n    if show_lower_only:\n        mask = pd.DataFrame(\n            False,\n            index=corr_diff.index,\n            columns=corr_diff.columns\n        )\n        # Set upper triangle and diagonal to True (to be masked)\n        for i in range(len(corr_diff)):\n            for j in range(i, len(corr_diff)):\n                mask.iloc[i, j] = True\n\n        # Apply mask by setting values to NaN\n        corr_diff = corr_diff.where(~mask)\n\n    # Determine color scale range (symmetric around 0)\n    max_abs = max(abs(corr_diff.min().min()), abs(corr_diff.max().max()))\n    if pd.isna(max_abs):\n        max_abs = 1.0\n\n    # Create heatmap\n    fig = px.imshow(\n        corr_diff,\n        x=corr_diff.columns,\n        y=corr_diff.index,\n        color_continuous_scale='RdBu_r',\n        color_continuous_midpoint=0,\n        zmin=-max_abs,\n        zmax=max_abs,\n        aspect='auto',\n    )\n\n    # Update layout for better readability\n    fig.update_layout(\n        xaxis_title='',\n        yaxis_title='',\n        coloraxis_colorbar=dict(title='\u0394 Correlation&lt;br&gt;(Selection - Full)'),\n    )\n\n    # Improve text readability\n    fig.update_traces(\n        text=corr_diff.round(2).values,\n        texttemplate='%{text}',\n        textfont=dict(size=10),\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DiurnalProfileOverlay","title":"DiurnalProfileOverlay","text":"<p>Overlay mean diurnal (hour-of-day) profiles for full vs selected data.</p> <p>Creates a plot showing the average value by hour of day for each variable, comparing the full dataset to the selection. This helps visualize how well the selection preserves daily patterns, which is related to DiurnalFidelity score component.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compare diurnal patterns\n&gt;&gt;&gt; diurnal_plot = DiurnalProfileOverlay()\n&gt;&gt;&gt; full_data = context.df_raw[['demand', 'wind', 'solar']]\n&gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n&gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, ['demand', 'wind', 'solar']]\n&gt;&gt;&gt; fig = diurnal_plot.plot(full_data, selected_data)\n&gt;&gt;&gt; fig.update_layout(title='Diurnal Profiles: Full vs Selected')\n&gt;&gt;&gt; fig.show()\n\n&gt;&gt;&gt; # Single variable\n&gt;&gt;&gt; fig = diurnal_plot.plot(\n...     full_data[['demand']],\n...     selected_data[['demand']]\n... )\n\n&gt;&gt;&gt; # Subset of variables\n&gt;&gt;&gt; fig = diurnal_plot.plot(\n...     full_data,\n...     selected_data,\n...     variables=['demand', 'wind']\n... )\n</code></pre> Source code in <code>energy_repset/diagnostics/score_components/diurnal_profile_overlay.py</code> <pre><code>class DiurnalProfileOverlay:\n    \"\"\"Overlay mean diurnal (hour-of-day) profiles for full vs selected data.\n\n    Creates a plot showing the average value by hour of day for each variable,\n    comparing the full dataset to the selection. This helps visualize how well\n    the selection preserves daily patterns, which is related to DiurnalFidelity\n    score component.\n\n    Examples:\n\n        &gt;&gt;&gt; # Compare diurnal patterns\n        &gt;&gt;&gt; diurnal_plot = DiurnalProfileOverlay()\n        &gt;&gt;&gt; full_data = context.df_raw[['demand', 'wind', 'solar']]\n        &gt;&gt;&gt; selected_indices = context.slicer.get_indices_for_slices(result.selection)\n        &gt;&gt;&gt; selected_data = context.df_raw.loc[selected_indices, ['demand', 'wind', 'solar']]\n        &gt;&gt;&gt; fig = diurnal_plot.plot(full_data, selected_data)\n        &gt;&gt;&gt; fig.update_layout(title='Diurnal Profiles: Full vs Selected')\n        &gt;&gt;&gt; fig.show()\n\n        &gt;&gt;&gt; # Single variable\n        &gt;&gt;&gt; fig = diurnal_plot.plot(\n        ...     full_data[['demand']],\n        ...     selected_data[['demand']]\n        ... )\n\n        &gt;&gt;&gt; # Subset of variables\n        &gt;&gt;&gt; fig = diurnal_plot.plot(\n        ...     full_data,\n        ...     selected_data,\n        ...     variables=['demand', 'wind']\n        ... )\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the diurnal profile overlay diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        df_full: pd.DataFrame,\n        df_selection: pd.DataFrame,\n        variables: list[str] = None,\n        full_label: str = 'Full',\n        selection_label: str = 'Selection',\n    ) -&gt; go.Figure:\n        \"\"\"Create a diurnal profile overlay plot.\n\n        Args:\n            df_full: DataFrame with DatetimeIndex and variable columns for full dataset.\n            df_selection: DataFrame with DatetimeIndex and variable columns for selection.\n                Must have the same columns as df_full.\n            variables: List of variable names to include. If None, uses all columns.\n            full_label: Label suffix for full dataset traces. Default 'Full'.\n            selection_label: Label suffix for selection traces. Default 'Selection'.\n\n        Returns:\n            Plotly figure object ready for display or further customization.\n\n        Raises:\n            ValueError: If DataFrames don't have DatetimeIndex or columns don't match.\n        \"\"\"\n        if not isinstance(df_full.index, pd.DatetimeIndex):\n            raise ValueError(\"df_full must have a DatetimeIndex\")\n        if not isinstance(df_selection.index, pd.DatetimeIndex):\n            raise ValueError(\"df_selection must have a DatetimeIndex\")\n        if not df_full.columns.equals(df_selection.columns):\n            raise ValueError(\"df_full and df_selection must have the same columns\")\n\n        # Determine which variables to plot\n        if variables is None:\n            variables = list(df_full.columns)\n        else:\n            # Validate requested variables\n            missing = set(variables) - set(df_full.columns)\n            if missing:\n                raise ValueError(f\"Variables not found in DataFrames: {missing}\")\n\n        # Extract hour from index\n        df_full_with_hour = df_full[variables].copy()\n        df_full_with_hour['hour'] = df_full.index.hour\n\n        df_selection_with_hour = df_selection[variables].copy()\n        df_selection_with_hour['hour'] = df_selection.index.hour\n\n        # Calculate mean profiles\n        full_profile = df_full_with_hour.groupby('hour').mean(numeric_only=True)\n        selection_profile = df_selection_with_hour.groupby('hour').mean(numeric_only=True)\n\n        # Create figure\n        fig = go.Figure()\n\n        # Add traces for each variable\n        for variable in variables:\n            # Full dataset trace\n            fig.add_trace(go.Scatter(\n                x=full_profile.index,\n                y=full_profile[variable],\n                mode='lines+markers',\n                name=f'{variable} ({full_label})',\n                line=dict(width=2),\n                marker=dict(size=6),\n            ))\n\n            # Selection trace\n            fig.add_trace(go.Scatter(\n                x=selection_profile.index,\n                y=selection_profile[variable],\n                mode='lines+markers',\n                name=f'{variable} ({selection_label})',\n                line=dict(width=2, dash='dash'),\n                marker=dict(size=6, symbol='diamond'),\n            ))\n\n        # Update layout\n        fig.update_layout(\n            xaxis_title='Hour of Day',\n            yaxis_title='Mean Value',\n            hovermode='x unified',\n            xaxis=dict(\n                tickmode='linear',\n                tick0=0,\n                dtick=2,\n                range=[-0.5, 23.5],\n            ),\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DiurnalProfileOverlay.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the diurnal profile overlay diagnostic.</p> Source code in <code>energy_repset/diagnostics/score_components/diurnal_profile_overlay.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the diurnal profile overlay diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.score_components.DiurnalProfileOverlay.plot","title":"plot","text":"<pre><code>plot(df_full: DataFrame, df_selection: DataFrame, variables: list[str] = None, full_label: str = 'Full', selection_label: str = 'Selection') -&gt; Figure\n</code></pre> <p>Create a diurnal profile overlay plot.</p> <p>Parameters:</p> Name Type Description Default <code>df_full</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex and variable columns for full dataset.</p> required <code>df_selection</code> <code>DataFrame</code> <p>DataFrame with DatetimeIndex and variable columns for selection. Must have the same columns as df_full.</p> required <code>variables</code> <code>list[str]</code> <p>List of variable names to include. If None, uses all columns.</p> <code>None</code> <code>full_label</code> <code>str</code> <p>Label suffix for full dataset traces. Default 'Full'.</p> <code>'Full'</code> <code>selection_label</code> <code>str</code> <p>Label suffix for selection traces. Default 'Selection'.</p> <code>'Selection'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object ready for display or further customization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DataFrames don't have DatetimeIndex or columns don't match.</p> Source code in <code>energy_repset/diagnostics/score_components/diurnal_profile_overlay.py</code> <pre><code>def plot(\n    self,\n    df_full: pd.DataFrame,\n    df_selection: pd.DataFrame,\n    variables: list[str] = None,\n    full_label: str = 'Full',\n    selection_label: str = 'Selection',\n) -&gt; go.Figure:\n    \"\"\"Create a diurnal profile overlay plot.\n\n    Args:\n        df_full: DataFrame with DatetimeIndex and variable columns for full dataset.\n        df_selection: DataFrame with DatetimeIndex and variable columns for selection.\n            Must have the same columns as df_full.\n        variables: List of variable names to include. If None, uses all columns.\n        full_label: Label suffix for full dataset traces. Default 'Full'.\n        selection_label: Label suffix for selection traces. Default 'Selection'.\n\n    Returns:\n        Plotly figure object ready for display or further customization.\n\n    Raises:\n        ValueError: If DataFrames don't have DatetimeIndex or columns don't match.\n    \"\"\"\n    if not isinstance(df_full.index, pd.DatetimeIndex):\n        raise ValueError(\"df_full must have a DatetimeIndex\")\n    if not isinstance(df_selection.index, pd.DatetimeIndex):\n        raise ValueError(\"df_selection must have a DatetimeIndex\")\n    if not df_full.columns.equals(df_selection.columns):\n        raise ValueError(\"df_full and df_selection must have the same columns\")\n\n    # Determine which variables to plot\n    if variables is None:\n        variables = list(df_full.columns)\n    else:\n        # Validate requested variables\n        missing = set(variables) - set(df_full.columns)\n        if missing:\n            raise ValueError(f\"Variables not found in DataFrames: {missing}\")\n\n    # Extract hour from index\n    df_full_with_hour = df_full[variables].copy()\n    df_full_with_hour['hour'] = df_full.index.hour\n\n    df_selection_with_hour = df_selection[variables].copy()\n    df_selection_with_hour['hour'] = df_selection.index.hour\n\n    # Calculate mean profiles\n    full_profile = df_full_with_hour.groupby('hour').mean(numeric_only=True)\n    selection_profile = df_selection_with_hour.groupby('hour').mean(numeric_only=True)\n\n    # Create figure\n    fig = go.Figure()\n\n    # Add traces for each variable\n    for variable in variables:\n        # Full dataset trace\n        fig.add_trace(go.Scatter(\n            x=full_profile.index,\n            y=full_profile[variable],\n            mode='lines+markers',\n            name=f'{variable} ({full_label})',\n            line=dict(width=2),\n            marker=dict(size=6),\n        ))\n\n        # Selection trace\n        fig.add_trace(go.Scatter(\n            x=selection_profile.index,\n            y=selection_profile[variable],\n            mode='lines+markers',\n            name=f'{variable} ({selection_label})',\n            line=dict(width=2, dash='dash'),\n            marker=dict(size=6, symbol='diamond'),\n        ))\n\n    # Update layout\n    fig.update_layout(\n        xaxis_title='Hour of Day',\n        yaxis_title='Mean Value',\n        hovermode='x unified',\n        xaxis=dict(\n            tickmode='linear',\n            tick0=0,\n            dtick=2,\n            range=[-0.5, 23.5],\n        ),\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#results","title":"Results","text":""},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ResponsibilityBars","title":"ResponsibilityBars","text":"<p>Bar chart showing responsibility weights for selected representatives.</p> <p>Visualizes the weight distribution across selected periods as computed by a RepresentationModel. Each bar shows how much each representative contributes to the full dataset representation.</p> <p>Optionally displays a reference line showing uniform weights (1/k) for comparison with non-uniform weighting schemes like cluster-size based weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ResponsibilityBars\n&gt;&gt;&gt;\n&gt;&gt;&gt; # After running workflow with result containing weights\n&gt;&gt;&gt; weights = result.weights  # e.g., {Period('2024-01'): 0.35, ...}\n&gt;&gt;&gt; bars = ResponsibilityBars()\n&gt;&gt;&gt; fig = bars.plot(weights, show_uniform_reference=True)\n&gt;&gt;&gt; fig.update_layout(title='Responsibility Weights')\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>energy_repset/diagnostics/results/responsibility_bars.py</code> <pre><code>class ResponsibilityBars:\n    \"\"\"Bar chart showing responsibility weights for selected representatives.\n\n    Visualizes the weight distribution across selected periods as computed by\n    a RepresentationModel. Each bar shows how much each representative\n    contributes to the full dataset representation.\n\n    Optionally displays a reference line showing uniform weights (1/k) for\n    comparison with non-uniform weighting schemes like cluster-size based\n    weights.\n\n    Examples:\n\n        &gt;&gt;&gt; from energy_repset.diagnostics.results import ResponsibilityBars\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # After running workflow with result containing weights\n        &gt;&gt;&gt; weights = result.weights  # e.g., {Period('2024-01'): 0.35, ...}\n        &gt;&gt;&gt; bars = ResponsibilityBars()\n        &gt;&gt;&gt; fig = bars.plot(weights, show_uniform_reference=True)\n        &gt;&gt;&gt; fig.update_layout(title='Responsibility Weights')\n        &gt;&gt;&gt; fig.show()\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize ResponsibilityBars diagnostic.\"\"\"\n        pass\n\n    def plot(\n        self,\n        weights: Dict[Hashable, float],\n        show_uniform_reference: bool = True,\n    ) -&gt; go.Figure:\n        \"\"\"Create bar chart of responsibility weights.\n\n        Args:\n            weights: Dictionary mapping slice identifiers to their weights.\n                Weights should sum to 1.0 for meaningful comparison with\n                the uniform reference line.\n            show_uniform_reference: If True, adds horizontal dashed line\n                showing uniform weight (1/k) for comparison.\n\n        Returns:\n            Plotly figure with bar chart. X-axis shows slice labels, Y-axis\n            shows weight values. Text labels show weights to 3 decimal places.\n\n        Raises:\n            ValueError: If weights dictionary is empty.\n        \"\"\"\n        if not weights:\n            raise ValueError(\"Weights dictionary cannot be empty\")\n\n        # Prepare data for plotting\n        df = pd.DataFrame({\n            'slice': [str(s) for s in weights.keys()],\n            'weight': list(weights.values())\n        })\n\n        # Create bar chart\n        fig = px.bar(\n            df,\n            x='slice',\n            y='weight',\n            text='weight'\n        )\n\n        # Format text labels to 3 decimal places, position outside bars\n        fig.update_traces(\n            texttemplate='%{y:.3f}',\n            textposition='outside'\n        )\n\n        # Set y-axis range and label\n        fig.update_yaxes(\n            range=[0, max(df['weight']) * 1.15],  # Add headroom for text labels\n            title='Responsibility Weight'\n        )\n\n        fig.update_xaxes(title='Selected Period')\n\n        # Add uniform reference line if requested\n        if show_uniform_reference and len(weights) &gt; 0:\n            uniform_weight = 1.0 / len(weights)\n            fig.add_hline(\n                y=uniform_weight,\n                line_dash='dot',\n                annotation_text=f'Uniform ({uniform_weight:.3f})',\n                annotation_position='top left'\n            )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ResponsibilityBars.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize ResponsibilityBars diagnostic.</p> Source code in <code>energy_repset/diagnostics/results/responsibility_bars.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize ResponsibilityBars diagnostic.\"\"\"\n    pass\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ResponsibilityBars.plot","title":"plot","text":"<pre><code>plot(weights: dict[Hashable, float], show_uniform_reference: bool = True) -&gt; Figure\n</code></pre> <p>Create bar chart of responsibility weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>dict[Hashable, float]</code> <p>Dictionary mapping slice identifiers to their weights. Weights should sum to 1.0 for meaningful comparison with the uniform reference line.</p> required <code>show_uniform_reference</code> <code>bool</code> <p>If True, adds horizontal dashed line showing uniform weight (1/k) for comparison.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with bar chart. X-axis shows slice labels, Y-axis</p> <code>Figure</code> <p>shows weight values. Text labels show weights to 3 decimal places.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If weights dictionary is empty.</p> Source code in <code>energy_repset/diagnostics/results/responsibility_bars.py</code> <pre><code>def plot(\n    self,\n    weights: Dict[Hashable, float],\n    show_uniform_reference: bool = True,\n) -&gt; go.Figure:\n    \"\"\"Create bar chart of responsibility weights.\n\n    Args:\n        weights: Dictionary mapping slice identifiers to their weights.\n            Weights should sum to 1.0 for meaningful comparison with\n            the uniform reference line.\n        show_uniform_reference: If True, adds horizontal dashed line\n            showing uniform weight (1/k) for comparison.\n\n    Returns:\n        Plotly figure with bar chart. X-axis shows slice labels, Y-axis\n        shows weight values. Text labels show weights to 3 decimal places.\n\n    Raises:\n        ValueError: If weights dictionary is empty.\n    \"\"\"\n    if not weights:\n        raise ValueError(\"Weights dictionary cannot be empty\")\n\n    # Prepare data for plotting\n    df = pd.DataFrame({\n        'slice': [str(s) for s in weights.keys()],\n        'weight': list(weights.values())\n    })\n\n    # Create bar chart\n    fig = px.bar(\n        df,\n        x='slice',\n        y='weight',\n        text='weight'\n    )\n\n    # Format text labels to 3 decimal places, position outside bars\n    fig.update_traces(\n        texttemplate='%{y:.3f}',\n        textposition='outside'\n    )\n\n    # Set y-axis range and label\n    fig.update_yaxes(\n        range=[0, max(df['weight']) * 1.15],  # Add headroom for text labels\n        title='Responsibility Weight'\n    )\n\n    fig.update_xaxes(title='Selected Period')\n\n    # Add uniform reference line if requested\n    if show_uniform_reference and len(weights) &gt; 0:\n        uniform_weight = 1.0 / len(weights)\n        fig.add_hline(\n            y=uniform_weight,\n            line_dash='dot',\n            annotation_text=f'Uniform ({uniform_weight:.3f})',\n            annotation_position='top left'\n        )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatter2D","title":"ParetoScatter2D","text":"<p>2D scatter plot of all evaluated combinations with Pareto front highlighted.</p> <p>Visualizes the objective space for two objectives, showing: - All evaluated combinations as scatter points - Pareto-optimal solutions highlighted - Selected combination (if provided) marked distinctly - Feasible vs infeasible solutions (if constraints exist)</p> <p>Parameters:</p> Name Type Description Default <code>objective_x</code> <code>str</code> <p>Name of objective for x-axis.</p> required <code>objective_y</code> <code>str</code> <p>Name of objective for y-axis.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoScatter2D\n&gt;&gt;&gt; scatter = ParetoScatter2D(objective_x='wasserstein', objective_y='correlation')\n&gt;&gt;&gt; fig = scatter.plot(\n...     search_algorithm=workflow.search_algorithm,\n...     selected_combination=result.selection\n... )\n&gt;&gt;&gt; fig.update_layout(title='Pareto Front: Wasserstein vs Correlation')\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>energy_repset/diagnostics/results/pareto_scatter.py</code> <pre><code>class ParetoScatter2D:\n    \"\"\"2D scatter plot of all evaluated combinations with Pareto front highlighted.\n\n    Visualizes the objective space for two objectives, showing:\n    - All evaluated combinations as scatter points\n    - Pareto-optimal solutions highlighted\n    - Selected combination (if provided) marked distinctly\n    - Feasible vs infeasible solutions (if constraints exist)\n\n    Args:\n        objective_x: Name of objective for x-axis.\n        objective_y: Name of objective for y-axis.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoScatter2D\n        &gt;&gt;&gt; scatter = ParetoScatter2D(objective_x='wasserstein', objective_y='correlation')\n        &gt;&gt;&gt; fig = scatter.plot(\n        ...     search_algorithm=workflow.search_algorithm,\n        ...     selected_combination=result.selection\n        ... )\n        &gt;&gt;&gt; fig.update_layout(title='Pareto Front: Wasserstein vs Correlation')\n        &gt;&gt;&gt; fig.show()\n    \"\"\"\n\n    def __init__(self, objective_x: str, objective_y: str):\n        \"\"\"Initialize Pareto scatter diagnostic.\n\n        Args:\n            objective_x: Name of objective for x-axis.\n            objective_y: Name of objective for y-axis.\n        \"\"\"\n        self.objective_x = objective_x\n        self.objective_y = objective_y\n\n    def plot(\n        self,\n        search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm,\n        selected_combination: SliceCombination | None = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create 2D scatter plot of Pareto front.\n\n        Args:\n            search_algorithm: Search algorithm after find_selection() has been called.\n            selected_combination: Optional combination to highlight (e.g., result.selection).\n\n        Returns:\n            Plotly figure with scatter plot.\n\n        Raises:\n            ValueError: If find_selection() hasn't been called or objectives not found.\n        \"\"\"\n        df = search_algorithm.get_all_scores()\n\n        if self.objective_x not in df.columns:\n            raise ValueError(f\"Objective '{self.objective_x}' not found in scores\")\n        if self.objective_y not in df.columns:\n            raise ValueError(f\"Objective '{self.objective_y}' not found in scores\")\n\n        has_pareto = hasattr(search_algorithm.selection_policy, 'pareto_mask')\n        pareto_mask = None\n        feasible_mask = None\n\n        if has_pareto and search_algorithm.selection_policy.pareto_mask is not None:\n            pareto_mask = search_algorithm.selection_policy.pareto_mask\n            feasible_mask = search_algorithm.selection_policy.feasible_mask\n\n        fig = go.Figure()\n\n        x_vals = df[self.objective_x]\n        y_vals = df[self.objective_y]\n\n        if has_pareto and pareto_mask is not None:\n            pareto = pareto_mask.values\n            feasible = feasible_mask.values\n\n            infeasible = ~feasible\n            if infeasible.any():\n                fig.add_trace(go.Scatter(\n                    x=x_vals[infeasible],\n                    y=y_vals[infeasible],\n                    mode='markers',\n                    marker=dict(size=6, opacity=0.3),\n                    name='Infeasible',\n                    hovertemplate=(\n                        f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                        f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                        '&lt;extra&gt;&lt;/extra&gt;'\n                    ),\n                ))\n\n            dominated = feasible &amp; ~pareto\n            if dominated.any():\n                fig.add_trace(go.Scatter(\n                    x=x_vals[dominated],\n                    y=y_vals[dominated],\n                    mode='markers',\n                    marker=dict(size=6, opacity=0.5),\n                    name='Dominated',\n                    hovertemplate=(\n                        f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                        f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                        '&lt;extra&gt;&lt;/extra&gt;'\n                    ),\n                ))\n\n            pareto_points = pareto &amp; feasible\n            if pareto_points.any():\n                fig.add_trace(go.Scatter(\n                    x=x_vals[pareto_points],\n                    y=y_vals[pareto_points],\n                    mode='markers',\n                    marker=dict(size=10, symbol='diamond'),\n                    name='Pareto Front',\n                    hovertemplate=(\n                        f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                        f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                        '&lt;extra&gt;&lt;/extra&gt;'\n                    ),\n                ))\n        else:\n            fig.add_trace(go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode='markers',\n                marker=dict(size=6, opacity=0.5),\n                name='All Combinations',\n                hovertemplate=(\n                    f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                    f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                    '&lt;extra&gt;&lt;/extra&gt;'\n                ),\n            ))\n\n        if selected_combination is not None:\n            selected_idx = df['slices'].apply(lambda x: x == selected_combination)\n            if selected_idx.any():\n                sel_x = x_vals[selected_idx].values[0]\n                sel_y = y_vals[selected_idx].values[0]\n                fig.add_trace(go.Scatter(\n                    x=[sel_x],\n                    y=[sel_y],\n                    mode='markers',\n                    marker=dict(\n                        size=15,\n                        symbol='star',\n                        line=dict(width=2, color='black')\n                    ),\n                    name='Selected',\n                    hovertemplate=(\n                        f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                        f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                        '&lt;b&gt;SELECTED&lt;/b&gt;&lt;br&gt;'\n                        '&lt;extra&gt;&lt;/extra&gt;'\n                    ),\n                ))\n\n        fig.update_layout(\n            xaxis_title=self.objective_x,\n            yaxis_title=self.objective_y,\n            hovermode='closest',\n            showlegend=True,\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatter2D.__init__","title":"__init__","text":"<pre><code>__init__(objective_x: str, objective_y: str)\n</code></pre> <p>Initialize Pareto scatter diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>objective_x</code> <code>str</code> <p>Name of objective for x-axis.</p> required <code>objective_y</code> <code>str</code> <p>Name of objective for y-axis.</p> required Source code in <code>energy_repset/diagnostics/results/pareto_scatter.py</code> <pre><code>def __init__(self, objective_x: str, objective_y: str):\n    \"\"\"Initialize Pareto scatter diagnostic.\n\n    Args:\n        objective_x: Name of objective for x-axis.\n        objective_y: Name of objective for y-axis.\n    \"\"\"\n    self.objective_x = objective_x\n    self.objective_y = objective_y\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatter2D.plot","title":"plot","text":"<pre><code>plot(search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm, selected_combination: SliceCombination | None = None) -&gt; Figure\n</code></pre> <p>Create 2D scatter plot of Pareto front.</p> <p>Parameters:</p> Name Type Description Default <code>search_algorithm</code> <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <p>Search algorithm after find_selection() has been called.</p> required <code>selected_combination</code> <code>SliceCombination | None</code> <p>Optional combination to highlight (e.g., result.selection).</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with scatter plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() hasn't been called or objectives not found.</p> Source code in <code>energy_repset/diagnostics/results/pareto_scatter.py</code> <pre><code>def plot(\n    self,\n    search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm,\n    selected_combination: SliceCombination | None = None,\n) -&gt; go.Figure:\n    \"\"\"Create 2D scatter plot of Pareto front.\n\n    Args:\n        search_algorithm: Search algorithm after find_selection() has been called.\n        selected_combination: Optional combination to highlight (e.g., result.selection).\n\n    Returns:\n        Plotly figure with scatter plot.\n\n    Raises:\n        ValueError: If find_selection() hasn't been called or objectives not found.\n    \"\"\"\n    df = search_algorithm.get_all_scores()\n\n    if self.objective_x not in df.columns:\n        raise ValueError(f\"Objective '{self.objective_x}' not found in scores\")\n    if self.objective_y not in df.columns:\n        raise ValueError(f\"Objective '{self.objective_y}' not found in scores\")\n\n    has_pareto = hasattr(search_algorithm.selection_policy, 'pareto_mask')\n    pareto_mask = None\n    feasible_mask = None\n\n    if has_pareto and search_algorithm.selection_policy.pareto_mask is not None:\n        pareto_mask = search_algorithm.selection_policy.pareto_mask\n        feasible_mask = search_algorithm.selection_policy.feasible_mask\n\n    fig = go.Figure()\n\n    x_vals = df[self.objective_x]\n    y_vals = df[self.objective_y]\n\n    if has_pareto and pareto_mask is not None:\n        pareto = pareto_mask.values\n        feasible = feasible_mask.values\n\n        infeasible = ~feasible\n        if infeasible.any():\n            fig.add_trace(go.Scatter(\n                x=x_vals[infeasible],\n                y=y_vals[infeasible],\n                mode='markers',\n                marker=dict(size=6, opacity=0.3),\n                name='Infeasible',\n                hovertemplate=(\n                    f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                    f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                    '&lt;extra&gt;&lt;/extra&gt;'\n                ),\n            ))\n\n        dominated = feasible &amp; ~pareto\n        if dominated.any():\n            fig.add_trace(go.Scatter(\n                x=x_vals[dominated],\n                y=y_vals[dominated],\n                mode='markers',\n                marker=dict(size=6, opacity=0.5),\n                name='Dominated',\n                hovertemplate=(\n                    f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                    f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                    '&lt;extra&gt;&lt;/extra&gt;'\n                ),\n            ))\n\n        pareto_points = pareto &amp; feasible\n        if pareto_points.any():\n            fig.add_trace(go.Scatter(\n                x=x_vals[pareto_points],\n                y=y_vals[pareto_points],\n                mode='markers',\n                marker=dict(size=10, symbol='diamond'),\n                name='Pareto Front',\n                hovertemplate=(\n                    f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                    f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                    '&lt;extra&gt;&lt;/extra&gt;'\n                ),\n            ))\n    else:\n        fig.add_trace(go.Scatter(\n            x=x_vals,\n            y=y_vals,\n            mode='markers',\n            marker=dict(size=6, opacity=0.5),\n            name='All Combinations',\n            hovertemplate=(\n                f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                '&lt;extra&gt;&lt;/extra&gt;'\n            ),\n        ))\n\n    if selected_combination is not None:\n        selected_idx = df['slices'].apply(lambda x: x == selected_combination)\n        if selected_idx.any():\n            sel_x = x_vals[selected_idx].values[0]\n            sel_y = y_vals[selected_idx].values[0]\n            fig.add_trace(go.Scatter(\n                x=[sel_x],\n                y=[sel_y],\n                mode='markers',\n                marker=dict(\n                    size=15,\n                    symbol='star',\n                    line=dict(width=2, color='black')\n                ),\n                name='Selected',\n                hovertemplate=(\n                    f'{self.objective_x}: %{{x:.4f}}&lt;br&gt;'\n                    f'{self.objective_y}: %{{y:.4f}}&lt;br&gt;'\n                    '&lt;b&gt;SELECTED&lt;/b&gt;&lt;br&gt;'\n                    '&lt;extra&gt;&lt;/extra&gt;'\n                ),\n            ))\n\n    fig.update_layout(\n        xaxis_title=self.objective_x,\n        yaxis_title=self.objective_y,\n        hovermode='closest',\n        showlegend=True,\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatterMatrix","title":"ParetoScatterMatrix","text":"<p>Scatter matrix of all objectives showing Pareto front.</p> <p>Creates a scatter plot matrix (SPLOM) showing pairwise relationships between all objectives. Each subplot shows two objectives with Pareto front highlighted.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all objectives).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoScatterMatrix\n&gt;&gt;&gt; scatter_matrix = ParetoScatterMatrix(\n...     objectives=['wasserstein', 'correlation', 'diurnal']\n... )\n&gt;&gt;&gt; fig = scatter_matrix.plot(\n...     search_algorithm=workflow.search_algorithm,\n...     selected_combination=result.selection\n... )\n&gt;&gt;&gt; fig.update_layout(title='Pareto Front: All Objectives')\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>energy_repset/diagnostics/results/pareto_scatter.py</code> <pre><code>class ParetoScatterMatrix:\n    \"\"\"Scatter matrix of all objectives showing Pareto front.\n\n    Creates a scatter plot matrix (SPLOM) showing pairwise relationships between\n    all objectives. Each subplot shows two objectives with Pareto front highlighted.\n\n    Args:\n        objectives: List of objective names to include (None = all objectives).\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoScatterMatrix\n        &gt;&gt;&gt; scatter_matrix = ParetoScatterMatrix(\n        ...     objectives=['wasserstein', 'correlation', 'diurnal']\n        ... )\n        &gt;&gt;&gt; fig = scatter_matrix.plot(\n        ...     search_algorithm=workflow.search_algorithm,\n        ...     selected_combination=result.selection\n        ... )\n        &gt;&gt;&gt; fig.update_layout(title='Pareto Front: All Objectives')\n        &gt;&gt;&gt; fig.show()\n    \"\"\"\n\n    def __init__(self, objectives: list[str] | None = None):\n        \"\"\"Initialize Pareto scatter matrix diagnostic.\n\n        Args:\n            objectives: List of objective names to include (None = all).\n        \"\"\"\n        self.objectives = objectives\n\n    def plot(\n        self,\n        search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm,\n        selected_combination: SliceCombination | None = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create scatter matrix of Pareto front.\n\n        Args:\n            search_algorithm: Search algorithm after find_selection() has been called.\n            selected_combination: Optional combination to highlight.\n\n        Returns:\n            Plotly figure with scatter matrix.\n\n        Raises:\n            ValueError: If find_selection() hasn't been called.\n        \"\"\"\n        df = search_algorithm.get_all_scores()\n\n        if self.objectives is None:\n            obj_cols = [col for col in df.columns if col not in ['slices', 'label']]\n        else:\n            obj_cols = self.objectives\n            for obj in obj_cols:\n                if obj not in df.columns:\n                    raise ValueError(f\"Objective '{obj}' not found in scores\")\n\n        if len(obj_cols) &lt; 2:\n            raise ValueError(\"Need at least 2 objectives for scatter matrix\")\n\n        has_pareto = hasattr(search_algorithm.selection_policy, 'pareto_mask')\n        pareto_mask = None\n        feasible_mask = None\n\n        if has_pareto and search_algorithm.selection_policy.pareto_mask is not None:\n            pareto_mask = search_algorithm.selection_policy.pareto_mask\n            feasible_mask = search_algorithm.selection_policy.feasible_mask\n\n        color_col = None\n        if has_pareto and pareto_mask is not None:\n            df_plot = df.copy()\n            pareto = pareto_mask.values\n            feasible = feasible_mask.values\n            df_plot['category'] = 'Dominated'\n            df_plot.loc[~feasible, 'category'] = 'Infeasible'\n            df_plot.loc[pareto &amp; feasible, 'category'] = 'Pareto Front'\n            color_col = 'category'\n        else:\n            df_plot = df.copy()\n\n        dimensions = []\n        for obj in obj_cols:\n            dimensions.append(dict(\n                label=obj,\n                values=df_plot[obj]\n            ))\n\n        fig = go.Figure(data=go.Splom(\n            dimensions=dimensions,\n            marker=dict(\n                size=5,\n                color=df_plot[color_col].map({\n                    'Infeasible': 0,\n                    'Dominated': 1,\n                    'Pareto Front': 2\n                }) if color_col else None,\n                colorscale=[[0, 'lightgray'], [0.5, 'steelblue'], [1, 'darkorange']] if color_col else None,\n                showscale=False,\n                line=dict(width=0.5, color='white')\n            ),\n            text=df_plot['label'] if 'label' in df_plot else None,\n            diagonal_visible=False,\n            showupperhalf=False,\n        ))\n\n        if selected_combination is not None:\n            selected_idx = df['slices'].apply(lambda x: x == selected_combination)\n            if selected_idx.any():\n                selected_vals = [df_plot.loc[selected_idx, obj].values[0] for obj in obj_cols]\n                n_dims = len(obj_cols)\n                for i in range(n_dims):\n                    for j in range(i):\n                        xaxis = f'x{j*n_dims + i + 1}' if (j*n_dims + i) &gt; 0 else 'x'\n                        yaxis = f'y{j*n_dims + i + 1}' if (j*n_dims + i) &gt; 0 else 'y'\n\n        fig.update_layout(\n            title='Scatter Matrix: All Objectives',\n            height=150 * len(obj_cols),\n            width=150 * len(obj_cols),\n            showlegend=False,\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatterMatrix.__init__","title":"__init__","text":"<pre><code>__init__(objectives: list[str] | None = None)\n</code></pre> <p>Initialize Pareto scatter matrix diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all).</p> <code>None</code> Source code in <code>energy_repset/diagnostics/results/pareto_scatter.py</code> <pre><code>def __init__(self, objectives: list[str] | None = None):\n    \"\"\"Initialize Pareto scatter matrix diagnostic.\n\n    Args:\n        objectives: List of objective names to include (None = all).\n    \"\"\"\n    self.objectives = objectives\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoScatterMatrix.plot","title":"plot","text":"<pre><code>plot(search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm, selected_combination: SliceCombination | None = None) -&gt; Figure\n</code></pre> <p>Create scatter matrix of Pareto front.</p> <p>Parameters:</p> Name Type Description Default <code>search_algorithm</code> <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <p>Search algorithm after find_selection() has been called.</p> required <code>selected_combination</code> <code>SliceCombination | None</code> <p>Optional combination to highlight.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with scatter matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() hasn't been called.</p> Source code in <code>energy_repset/diagnostics/results/pareto_scatter.py</code> <pre><code>def plot(\n    self,\n    search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm,\n    selected_combination: SliceCombination | None = None,\n) -&gt; go.Figure:\n    \"\"\"Create scatter matrix of Pareto front.\n\n    Args:\n        search_algorithm: Search algorithm after find_selection() has been called.\n        selected_combination: Optional combination to highlight.\n\n    Returns:\n        Plotly figure with scatter matrix.\n\n    Raises:\n        ValueError: If find_selection() hasn't been called.\n    \"\"\"\n    df = search_algorithm.get_all_scores()\n\n    if self.objectives is None:\n        obj_cols = [col for col in df.columns if col not in ['slices', 'label']]\n    else:\n        obj_cols = self.objectives\n        for obj in obj_cols:\n            if obj not in df.columns:\n                raise ValueError(f\"Objective '{obj}' not found in scores\")\n\n    if len(obj_cols) &lt; 2:\n        raise ValueError(\"Need at least 2 objectives for scatter matrix\")\n\n    has_pareto = hasattr(search_algorithm.selection_policy, 'pareto_mask')\n    pareto_mask = None\n    feasible_mask = None\n\n    if has_pareto and search_algorithm.selection_policy.pareto_mask is not None:\n        pareto_mask = search_algorithm.selection_policy.pareto_mask\n        feasible_mask = search_algorithm.selection_policy.feasible_mask\n\n    color_col = None\n    if has_pareto and pareto_mask is not None:\n        df_plot = df.copy()\n        pareto = pareto_mask.values\n        feasible = feasible_mask.values\n        df_plot['category'] = 'Dominated'\n        df_plot.loc[~feasible, 'category'] = 'Infeasible'\n        df_plot.loc[pareto &amp; feasible, 'category'] = 'Pareto Front'\n        color_col = 'category'\n    else:\n        df_plot = df.copy()\n\n    dimensions = []\n    for obj in obj_cols:\n        dimensions.append(dict(\n            label=obj,\n            values=df_plot[obj]\n        ))\n\n    fig = go.Figure(data=go.Splom(\n        dimensions=dimensions,\n        marker=dict(\n            size=5,\n            color=df_plot[color_col].map({\n                'Infeasible': 0,\n                'Dominated': 1,\n                'Pareto Front': 2\n            }) if color_col else None,\n            colorscale=[[0, 'lightgray'], [0.5, 'steelblue'], [1, 'darkorange']] if color_col else None,\n            showscale=False,\n            line=dict(width=0.5, color='white')\n        ),\n        text=df_plot['label'] if 'label' in df_plot else None,\n        diagonal_visible=False,\n        showupperhalf=False,\n    ))\n\n    if selected_combination is not None:\n        selected_idx = df['slices'].apply(lambda x: x == selected_combination)\n        if selected_idx.any():\n            selected_vals = [df_plot.loc[selected_idx, obj].values[0] for obj in obj_cols]\n            n_dims = len(obj_cols)\n            for i in range(n_dims):\n                for j in range(i):\n                    xaxis = f'x{j*n_dims + i + 1}' if (j*n_dims + i) &gt; 0 else 'x'\n                    yaxis = f'y{j*n_dims + i + 1}' if (j*n_dims + i) &gt; 0 else 'y'\n\n    fig.update_layout(\n        title='Scatter Matrix: All Objectives',\n        height=150 * len(obj_cols),\n        width=150 * len(obj_cols),\n        showlegend=False,\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoParallelCoordinates","title":"ParetoParallelCoordinates","text":"<p>Parallel coordinates plot of Pareto front.</p> <p>Visualizes multi-objective trade-offs using parallel coordinates where each vertical axis represents one objective. Lines connecting axes show individual solutions, with Pareto-optimal solutions highlighted.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all objectives).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoParallelCoordinates\n&gt;&gt;&gt; parallel = ParetoParallelCoordinates()\n&gt;&gt;&gt; fig = parallel.plot(search_algorithm=workflow.search_algorithm)\n&gt;&gt;&gt; fig.update_layout(title='Pareto Front: Parallel Coordinates')\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>energy_repset/diagnostics/results/pareto_parallel_coords.py</code> <pre><code>class ParetoParallelCoordinates:\n    \"\"\"Parallel coordinates plot of Pareto front.\n\n    Visualizes multi-objective trade-offs using parallel coordinates where each\n    vertical axis represents one objective. Lines connecting axes show individual\n    solutions, with Pareto-optimal solutions highlighted.\n\n    Args:\n        objectives: List of objective names to include (None = all objectives).\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.diagnostics.results import ParetoParallelCoordinates\n        &gt;&gt;&gt; parallel = ParetoParallelCoordinates()\n        &gt;&gt;&gt; fig = parallel.plot(search_algorithm=workflow.search_algorithm)\n        &gt;&gt;&gt; fig.update_layout(title='Pareto Front: Parallel Coordinates')\n        &gt;&gt;&gt; fig.show()\n    \"\"\"\n\n    def __init__(self, objectives: list[str] | None = None):\n        \"\"\"Initialize parallel coordinates diagnostic.\n\n        Args:\n            objectives: List of objective names to include (None = all).\n        \"\"\"\n        self.objectives = objectives\n\n    def plot(\n        self,\n        search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm,\n    ) -&gt; go.Figure:\n        \"\"\"Create parallel coordinates plot of Pareto front.\n\n        Args:\n            search_algorithm: Search algorithm after find_selection() has been called.\n\n        Returns:\n            Plotly figure with parallel coordinates plot.\n\n        Raises:\n            ValueError: If find_selection() hasn't been called.\n        \"\"\"\n        df = search_algorithm.get_all_scores()\n\n        if self.objectives is None:\n            obj_cols = [col for col in df.columns if col not in ['slices', 'label']]\n        else:\n            obj_cols = self.objectives\n            for obj in obj_cols:\n                if obj not in df.columns:\n                    raise ValueError(f\"Objective '{obj}' not found in scores\")\n\n        if len(obj_cols) &lt; 2:\n            raise ValueError(\"Need at least 2 objectives for parallel coordinates\")\n\n        has_pareto = hasattr(search_algorithm.selection_policy, 'pareto_mask')\n        pareto_mask = None\n        feasible_mask = None\n\n        if has_pareto and search_algorithm.selection_policy.pareto_mask is not None:\n            pareto_mask = search_algorithm.selection_policy.pareto_mask\n            feasible_mask = search_algorithm.selection_policy.feasible_mask\n\n        dimensions = []\n        for obj in obj_cols:\n            dimensions.append(dict(\n                label=obj,\n                values=df[obj]\n            ))\n\n        if has_pareto and pareto_mask is not None:\n            pareto = pareto_mask.values\n            feasible = feasible_mask.values\n\n            color_values = []\n            for i in range(len(df)):\n                if not feasible[i]:\n                    color_values.append(0)\n                elif pareto[i]:\n                    color_values.append(2)\n                else:\n                    color_values.append(1)\n\n            fig = go.Figure(data=go.Parcoords(\n                dimensions=dimensions,\n                line=dict(\n                    color=color_values,\n                    colorscale=[\n                        [0, 'lightgray'],\n                        [0.5, 'steelblue'],\n                        [1, 'darkorange']\n                    ],\n                    showscale=True,\n                    cmin=0,\n                    cmax=2,\n                    colorbar=dict(\n                        title='Status',\n                        tickvals=[0, 1, 2],\n                        ticktext=['Infeasible', 'Dominated', 'Pareto'],\n                    )\n                )\n            ))\n        else:\n            fig = go.Figure(data=go.Parcoords(\n                dimensions=dimensions,\n                line=dict(\n                    color='steelblue',\n                    showscale=False,\n                )\n            ))\n\n        fig.update_layout(\n            title='Parallel Coordinates: All Objectives',\n            height=500,\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoParallelCoordinates.__init__","title":"__init__","text":"<pre><code>__init__(objectives: list[str] | None = None)\n</code></pre> <p>Initialize parallel coordinates diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>list[str] | None</code> <p>List of objective names to include (None = all).</p> <code>None</code> Source code in <code>energy_repset/diagnostics/results/pareto_parallel_coords.py</code> <pre><code>def __init__(self, objectives: list[str] | None = None):\n    \"\"\"Initialize parallel coordinates diagnostic.\n\n    Args:\n        objectives: List of objective names to include (None = all).\n    \"\"\"\n    self.objectives = objectives\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ParetoParallelCoordinates.plot","title":"plot","text":"<pre><code>plot(search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm) -&gt; Figure\n</code></pre> <p>Create parallel coordinates plot of Pareto front.</p> <p>Parameters:</p> Name Type Description Default <code>search_algorithm</code> <code>ObjectiveDrivenCombinatorialSearchAlgorithm</code> <p>Search algorithm after find_selection() has been called.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with parallel coordinates plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() hasn't been called.</p> Source code in <code>energy_repset/diagnostics/results/pareto_parallel_coords.py</code> <pre><code>def plot(\n    self,\n    search_algorithm: ObjectiveDrivenCombinatorialSearchAlgorithm,\n) -&gt; go.Figure:\n    \"\"\"Create parallel coordinates plot of Pareto front.\n\n    Args:\n        search_algorithm: Search algorithm after find_selection() has been called.\n\n    Returns:\n        Plotly figure with parallel coordinates plot.\n\n    Raises:\n        ValueError: If find_selection() hasn't been called.\n    \"\"\"\n    df = search_algorithm.get_all_scores()\n\n    if self.objectives is None:\n        obj_cols = [col for col in df.columns if col not in ['slices', 'label']]\n    else:\n        obj_cols = self.objectives\n        for obj in obj_cols:\n            if obj not in df.columns:\n                raise ValueError(f\"Objective '{obj}' not found in scores\")\n\n    if len(obj_cols) &lt; 2:\n        raise ValueError(\"Need at least 2 objectives for parallel coordinates\")\n\n    has_pareto = hasattr(search_algorithm.selection_policy, 'pareto_mask')\n    pareto_mask = None\n    feasible_mask = None\n\n    if has_pareto and search_algorithm.selection_policy.pareto_mask is not None:\n        pareto_mask = search_algorithm.selection_policy.pareto_mask\n        feasible_mask = search_algorithm.selection_policy.feasible_mask\n\n    dimensions = []\n    for obj in obj_cols:\n        dimensions.append(dict(\n            label=obj,\n            values=df[obj]\n        ))\n\n    if has_pareto and pareto_mask is not None:\n        pareto = pareto_mask.values\n        feasible = feasible_mask.values\n\n        color_values = []\n        for i in range(len(df)):\n            if not feasible[i]:\n                color_values.append(0)\n            elif pareto[i]:\n                color_values.append(2)\n            else:\n                color_values.append(1)\n\n        fig = go.Figure(data=go.Parcoords(\n            dimensions=dimensions,\n            line=dict(\n                color=color_values,\n                colorscale=[\n                    [0, 'lightgray'],\n                    [0.5, 'steelblue'],\n                    [1, 'darkorange']\n                ],\n                showscale=True,\n                cmin=0,\n                cmax=2,\n                colorbar=dict(\n                    title='Status',\n                    tickvals=[0, 1, 2],\n                    ticktext=['Infeasible', 'Dominated', 'Pareto'],\n                )\n            )\n        ))\n    else:\n        fig = go.Figure(data=go.Parcoords(\n            dimensions=dimensions,\n            line=dict(\n                color='steelblue',\n                showscale=False,\n            )\n        ))\n\n    fig.update_layout(\n        title='Parallel Coordinates: All Objectives',\n        height=500,\n    )\n\n    return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ScoreContributionBars","title":"ScoreContributionBars","text":"<p>Bar chart showing final scores from each objective component.</p> <p>Visualizes the contribution of each score component to understand which objectives were most influential in the final selection. Can display absolute scores or normalized as fractions of total.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.diagnostics.results import ScoreContributionBars\n&gt;&gt;&gt; contrib = ScoreContributionBars()\n&gt;&gt;&gt; fig = contrib.plot(result.scores, normalize=True)\n&gt;&gt;&gt; fig.update_layout(title='Score Component Contributions')\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>energy_repset/diagnostics/results/score_contribution_bars.py</code> <pre><code>class ScoreContributionBars:\n    \"\"\"Bar chart showing final scores from each objective component.\n\n    Visualizes the contribution of each score component to understand which\n    objectives were most influential in the final selection. Can display\n    absolute scores or normalized as fractions of total.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.diagnostics.results import ScoreContributionBars\n        &gt;&gt;&gt; contrib = ScoreContributionBars()\n        &gt;&gt;&gt; fig = contrib.plot(result.scores, normalize=True)\n        &gt;&gt;&gt; fig.update_layout(title='Score Component Contributions')\n        &gt;&gt;&gt; fig.show()\n    \"\"\"\n\n    def plot(\n        self,\n        scores: Dict[str, float],\n        normalize: bool = False\n    ) -&gt; go.Figure:\n        \"\"\"Create bar chart of score component contributions.\n\n        Args:\n            scores: Dictionary of scores from each component (from result.scores).\n            normalize: If True, show as fractions of total score.\n\n        Returns:\n            Plotly figure with bar chart.\n        \"\"\"\n        if not scores:\n            raise ValueError(\"Scores dictionary is empty\")\n\n        component_names = list(scores.keys())\n        score_values = list(scores.values())\n\n        if normalize:\n            total = sum(score_values)\n            if total == 0:\n                raise ValueError(\"Cannot normalize: total score is zero\")\n            score_values = [v / total for v in score_values]\n            y_title = 'Normalized Score (fraction)'\n        else:\n            y_title = 'Score Value'\n\n        fig = go.Figure(data=[\n            go.Bar(\n                x=component_names,\n                y=score_values,\n                text=[f'{v:.4f}' for v in score_values],\n                textposition='auto',\n            )\n        ])\n\n        fig.update_layout(\n            xaxis_title='Score Component',\n            yaxis_title=y_title,\n            showlegend=False,\n            hovermode='x',\n        )\n\n        return fig\n</code></pre>"},{"location":"api/diagnostics/#energy_repset.diagnostics.results.ScoreContributionBars.plot","title":"plot","text":"<pre><code>plot(scores: dict[str, float], normalize: bool = False) -&gt; Figure\n</code></pre> <p>Create bar chart of score component contributions.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>dict[str, float]</code> <p>Dictionary of scores from each component (from result.scores).</p> required <code>normalize</code> <code>bool</code> <p>If True, show as fractions of total score.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with bar chart.</p> Source code in <code>energy_repset/diagnostics/results/score_contribution_bars.py</code> <pre><code>def plot(\n    self,\n    scores: Dict[str, float],\n    normalize: bool = False\n) -&gt; go.Figure:\n    \"\"\"Create bar chart of score component contributions.\n\n    Args:\n        scores: Dictionary of scores from each component (from result.scores).\n        normalize: If True, show as fractions of total score.\n\n    Returns:\n        Plotly figure with bar chart.\n    \"\"\"\n    if not scores:\n        raise ValueError(\"Scores dictionary is empty\")\n\n    component_names = list(scores.keys())\n    score_values = list(scores.values())\n\n    if normalize:\n        total = sum(score_values)\n        if total == 0:\n            raise ValueError(\"Cannot normalize: total score is zero\")\n        score_values = [v / total for v in score_values]\n        y_title = 'Normalized Score (fraction)'\n    else:\n        y_title = 'Score Value'\n\n    fig = go.Figure(data=[\n        go.Bar(\n            x=component_names,\n            y=score_values,\n            text=[f'{v:.4f}' for v in score_values],\n            textposition='auto',\n        )\n    ])\n\n    fig.update_layout(\n        xaxis_title='Score Component',\n        yaxis_title=y_title,\n        showlegend=False,\n        hovermode='x',\n    )\n\n    return fig\n</code></pre>"},{"location":"api/feature_engineering/","title":"Feature Engineering","text":""},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeatureEngineer","title":"FeatureEngineer","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for feature engineering transformations (Pillar F).</p> <p>Transforms raw sliced time-series data into a feature matrix that can be used for comparing and selecting representative periods. Implementations define how raw data is converted into a comparable feature space.</p> <p>The run() method creates a new ProblemContext with df_features populated, while subclasses implement _calc_and_get_features_df() to define the specific feature engineering logic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class SimpleStatsFeatureEngineer(FeatureEngineer):\n...     def _calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n...         features = []\n...         for slice_id in context.slicer.slices:\n...             slice_data = context.df_raw.loc[slice_id]\n...             features.append({\n...                 'mean': slice_data.mean().mean(),\n...                 'std': slice_data.std().mean(),\n...                 'max': slice_data.max().max()\n...             })\n...         return pd.DataFrame(features, index=context.slicer.slices)\n...\n&gt;&gt;&gt; engineer = SimpleStatsFeatureEngineer()\n&gt;&gt;&gt; context_with_features = engineer.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.head())\n</code></pre> Source code in <code>energy_repset/feature_engineering/base_feature_engineer.py</code> <pre><code>class FeatureEngineer(ABC):\n    \"\"\"Base class for feature engineering transformations (Pillar F).\n\n    Transforms raw sliced time-series data into a feature matrix that can be used\n    for comparing and selecting representative periods. Implementations define how\n    raw data is converted into a comparable feature space.\n\n    The run() method creates a new ProblemContext with df_features populated,\n    while subclasses implement _calc_and_get_features_df() to define the specific\n    feature engineering logic.\n\n    Examples:\n        &gt;&gt;&gt; class SimpleStatsFeatureEngineer(FeatureEngineer):\n        ...     def _calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n        ...         features = []\n        ...         for slice_id in context.slicer.slices:\n        ...             slice_data = context.df_raw.loc[slice_id]\n        ...             features.append({\n        ...                 'mean': slice_data.mean().mean(),\n        ...                 'std': slice_data.std().mean(),\n        ...                 'max': slice_data.max().max()\n        ...             })\n        ...         return pd.DataFrame(features, index=context.slicer.slices)\n        ...\n        &gt;&gt;&gt; engineer = SimpleStatsFeatureEngineer()\n        &gt;&gt;&gt; context_with_features = engineer.run(context)\n        &gt;&gt;&gt; print(context_with_features.df_features.head())\n    \"\"\"\n    def run(self, context: ProblemContext) -&gt; ProblemContext:\n        \"\"\"Calculate features and return a new context with df_features populated.\n\n        Args:\n            context: The problem context containing raw time-series data and slicing\n                information.\n\n        Returns:\n            A new ProblemContext instance with df_features set to the computed\n            feature matrix. The original context is not modified.\n        \"\"\"\n        context_with_features = context.copy()\n        context_with_features.df_features = self.calc_and_get_features_df(context)\n        return context_with_features\n\n    @abstractmethod\n    def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n        \"\"\"Calculate and return the feature matrix.\n\n        Args:\n            context: The problem context containing raw data and slicing information.\n\n        Returns:\n            A DataFrame where each row represents one slice (candidate period) and\n            each column represents a feature. The index should match the slice\n            identifiers from context.slicer.slices.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeatureEngineer.run","title":"run","text":"<pre><code>run(context: ProblemContext) -&gt; ProblemContext\n</code></pre> <p>Calculate features and return a new context with df_features populated.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context containing raw time-series data and slicing information.</p> required <p>Returns:</p> Type Description <code>ProblemContext</code> <p>A new ProblemContext instance with df_features set to the computed</p> <code>ProblemContext</code> <p>feature matrix. The original context is not modified.</p> Source code in <code>energy_repset/feature_engineering/base_feature_engineer.py</code> <pre><code>def run(self, context: ProblemContext) -&gt; ProblemContext:\n    \"\"\"Calculate features and return a new context with df_features populated.\n\n    Args:\n        context: The problem context containing raw time-series data and slicing\n            information.\n\n    Returns:\n        A new ProblemContext instance with df_features set to the computed\n        feature matrix. The original context is not modified.\n    \"\"\"\n    context_with_features = context.copy()\n    context_with_features.df_features = self.calc_and_get_features_df(context)\n    return context_with_features\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df  <code>abstractmethod</code>","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Calculate and return the feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context containing raw data and slicing information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where each row represents one slice (candidate period) and</p> <code>DataFrame</code> <p>each column represents a feature. The index should match the slice</p> <code>DataFrame</code> <p>identifiers from context.slicer.slices.</p> Source code in <code>energy_repset/feature_engineering/base_feature_engineer.py</code> <pre><code>@abstractmethod\ndef calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n    \"\"\"Calculate and return the feature matrix.\n\n    Args:\n        context: The problem context containing raw data and slicing information.\n\n    Returns:\n        A DataFrame where each row represents one slice (candidate period) and\n        each column represents a feature. The index should match the slice\n        identifiers from context.slicer.slices.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeaturePipeline","title":"FeaturePipeline","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Chains multiple feature engineers to create a combined feature space.</p> <p>Runs multiple feature engineering transformations sequentially and concatenates their outputs into a single feature matrix. Useful for combining different feature types (e.g., statistical summaries + PCA components).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.feature_engineering import StandardStatsFeatureEngineer, PCAFeatureEngineer\n&gt;&gt;&gt; stats_engineer = StandardStatsFeatureEngineer()\n&gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=3)\n&gt;&gt;&gt; pipeline = FeaturePipeline({'stats': stats_engineer, 'pca': pca_engineer})\n&gt;&gt;&gt; context_with_features = pipeline.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.columns)\n    # Shows columns from both engineers: ['mean', 'std', 'max', 'min', 'pc1', 'pc2', 'pc3']\n</code></pre> Source code in <code>energy_repset/feature_engineering/base_feature_engineer.py</code> <pre><code>class FeaturePipeline(FeatureEngineer):\n    \"\"\"Chains multiple feature engineers to create a combined feature space.\n\n    Runs multiple feature engineering transformations sequentially and concatenates\n    their outputs into a single feature matrix. Useful for combining different\n    feature types (e.g., statistical summaries + PCA components).\n\n    Examples:\n\n        &gt;&gt;&gt; from energy_repset.feature_engineering import StandardStatsFeatureEngineer, PCAFeatureEngineer\n        &gt;&gt;&gt; stats_engineer = StandardStatsFeatureEngineer()\n        &gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=3)\n        &gt;&gt;&gt; pipeline = FeaturePipeline({'stats': stats_engineer, 'pca': pca_engineer})\n        &gt;&gt;&gt; context_with_features = pipeline.run(context)\n        &gt;&gt;&gt; print(context_with_features.df_features.columns)\n            # Shows columns from both engineers: ['mean', 'std', 'max', 'min', 'pc1', 'pc2', 'pc3']\n    \"\"\"\n    def __init__(self, engineers: Dict[str, FeatureEngineer]):\n        \"\"\"Initialize the feature pipeline.\n\n        Args:\n            engineers: Dict of FeatureEngineer instances to run sequentially.\n                Features from all engineers will be concatenated column-wise.\n        \"\"\"\n        self.engineers = engineers\n\n    def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n        \"\"\"Calculate features from all engineers sequentially, accumulating results.\n\n        Each engineer in the pipeline sees the accumulated features from all\n        previous engineers via context.df_features. New features from each stage\n        are concatenated to the existing feature set. This allows:\n        - Early engineers to create base features (e.g., StandardStatsFeatureEngineer)\n        - Later engineers to transform or add to those features (e.g., PCAFeatureEngineer)\n\n        Args:\n            context: The problem context containing raw data.\n\n        Returns:\n            A DataFrame with columns from all engineers concatenated horizontally.\n            Each engineer's features are added to the cumulative feature set.\n        \"\"\"\n        # Create a mutable working copy of the context\n        working_context = context.copy()\n\n        # Accumulate features from each engineer\n        all_features = []\n        for _, engineer in self.engineers.items():\n            features = engineer.calc_and_get_features_df(working_context)\n            all_features.append(features)\n\n            # Update context so next engineer can see accumulated features\n            if all_features:\n                working_context._df_features = pd.concat(all_features, axis=1)\n\n        # Return the concatenated feature set\n        return pd.concat(all_features, axis=1)\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeaturePipeline.__init__","title":"__init__","text":"<pre><code>__init__(engineers: dict[str, FeatureEngineer])\n</code></pre> <p>Initialize the feature pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>engineers</code> <code>dict[str, FeatureEngineer]</code> <p>Dict of FeatureEngineer instances to run sequentially. Features from all engineers will be concatenated column-wise.</p> required Source code in <code>energy_repset/feature_engineering/base_feature_engineer.py</code> <pre><code>def __init__(self, engineers: Dict[str, FeatureEngineer]):\n    \"\"\"Initialize the feature pipeline.\n\n    Args:\n        engineers: Dict of FeatureEngineer instances to run sequentially.\n            Features from all engineers will be concatenated column-wise.\n    \"\"\"\n    self.engineers = engineers\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.FeaturePipeline.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Calculate features from all engineers sequentially, accumulating results.</p> <p>Each engineer in the pipeline sees the accumulated features from all previous engineers via context.df_features. New features from each stage are concatenated to the existing feature set. This allows: - Early engineers to create base features (e.g., StandardStatsFeatureEngineer) - Later engineers to transform or add to those features (e.g., PCAFeatureEngineer)</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context containing raw data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns from all engineers concatenated horizontally.</p> <code>DataFrame</code> <p>Each engineer's features are added to the cumulative feature set.</p> Source code in <code>energy_repset/feature_engineering/base_feature_engineer.py</code> <pre><code>def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n    \"\"\"Calculate features from all engineers sequentially, accumulating results.\n\n    Each engineer in the pipeline sees the accumulated features from all\n    previous engineers via context.df_features. New features from each stage\n    are concatenated to the existing feature set. This allows:\n    - Early engineers to create base features (e.g., StandardStatsFeatureEngineer)\n    - Later engineers to transform or add to those features (e.g., PCAFeatureEngineer)\n\n    Args:\n        context: The problem context containing raw data.\n\n    Returns:\n        A DataFrame with columns from all engineers concatenated horizontally.\n        Each engineer's features are added to the cumulative feature set.\n    \"\"\"\n    # Create a mutable working copy of the context\n    working_context = context.copy()\n\n    # Accumulate features from each engineer\n    all_features = []\n    for _, engineer in self.engineers.items():\n        features = engineer.calc_and_get_features_df(working_context)\n        all_features.append(features)\n\n        # Update context so next engineer can see accumulated features\n        if all_features:\n            working_context._df_features = pd.concat(all_features, axis=1)\n\n    # Return the concatenated feature set\n    return pd.concat(all_features, axis=1)\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer","title":"StandardStatsFeatureEngineer","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Extracts statistical features from time-series slices with robust scaling.</p> <p>For each original variable and slice, computes: - Central tendency: mean, median (q50) - Dispersion: std, IQR (q90 - q10), q10, q90 - Distribution shape: neg_share (proportion of negative values) - Temporal dynamics: ramp_std (std of first differences)</p> <p>Optionally includes cross-variable correlations within each slice (upper triangle only, Fisher-z transformed). Features are z-score normalized across slices to ensure comparability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; engineer = StandardStatsFeatureEngineer()\n&gt;&gt;&gt; context_with_features = engineer.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.columns)\n# ['mean__demand', 'mean__solar', 'std__demand', 'std__solar', ...]\n</code></pre> <pre><code>&gt;&gt;&gt; engineer_no_corr = StandardStatsFeatureEngineer(\n...     include_correlations=False,\n...     scale='zscore'\n... )\n&gt;&gt;&gt; context_with_features = engineer_no_corr.run(context)\n&gt;&gt;&gt; print(context_with_features.df_features.shape)\n# (12, 16) for 12 months, 2 variables, 8 stats each\n</code></pre> Source code in <code>energy_repset/feature_engineering/standard_stats.py</code> <pre><code>class StandardStatsFeatureEngineer(FeatureEngineer):\n    \"\"\"Extracts statistical features from time-series slices with robust scaling.\n\n    For each original variable and slice, computes:\n    - Central tendency: mean, median (q50)\n    - Dispersion: std, IQR (q90 - q10), q10, q90\n    - Distribution shape: neg_share (proportion of negative values)\n    - Temporal dynamics: ramp_std (std of first differences)\n\n    Optionally includes cross-variable correlations within each slice (upper\n    triangle only, Fisher-z transformed). Features are z-score normalized\n    across slices to ensure comparability.\n\n    Examples:\n        &gt;&gt;&gt; engineer = StandardStatsFeatureEngineer()\n        &gt;&gt;&gt; context_with_features = engineer.run(context)\n        &gt;&gt;&gt; print(context_with_features.df_features.columns)\n        # ['mean__demand', 'mean__solar', 'std__demand', 'std__solar', ...]\n\n        &gt;&gt;&gt; engineer_no_corr = StandardStatsFeatureEngineer(\n        ...     include_correlations=False,\n        ...     scale='zscore'\n        ... )\n        &gt;&gt;&gt; context_with_features = engineer_no_corr.run(context)\n        &gt;&gt;&gt; print(context_with_features.df_features.shape)\n        # (12, 16) for 12 months, 2 variables, 8 stats each\n    \"\"\"\n\n    def __init__(\n            self,\n            include_correlations: bool = True,\n            scale: Literal[\"zscore\", \"none\"] = \"zscore\",\n            min_rows_for_corr: int = 8,\n    ):\n        \"\"\"Initialize the statistical feature engineer.\n\n        Args:\n            include_correlations: If True, include cross-variable correlations\n                per slice (Fisher-z transformed).\n            scale: Scaling method. Currently only \"zscore\" is fully supported.\n            min_rows_for_corr: Minimum number of rows per slice required to\n                compute correlations. Slices with fewer rows get correlation\n                features set to 0.\n        \"\"\"\n        self.include_correlations = include_correlations\n        self.scale = scale\n        self.min_rows_for_corr = min_rows_for_corr\n\n        self._raw_feats_: pd.Series = None\n        self._means_: pd.Series = None\n        self._stds_: pd.Series = None\n        self._feature_names_: List[str] = None\n\n    def calc_and_get_features_df(self, context: \"ProblemContext\") -&gt; pd.DataFrame:\n        \"\"\"Calculate statistical features and return scaled feature matrix.\n\n        Args:\n            context: Problem context with raw time-series data.\n\n        Returns:\n            DataFrame where each row is a slice and columns are scaled statistical\n            features. Column names follow pattern '{stat}__{variable}'.\n        \"\"\"\n        self._fit(context)\n        return self._transform(context)\n\n    def _fit(self, context: \"ProblemContext\") -&gt; None:\n        \"\"\"Compute raw features and fit scaling parameters.\"\"\"\n        df_raw = context.df_raw\n        slicer = context.slicer\n\n        self._raw_feats_ = self._compute_raw_features(df_raw, slicer)\n        if self.scale == \"zscore\":\n            self._means_ = self._raw_feats_.mean(axis=0)\n            self._stds_ = self._raw_feats_.std(axis=0).replace(0, 1.0)\n        self._feature_names_ = list(self._raw_feats_.columns)\n\n    def _transform(self, context: \"ProblemContext\") -&gt; pd.DataFrame:\n        \"\"\"Apply scaling to raw features.\"\"\"\n        feats = self._raw_feats_\n        if self.scale == \"zscore\":\n            feats = (feats - self._means_) / self._stds_\n        elif self.scale == \"none\":\n            pass\n        else:\n            raise NotImplementedError(f\"Scaling {self.scale} not recognized.\")\n        feats = feats.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n        return feats\n\n    def feature_names(self) -&gt; List[str]:\n        \"\"\"Get list of feature column names.\n\n        Returns:\n            List of feature names in the format '{stat}__{variable}' or\n            'corr__{var1}__{var2}' for correlations.\n        \"\"\"\n        if self._feature_names_ is None:\n            return []\n        return list(self._feature_names_)\n\n    def _compute_raw_features(self, df: pd.DataFrame, slicer: TimeSlicer) -&gt; pd.DataFrame:\n        \"\"\"Compute raw (unscaled) statistical features for each slice.\"\"\"\n        X = df.select_dtypes(include=[np.number]).copy()\n        labels = pd.Index(slicer.labels_for_index(X.index), name=\"slice\")\n        grp = X.groupby(labels)\n\n        def neg_share(a: pd.Series) -&gt; float:\n            n = a.notna().sum()\n            return float((a &lt; 0).sum() / n) if n &gt; 0 else 0.0\n\n        def ramp_std(a: pd.Series) -&gt; float:\n            d = a.diff().dropna()\n            return float(d.std()) if len(d) else 0.0\n\n        stats: Dict[str, pd.DataFrame] = {}\n        stats[\"mean\"] = grp.mean(numeric_only=True)\n        stats[\"std\"] = grp.std(numeric_only=True).fillna(0.0)\n        stats[\"q10\"] = grp.quantile(0.10)\n        stats[\"q50\"] = grp.quantile(0.50)\n        stats[\"q90\"] = grp.quantile(0.90)\n        stats[\"iqr\"] = stats[\"q90\"] - stats[\"q10\"]\n        stats[\"neg_share\"] = grp.apply(lambda g: g.apply(neg_share, axis=0))\n        stats[\"ramp_std\"] = grp.apply(lambda g: g.apply(ramp_std, axis=0))\n\n        frames = []\n        for key, dfk in stats.items():\n            dfk = dfk.add_prefix(f\"{key}__\")\n            frames.append(dfk)\n\n        if self.include_correlations and X.shape[1] &gt;= 2:\n            cols = list(X.columns)\n            pairs = [(i, j) for i in range(len(cols)) for j in range(i + 1, len(cols))]\n            names = [f\"corr__{cols[i]}__{cols[j]}\" for i, j in pairs]\n            corr_rows = []\n            idx_rows = []\n            for s, g in grp:\n                if len(g) &gt;= self.min_rows_for_corr:\n                    C = g.corr().to_numpy()\n                    vals = [C[i, j] for i, j in pairs]\n                else:\n                    vals = [0.0] * len(pairs)\n                zvals = [0.5 * np.log((1 + v) / (1 - v)) if abs(v) &lt; 0.999 else np.sign(v) * 3.8 for v in vals]\n                corr_rows.append(zvals)\n                idx_rows.append(s)\n            corr_df = pd.DataFrame(corr_rows, index=idx_rows, columns=names)\n            frames.append(corr_df)\n\n        df_features = pd.concat(frames, axis=1).sort_index()\n        return df_features\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer.__init__","title":"__init__","text":"<pre><code>__init__(include_correlations: bool = True, scale: Literal['zscore', 'none'] = 'zscore', min_rows_for_corr: int = 8)\n</code></pre> <p>Initialize the statistical feature engineer.</p> <p>Parameters:</p> Name Type Description Default <code>include_correlations</code> <code>bool</code> <p>If True, include cross-variable correlations per slice (Fisher-z transformed).</p> <code>True</code> <code>scale</code> <code>Literal['zscore', 'none']</code> <p>Scaling method. Currently only \"zscore\" is fully supported.</p> <code>'zscore'</code> <code>min_rows_for_corr</code> <code>int</code> <p>Minimum number of rows per slice required to compute correlations. Slices with fewer rows get correlation features set to 0.</p> <code>8</code> Source code in <code>energy_repset/feature_engineering/standard_stats.py</code> <pre><code>def __init__(\n        self,\n        include_correlations: bool = True,\n        scale: Literal[\"zscore\", \"none\"] = \"zscore\",\n        min_rows_for_corr: int = 8,\n):\n    \"\"\"Initialize the statistical feature engineer.\n\n    Args:\n        include_correlations: If True, include cross-variable correlations\n            per slice (Fisher-z transformed).\n        scale: Scaling method. Currently only \"zscore\" is fully supported.\n        min_rows_for_corr: Minimum number of rows per slice required to\n            compute correlations. Slices with fewer rows get correlation\n            features set to 0.\n    \"\"\"\n    self.include_correlations = include_correlations\n    self.scale = scale\n    self.min_rows_for_corr = min_rows_for_corr\n\n    self._raw_feats_: pd.Series = None\n    self._means_: pd.Series = None\n    self._stds_: pd.Series = None\n    self._feature_names_: List[str] = None\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: 'ProblemContext') -&gt; DataFrame\n</code></pre> <p>Calculate statistical features and return scaled feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'ProblemContext'</code> <p>Problem context with raw time-series data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame where each row is a slice and columns are scaled statistical</p> <code>DataFrame</code> <p>features. Column names follow pattern '{stat}__{variable}'.</p> Source code in <code>energy_repset/feature_engineering/standard_stats.py</code> <pre><code>def calc_and_get_features_df(self, context: \"ProblemContext\") -&gt; pd.DataFrame:\n    \"\"\"Calculate statistical features and return scaled feature matrix.\n\n    Args:\n        context: Problem context with raw time-series data.\n\n    Returns:\n        DataFrame where each row is a slice and columns are scaled statistical\n        features. Column names follow pattern '{stat}__{variable}'.\n    \"\"\"\n    self._fit(context)\n    return self._transform(context)\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.StandardStatsFeatureEngineer.feature_names","title":"feature_names","text":"<pre><code>feature_names() -&gt; list[str]\n</code></pre> <p>Get list of feature column names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of feature names in the format '{stat}__{variable}' or</p> <code>list[str]</code> <p>'corr__{var1}__{var2}' for correlations.</p> Source code in <code>energy_repset/feature_engineering/standard_stats.py</code> <pre><code>def feature_names(self) -&gt; List[str]:\n    \"\"\"Get list of feature column names.\n\n    Returns:\n        List of feature names in the format '{stat}__{variable}' or\n        'corr__{var1}__{var2}' for correlations.\n    \"\"\"\n    if self._feature_names_ is None:\n        return []\n    return list(self._feature_names_)\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer","title":"PCAFeatureEngineer","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Performs PCA dimensionality reduction on existing features.</p> <p>Reduces the feature space using Principal Component Analysis, typically applied after statistical feature engineering. This is useful for: - Reducing dimensionality when you have many correlated features - Creating orthogonal feature representations - Focusing on the main axes of variation</p> <p>Commonly used in a FeaturePipeline after StandardStatsFeatureEngineer to compress statistical features into a smaller number of principal components.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int | float | None</code> <p>Number of principal components to retain. Can be: - int: Exact number of components - float (0.0-1.0): Retain enough components to explain this   fraction of variance - None: Retain all components (no reduction)</p> <code>None</code> <code>whiten</code> <code>bool</code> <p>If True, scale components to unit variance. This can improve results when PCA features are used with distance-based algorithms.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.feature_engineering import PCAFeatureEngineer\n&gt;&gt;&gt; # Use PCA alone (requires context to already have df_features)\n&gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=5)\n&gt;&gt;&gt; context_with_pca = pca_engineer.run(context_with_features)\n&gt;&gt;&gt; print(context_with_pca.df_features.columns)\n    ['pc_0', 'pc_1', 'pc_2', 'pc_3', 'pc_4']\n\n&gt;&gt;&gt; # More common: chain with StandardStats in a pipeline\n&gt;&gt;&gt; from energy_repset.feature_engineering import (\n...     StandardStatsFeatureEngineer,\n...     FeaturePipeline\n... )\n&gt;&gt;&gt; pipeline = FeaturePipeline([\n...     StandardStatsFeatureEngineer(),\n...     PCAFeatureEngineer(n_components=0.95)  # Keep 95% variance\n... ])\n&gt;&gt;&gt; context_with_both = pipeline.run(context)\n\n&gt;&gt;&gt; # Check explained variance\n&gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=10)\n&gt;&gt;&gt; context_out = pca_engineer.run(context_with_features)\n&gt;&gt;&gt; print(pca_engineer.explained_variance_ratio_)\n    [0.45, 0.22, 0.11, ...]\n</code></pre> Source code in <code>energy_repset/feature_engineering/pca.py</code> <pre><code>class PCAFeatureEngineer(FeatureEngineer):\n    \"\"\"Performs PCA dimensionality reduction on existing features.\n\n    Reduces the feature space using Principal Component Analysis, typically\n    applied after statistical feature engineering. This is useful for:\n    - Reducing dimensionality when you have many correlated features\n    - Creating orthogonal feature representations\n    - Focusing on the main axes of variation\n\n    Commonly used in a FeaturePipeline after StandardStatsFeatureEngineer\n    to compress statistical features into a smaller number of principal\n    components.\n\n    Args:\n        n_components: Number of principal components to retain. Can be:\n            - int: Exact number of components\n            - float (0.0-1.0): Retain enough components to explain this\n              fraction of variance\n            - None: Retain all components (no reduction)\n        whiten: If True, scale components to unit variance. This can improve\n            results when PCA features are used with distance-based algorithms.\n\n    Examples:\n\n        &gt;&gt;&gt; from energy_repset.feature_engineering import PCAFeatureEngineer\n        &gt;&gt;&gt; # Use PCA alone (requires context to already have df_features)\n        &gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=5)\n        &gt;&gt;&gt; context_with_pca = pca_engineer.run(context_with_features)\n        &gt;&gt;&gt; print(context_with_pca.df_features.columns)\n            ['pc_0', 'pc_1', 'pc_2', 'pc_3', 'pc_4']\n\n        &gt;&gt;&gt; # More common: chain with StandardStats in a pipeline\n        &gt;&gt;&gt; from energy_repset.feature_engineering import (\n        ...     StandardStatsFeatureEngineer,\n        ...     FeaturePipeline\n        ... )\n        &gt;&gt;&gt; pipeline = FeaturePipeline([\n        ...     StandardStatsFeatureEngineer(),\n        ...     PCAFeatureEngineer(n_components=0.95)  # Keep 95% variance\n        ... ])\n        &gt;&gt;&gt; context_with_both = pipeline.run(context)\n\n        &gt;&gt;&gt; # Check explained variance\n        &gt;&gt;&gt; pca_engineer = PCAFeatureEngineer(n_components=10)\n        &gt;&gt;&gt; context_out = pca_engineer.run(context_with_features)\n        &gt;&gt;&gt; print(pca_engineer.explained_variance_ratio_)\n            [0.45, 0.22, 0.11, ...]\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int | float | None = None,\n        whiten: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize PCA feature engineer.\n\n        Args:\n            n_components: Number of components to keep, or fraction of\n                variance to preserve (if float). None keeps all components.\n            whiten: Whether to whiten (scale) the principal components.\n        \"\"\"\n        self.n_components = n_components\n        self.whiten = whiten\n        self._pca: PCA | None = None\n        self._feature_names: List[str] = []\n\n    def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n        \"\"\"Apply PCA to existing features in context.\n\n        Args:\n            context: Problem context with df_features already populated\n                (typically by StandardStatsFeatureEngineer or similar).\n\n        Returns:\n            DataFrame with principal component features. Columns are named\n            'pc_0', 'pc_1', etc.\n\n        Raises:\n            ValueError: If context.df_features is None or empty.\n        \"\"\"\n        if context._df_features is None or context._df_features.empty:\n            raise ValueError(\n                \"PCAFeatureEngineer requires context.df_features to be populated. \"\n                \"Run StandardStatsFeatureEngineer or similar first, or use \"\n                \"FeaturePipeline([StandardStatsFeatureEngineer(), PCAFeatureEngineer()]).\"\n            )\n\n        X = context.df_features.values\n        index = context.df_features.index\n\n        # Handle NaN/inf values\n        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Fit PCA\n        self._pca = PCA(n_components=self.n_components, whiten=self.whiten)\n        X_transformed = self._pca.fit_transform(X)\n\n        # Create feature names\n        n_components_actual = X_transformed.shape[1]\n        self._feature_names = [f\"pc_{i}\" for i in range(n_components_actual)]\n\n        # Create DataFrame\n        df_pca = pd.DataFrame(\n            X_transformed,\n            index=index,\n            columns=self._feature_names\n        )\n\n        return df_pca\n\n    def feature_names(self) -&gt; List[str]:\n        \"\"\"Get list of principal component feature names.\n\n        Returns:\n            List of feature names: ['pc_0', 'pc_1', ...].\n        \"\"\"\n        return list(self._feature_names)\n\n    @property\n    def explained_variance_ratio_(self) -&gt; np.ndarray | None:\n        \"\"\"Get the proportion of variance explained by each component.\n\n        Returns:\n            Array of explained variance ratios, or None if PCA not fitted yet.\n\n        Examples:\n\n            &gt;&gt;&gt; pca_eng = PCAFeatureEngineer(n_components=5)\n            &gt;&gt;&gt; context_out = pca_eng.run(context_with_features)\n            &gt;&gt;&gt; print(pca_eng.explained_variance_ratio_)\n            # [0.45, 0.22, 0.15, 0.09, 0.05]\n            &gt;&gt;&gt; print(f\"Total variance explained: {pca_eng.explained_variance_ratio_.sum():.2%}\")\n            # Total variance explained: 96%\n        \"\"\"\n        if self._pca is None:\n            return None\n        return self._pca.explained_variance_ratio_\n\n    @property\n    def components_(self) -&gt; np.ndarray | None:\n        \"\"\"Get the principal component loadings.\n\n        Returns:\n            Array of shape (n_components, n_features) containing the\n            principal axes in feature space, or None if PCA not fitted yet.\n        \"\"\"\n        if self._pca is None:\n            return None\n        return self._pca.components_\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.explained_variance_ratio_","title":"explained_variance_ratio_  <code>property</code>","text":"<pre><code>explained_variance_ratio_: ndarray | None\n</code></pre> <p>Get the proportion of variance explained by each component.</p> <p>Returns:</p> Type Description <code>ndarray | None</code> <p>Array of explained variance ratios, or None if PCA not fitted yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pca_eng = PCAFeatureEngineer(n_components=5)\n&gt;&gt;&gt; context_out = pca_eng.run(context_with_features)\n&gt;&gt;&gt; print(pca_eng.explained_variance_ratio_)\n# [0.45, 0.22, 0.15, 0.09, 0.05]\n&gt;&gt;&gt; print(f\"Total variance explained: {pca_eng.explained_variance_ratio_.sum():.2%}\")\n# Total variance explained: 96%\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.components_","title":"components_  <code>property</code>","text":"<pre><code>components_: ndarray | None\n</code></pre> <p>Get the principal component loadings.</p> <p>Returns:</p> Type Description <code>ndarray | None</code> <p>Array of shape (n_components, n_features) containing the</p> <code>ndarray | None</code> <p>principal axes in feature space, or None if PCA not fitted yet.</p>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.__init__","title":"__init__","text":"<pre><code>__init__(n_components: int | float | None = None, whiten: bool = False) -&gt; None\n</code></pre> <p>Initialize PCA feature engineer.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int | float | None</code> <p>Number of components to keep, or fraction of variance to preserve (if float). None keeps all components.</p> <code>None</code> <code>whiten</code> <code>bool</code> <p>Whether to whiten (scale) the principal components.</p> <code>False</code> Source code in <code>energy_repset/feature_engineering/pca.py</code> <pre><code>def __init__(\n    self,\n    n_components: int | float | None = None,\n    whiten: bool = False\n) -&gt; None:\n    \"\"\"Initialize PCA feature engineer.\n\n    Args:\n        n_components: Number of components to keep, or fraction of\n            variance to preserve (if float). None keeps all components.\n        whiten: Whether to whiten (scale) the principal components.\n    \"\"\"\n    self.n_components = n_components\n    self.whiten = whiten\n    self._pca: PCA | None = None\n    self._feature_names: List[str] = []\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Apply PCA to existing features in context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with df_features already populated (typically by StandardStatsFeatureEngineer or similar).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with principal component features. Columns are named</p> <code>DataFrame</code> <p>'pc_0', 'pc_1', etc.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If context.df_features is None or empty.</p> Source code in <code>energy_repset/feature_engineering/pca.py</code> <pre><code>def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n    \"\"\"Apply PCA to existing features in context.\n\n    Args:\n        context: Problem context with df_features already populated\n            (typically by StandardStatsFeatureEngineer or similar).\n\n    Returns:\n        DataFrame with principal component features. Columns are named\n        'pc_0', 'pc_1', etc.\n\n    Raises:\n        ValueError: If context.df_features is None or empty.\n    \"\"\"\n    if context._df_features is None or context._df_features.empty:\n        raise ValueError(\n            \"PCAFeatureEngineer requires context.df_features to be populated. \"\n            \"Run StandardStatsFeatureEngineer or similar first, or use \"\n            \"FeaturePipeline([StandardStatsFeatureEngineer(), PCAFeatureEngineer()]).\"\n        )\n\n    X = context.df_features.values\n    index = context.df_features.index\n\n    # Handle NaN/inf values\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Fit PCA\n    self._pca = PCA(n_components=self.n_components, whiten=self.whiten)\n    X_transformed = self._pca.fit_transform(X)\n\n    # Create feature names\n    n_components_actual = X_transformed.shape[1]\n    self._feature_names = [f\"pc_{i}\" for i in range(n_components_actual)]\n\n    # Create DataFrame\n    df_pca = pd.DataFrame(\n        X_transformed,\n        index=index,\n        columns=self._feature_names\n    )\n\n    return df_pca\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.PCAFeatureEngineer.feature_names","title":"feature_names","text":"<pre><code>feature_names() -&gt; list[str]\n</code></pre> <p>Get list of principal component feature names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of feature names: ['pc_0', 'pc_1', ...].</p> Source code in <code>energy_repset/feature_engineering/pca.py</code> <pre><code>def feature_names(self) -&gt; List[str]:\n    \"\"\"Get list of principal component feature names.\n\n    Returns:\n        List of feature names: ['pc_0', 'pc_1', ...].\n    \"\"\"\n    return list(self._feature_names)\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.DirectProfileFeatureEngineer","title":"DirectProfileFeatureEngineer","text":"<p>               Bases: <code>FeatureEngineer</code></p> <p>Feature engineer that uses raw profile vectors directly (F_direct).</p> <p>For each slice, concatenates the raw hourly values across all variables into a single flat feature vector. This preserves the full temporal shape of each period, making it suitable for algorithms that compare time-series profiles directly (e.g., Snippet Algorithm, DTW-based methods).</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>dict[str, float] | None</code> <p>Optional dict mapping column names to scalar weights. Weighted columns are multiplied by their weight before flattening. Columns not in the dict are included with weight 1.0.</p> <code>None</code> <p>Examples:</p> <p>Basic usage with daily slicing:</p> <pre><code>&gt;&gt;&gt; from energy_repset.feature_engineering import DirectProfileFeatureEngineer\n&gt;&gt;&gt; engineer = DirectProfileFeatureEngineer()\n&gt;&gt;&gt; context_with_features = engineer.run(context)\n&gt;&gt;&gt; context_with_features.df_features.shape\n(365, 72)  # 365 days x (24 hours * 3 variables)\n</code></pre> Source code in <code>energy_repset/feature_engineering/direct_profile.py</code> <pre><code>class DirectProfileFeatureEngineer(FeatureEngineer):\n    \"\"\"Feature engineer that uses raw profile vectors directly (F_direct).\n\n    For each slice, concatenates the raw hourly values across all variables\n    into a single flat feature vector. This preserves the full temporal shape\n    of each period, making it suitable for algorithms that compare time-series\n    profiles directly (e.g., Snippet Algorithm, DTW-based methods).\n\n    Args:\n        variable_weights: Optional dict mapping column names to scalar weights.\n            Weighted columns are multiplied by their weight before flattening.\n            Columns not in the dict are included with weight 1.0.\n\n    Examples:\n        Basic usage with daily slicing:\n\n        &gt;&gt;&gt; from energy_repset.feature_engineering import DirectProfileFeatureEngineer\n        &gt;&gt;&gt; engineer = DirectProfileFeatureEngineer()\n        &gt;&gt;&gt; context_with_features = engineer.run(context)\n        &gt;&gt;&gt; context_with_features.df_features.shape\n        (365, 72)  # 365 days x (24 hours * 3 variables)\n    \"\"\"\n\n    def __init__(self, variable_weights: Optional[Dict[str, float]] = None):\n        \"\"\"Initialize direct profile feature engineer.\n\n        Args:\n            variable_weights: Optional mapping of variable names to weights.\n                Variables not in the dict receive weight 1.0.\n        \"\"\"\n        self.variable_weights = variable_weights or {}\n\n    def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n        \"\"\"Flatten each slice's raw values into a single feature row.\n\n        Args:\n            context: Problem context with raw time-series data.\n\n        Returns:\n            DataFrame where each row is one slice and columns are the\n            flattened hourly values (hours x variables).\n        \"\"\"\n        df = context.df_raw.select_dtypes(include=[np.number]).copy()\n\n        for col, w in self.variable_weights.items():\n            if col in df.columns:\n                df[col] = df[col] * w\n\n        slice_labels = context.slicer.labels_for_index(df.index)\n        unique_slices = context.slicer.unique_slices(df.index)\n\n        rows = []\n        for s in unique_slices:\n            mask = slice_labels == s\n            chunk = df.loc[mask]\n            flat = chunk.values.flatten(order='C')\n            rows.append(flat)\n\n        max_len = max(len(r) for r in rows)\n        padded = []\n        for r in rows:\n            if len(r) &lt; max_len:\n                r = np.pad(r, (0, max_len - len(r)), constant_values=np.nan)\n            padded.append(r)\n\n        col_names = [f\"t{i}\" for i in range(max_len)]\n        return pd.DataFrame(padded, index=unique_slices, columns=col_names)\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.DirectProfileFeatureEngineer.__init__","title":"__init__","text":"<pre><code>__init__(variable_weights: dict[str, float] | None = None)\n</code></pre> <p>Initialize direct profile feature engineer.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>dict[str, float] | None</code> <p>Optional mapping of variable names to weights. Variables not in the dict receive weight 1.0.</p> <code>None</code> Source code in <code>energy_repset/feature_engineering/direct_profile.py</code> <pre><code>def __init__(self, variable_weights: Optional[Dict[str, float]] = None):\n    \"\"\"Initialize direct profile feature engineer.\n\n    Args:\n        variable_weights: Optional mapping of variable names to weights.\n            Variables not in the dict receive weight 1.0.\n    \"\"\"\n    self.variable_weights = variable_weights or {}\n</code></pre>"},{"location":"api/feature_engineering/#energy_repset.feature_engineering.DirectProfileFeatureEngineer.calc_and_get_features_df","title":"calc_and_get_features_df","text":"<pre><code>calc_and_get_features_df(context: ProblemContext) -&gt; DataFrame\n</code></pre> <p>Flatten each slice's raw values into a single feature row.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with raw time-series data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame where each row is one slice and columns are the</p> <code>DataFrame</code> <p>flattened hourly values (hours x variables).</p> Source code in <code>energy_repset/feature_engineering/direct_profile.py</code> <pre><code>def calc_and_get_features_df(self, context: ProblemContext) -&gt; pd.DataFrame:\n    \"\"\"Flatten each slice's raw values into a single feature row.\n\n    Args:\n        context: Problem context with raw time-series data.\n\n    Returns:\n        DataFrame where each row is one slice and columns are the\n        flattened hourly values (hours x variables).\n    \"\"\"\n    df = context.df_raw.select_dtypes(include=[np.number]).copy()\n\n    for col, w in self.variable_weights.items():\n        if col in df.columns:\n            df[col] = df[col] * w\n\n    slice_labels = context.slicer.labels_for_index(df.index)\n    unique_slices = context.slicer.unique_slices(df.index)\n\n    rows = []\n    for s in unique_slices:\n        mask = slice_labels == s\n        chunk = df.loc[mask]\n        flat = chunk.values.flatten(order='C')\n        rows.append(flat)\n\n    max_len = max(len(r) for r in rows)\n    padded = []\n    for r in rows:\n        if len(r) &lt; max_len:\n            r = np.pad(r, (0, max_len - len(r)), constant_values=np.nan)\n        padded.append(r)\n\n    col_names = [f\"t{i}\" for i in range(max_len)]\n    return pd.DataFrame(padded, index=unique_slices, columns=col_names)\n</code></pre>"},{"location":"api/objectives/","title":"Objectives","text":""},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSpec","title":"ObjectiveSpec  <code>dataclass</code>","text":"<p>Specification for a single score component with its preference weight.</p> <p>Attributes:</p> Name Type Description <code>component</code> <code>ScoreComponent</code> <p>The ScoreComponent that computes the metric.</p> <code>weight</code> <code>float</code> <p>Non-negative weight indicating the component's importance (&gt;= 0).</p> Source code in <code>energy_repset/objectives.py</code> <pre><code>@dataclass(frozen=True)\nclass ObjectiveSpec:\n    \"\"\"Specification for a single score component with its preference weight.\n\n    Attributes:\n        component: The ScoreComponent that computes the metric.\n        weight: Non-negative weight indicating the component's importance (&gt;= 0).\n    \"\"\"\n    component: ScoreComponent\n    weight: float\n</code></pre>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet","title":"ObjectiveSet","text":"<p>Pillar O: A collection of weighted score components for evaluating selections.</p> <p>This class holds multiple ScoreComponents, each with a weight indicating its importance. Components define their optimization direction (min/max), while weights specify preference magnitude. The ObjectiveSet prepares all components with context data and evaluates candidate selections.</p> <p>Attributes:</p> Name Type Description <code>weighted_score_components</code> <code>dict[str, ObjectiveSpec]</code> <p>Dictionary mapping component names to ObjectiveSpec instances containing the component and its weight.</p> <p>Examples:</p> <p>Create an objective set with multiple fidelity metrics:</p> <pre><code>&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt; from energy_repset.score_components import (\n...     WassersteinFidelity, CorrelationFidelity\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; objective_set = ObjectiveSet({\n...     'wasserstein': (0.5, WassersteinFidelity()),\n...     'correlation': (0.5, CorrelationFidelity()),\n... })\n</code></pre> <p>With variable-specific weights:</p> <pre><code>&gt;&gt;&gt; wass = WassersteinFidelity(variable_weights={'demand': 2.0, 'solar': 1.0})\n&gt;&gt;&gt; objective_set = ObjectiveSet({\n...     'wasserstein': (1.0, wass),\n... })\n</code></pre> Source code in <code>energy_repset/objectives.py</code> <pre><code>class ObjectiveSet:\n    \"\"\"Pillar O: A collection of weighted score components for evaluating selections.\n\n    This class holds multiple ScoreComponents, each with a weight indicating its\n    importance. Components define their optimization direction (min/max), while\n    weights specify preference magnitude. The ObjectiveSet prepares all components\n    with context data and evaluates candidate selections.\n\n    Attributes:\n        weighted_score_components: Dictionary mapping component names to ObjectiveSpec\n            instances containing the component and its weight.\n\n    Examples:\n        Create an objective set with multiple fidelity metrics:\n\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt; from energy_repset.score_components import (\n        ...     WassersteinFidelity, CorrelationFidelity\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; objective_set = ObjectiveSet({\n        ...     'wasserstein': (0.5, WassersteinFidelity()),\n        ...     'correlation': (0.5, CorrelationFidelity()),\n        ... })\n\n        With variable-specific weights:\n\n        &gt;&gt;&gt; wass = WassersteinFidelity(variable_weights={'demand': 2.0, 'solar': 1.0})\n        &gt;&gt;&gt; objective_set = ObjectiveSet({\n        ...     'wasserstein': (1.0, wass),\n        ... })\n    \"\"\"\n\n    def __init__(\n            self,\n            weighted_score_components: Dict[str, Tuple[float, ScoreComponent]]\n    ) -&gt; None:\n        \"\"\"Initialize ObjectiveSet with weighted score components.\n\n        Args:\n            weighted_score_components: Dictionary mapping component names to\n                tuples of (weight, ScoreComponent). Weights must be non-negative.\n\n        Raises:\n            ValueError: If any weight is negative or if any component lacks a\n                'direction' attribute set to 'min' or 'max'.\n        \"\"\"\n        self.weighted_score_components: Dict[str, ObjectiveSpec] = {\n            name: s if isinstance(s, ObjectiveSpec) else ObjectiveSpec(component=s[1], weight=float(s[0]))\n            for name, s in weighted_score_components.items()\n        }\n        for s in self.weighted_score_components.values():\n            if s.weight &lt; 0:\n                raise ValueError(f\"Weight for {s.component.name} must be &gt;= 0.\")\n            if getattr(s.component, \"direction\", None) not in (\"min\", \"max\"):\n                raise ValueError(f\"Component {s.component.name} must declare direction 'min' or 'max'.\")\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Prepare all score components with context data.\n\n        This method calls prepare() on each component to allow pre-computation\n        of reference statistics, duration curves, etc.\n\n        Args:\n            context: ProblemContext containing raw data and features.\n        \"\"\"\n        for spec in self.weighted_score_components.values():\n            spec.component.prepare(context)\n\n    def evaluate(self, combination: SliceCombination, context: ProblemContext) -&gt; Dict[str, float]:\n        \"\"\"Evaluate a candidate selection across all score components.\n\n        Args:\n            combination: Tuple of slice labels forming the candidate selection.\n            context: ProblemContext for accessing data.\n\n        Returns:\n            Dictionary mapping component names to their unweighted scores.\n\n        Note:\n            Returns raw scores from components. Weights are applied by SelectionPolicy\n            during the selection process.\n        \"\"\"\n        return {\n           spec.component.name: float(spec.component.score(combination))\n           for spec in self.weighted_score_components.values()\n       }\n\n    def component_meta(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Get metadata for all components.\n\n        Returns:\n            Dictionary mapping component names to their metadata containing:\n                - 'direction': Optimization direction ('min' or 'max')\n                - 'pref': Preference weight (&gt;= 0)\n        \"\"\"\n        return {\n            spec.component.name: {\"direction\": spec.component.direction, \"pref\": float(spec.weight)}\n            for spec in self.weighted_score_components.values()\n        }\n</code></pre>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.__init__","title":"__init__","text":"<pre><code>__init__(weighted_score_components: dict[str, tuple[float, ScoreComponent]]) -&gt; None\n</code></pre> <p>Initialize ObjectiveSet with weighted score components.</p> <p>Parameters:</p> Name Type Description Default <code>weighted_score_components</code> <code>dict[str, tuple[float, ScoreComponent]]</code> <p>Dictionary mapping component names to tuples of (weight, ScoreComponent). Weights must be non-negative.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any weight is negative or if any component lacks a 'direction' attribute set to 'min' or 'max'.</p> Source code in <code>energy_repset/objectives.py</code> <pre><code>def __init__(\n        self,\n        weighted_score_components: Dict[str, Tuple[float, ScoreComponent]]\n) -&gt; None:\n    \"\"\"Initialize ObjectiveSet with weighted score components.\n\n    Args:\n        weighted_score_components: Dictionary mapping component names to\n            tuples of (weight, ScoreComponent). Weights must be non-negative.\n\n    Raises:\n        ValueError: If any weight is negative or if any component lacks a\n            'direction' attribute set to 'min' or 'max'.\n    \"\"\"\n    self.weighted_score_components: Dict[str, ObjectiveSpec] = {\n        name: s if isinstance(s, ObjectiveSpec) else ObjectiveSpec(component=s[1], weight=float(s[0]))\n        for name, s in weighted_score_components.items()\n    }\n    for s in self.weighted_score_components.values():\n        if s.weight &lt; 0:\n            raise ValueError(f\"Weight for {s.component.name} must be &gt;= 0.\")\n        if getattr(s.component, \"direction\", None) not in (\"min\", \"max\"):\n            raise ValueError(f\"Component {s.component.name} must declare direction 'min' or 'max'.\")\n</code></pre>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Prepare all score components with context data.</p> <p>This method calls prepare() on each component to allow pre-computation of reference statistics, duration curves, etc.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>ProblemContext containing raw data and features.</p> required Source code in <code>energy_repset/objectives.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Prepare all score components with context data.\n\n    This method calls prepare() on each component to allow pre-computation\n    of reference statistics, duration curves, etc.\n\n    Args:\n        context: ProblemContext containing raw data and features.\n    \"\"\"\n    for spec in self.weighted_score_components.values():\n        spec.component.prepare(context)\n</code></pre>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.evaluate","title":"evaluate","text":"<pre><code>evaluate(combination: SliceCombination, context: ProblemContext) -&gt; dict[str, float]\n</code></pre> <p>Evaluate a candidate selection across all score components.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels forming the candidate selection.</p> required <code>context</code> <code>ProblemContext</code> <p>ProblemContext for accessing data.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary mapping component names to their unweighted scores.</p> Note <p>Returns raw scores from components. Weights are applied by SelectionPolicy during the selection process.</p> Source code in <code>energy_repset/objectives.py</code> <pre><code>def evaluate(self, combination: SliceCombination, context: ProblemContext) -&gt; Dict[str, float]:\n    \"\"\"Evaluate a candidate selection across all score components.\n\n    Args:\n        combination: Tuple of slice labels forming the candidate selection.\n        context: ProblemContext for accessing data.\n\n    Returns:\n        Dictionary mapping component names to their unweighted scores.\n\n    Note:\n        Returns raw scores from components. Weights are applied by SelectionPolicy\n        during the selection process.\n    \"\"\"\n    return {\n       spec.component.name: float(spec.component.score(combination))\n       for spec in self.weighted_score_components.values()\n   }\n</code></pre>"},{"location":"api/objectives/#energy_repset.objectives.ObjectiveSet.component_meta","title":"component_meta","text":"<pre><code>component_meta() -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Get metadata for all components.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping component names to their metadata containing: - 'direction': Optimization direction ('min' or 'max') - 'pref': Preference weight (&gt;= 0)</p> Source code in <code>energy_repset/objectives.py</code> <pre><code>def component_meta(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Get metadata for all components.\n\n    Returns:\n        Dictionary mapping component names to their metadata containing:\n            - 'direction': Optimization direction ('min' or 'max')\n            - 'pref': Preference weight (&gt;= 0)\n    \"\"\"\n    return {\n        spec.component.name: {\"direction\": spec.component.direction, \"pref\": float(spec.weight)}\n        for spec in self.weighted_score_components.values()\n    }\n</code></pre>"},{"location":"api/objectives/#energy_repset.score_components.base_score_component.ScoreComponent","title":"ScoreComponent","text":"<p>               Bases: <code>ABC</code></p> <p>Protocol for a single metric used in evaluating candidate selections.</p> <p>ScoreComponents are the building blocks of the ObjectiveSet. Each component computes a scalar score measuring how well a candidate selection performs on a specific criterion (e.g., distribution fidelity, diversity, balance).</p> Implementations must define <ul> <li>A unique name identifying the component</li> <li>An optimization direction ('min' or 'max')</li> <li>A prepare() method to precompute reference data from the full context</li> <li>A score() method to evaluate a candidate selection</li> </ul> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for this component.</p> <code>direction</code> <code>ScoreComponentDirection</code> <p>Optimization direction, either \"min\" or \"max\".</p> <p>Examples:</p> <p>Implementing a simple score component:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components.base_score_component import ScoreComponent\n&gt;&gt;&gt; from energy_repset.context import ProblemContext\n&gt;&gt;&gt; from energy_repset.types import SliceCombination\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; class SimpleMeanDeviation(ScoreComponent):\n...     def __init__(self):\n...         self.name = \"mean_deviation\"\n...         self.direction = \"min\"\n...         self.full_mean = None\n...\n...     def prepare(self, context: ProblemContext) -&gt; None:\n...         '''Compute reference mean from full dataset.'''\n...         self.full_mean = context.df_raw.mean().mean()\n...\n...     def score(self, combination: SliceCombination) -&gt; float:\n...         '''Measure deviation from reference mean.'''\n...         # Get data for selection and compute deviation\n...         # (implementation details omitted)\n...         return abs(selection_mean - self.full_mean)\n</code></pre> Source code in <code>energy_repset/score_components/base_score_component.py</code> <pre><code>class ScoreComponent(ABC):\n    \"\"\"Protocol for a single metric used in evaluating candidate selections.\n\n    ScoreComponents are the building blocks of the ObjectiveSet. Each component\n    computes a scalar score measuring how well a candidate selection performs\n    on a specific criterion (e.g., distribution fidelity, diversity, balance).\n\n    Implementations must define:\n        - A unique name identifying the component\n        - An optimization direction ('min' or 'max')\n        - A prepare() method to precompute reference data from the full context\n        - A score() method to evaluate a candidate selection\n\n    Attributes:\n        name: Unique identifier for this component.\n        direction: Optimization direction, either \"min\" or \"max\".\n\n    Examples:\n        Implementing a simple score component:\n\n        &gt;&gt;&gt; from energy_repset.score_components.base_score_component import ScoreComponent\n        &gt;&gt;&gt; from energy_repset.context import ProblemContext\n        &gt;&gt;&gt; from energy_repset.types import SliceCombination\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class SimpleMeanDeviation(ScoreComponent):\n        ...     def __init__(self):\n        ...         self.name = \"mean_deviation\"\n        ...         self.direction = \"min\"\n        ...         self.full_mean = None\n        ...\n        ...     def prepare(self, context: ProblemContext) -&gt; None:\n        ...         '''Compute reference mean from full dataset.'''\n        ...         self.full_mean = context.df_raw.mean().mean()\n        ...\n        ...     def score(self, combination: SliceCombination) -&gt; float:\n        ...         '''Measure deviation from reference mean.'''\n        ...         # Get data for selection and compute deviation\n        ...         # (implementation details omitted)\n        ...         return abs(selection_mean - self.full_mean)\n    \"\"\"\n    name: str\n    direction: ScoreComponentDirection\n\n    @abstractmethod\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute state needed before scoring selections.\n\n        This method is called once before evaluating any combinations. Use it\n        to compute reference statistics, duration curves, or other data derived\n        from the full dataset that will be compared against selections.\n\n        Args:\n            context: ProblemContext containing raw data, features, and metadata.\n\n        Note:\n            This method should store computed state as instance attributes for\n            use in score().\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute the component score for a candidate selection.\n\n        Args:\n            combination: Tuple of slice labels forming the candidate selection.\n\n        Returns:\n            Scalar score. Lower is better for direction='min', higher is better\n            for direction='max'.\n\n        Note:\n            This method is called many times during search. Precompute expensive\n            operations in prepare() to avoid redundant calculations.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def _default_weight_normalization(\n            weights: Optional[Dict[str, float]],\n            keys: List[str]\n    ) -&gt; Dict[str, float]:\n        \"\"\"Normalize weight dictionary to match actual keys.\n\n        Normalize weights: None \u2192 equal (1.0), specified \u2192 use values (missing get 0.0)\n\n        Args:\n            weights: User-specified weights, or None for equal weights.\n            keys: Actual keys that need weights (e.g., variable names, feature names).\n\n        Returns:\n            Dictionary mapping each key to its weight:\n            - If weights is None: all keys get 1.0 (equal weights)\n            - If weights is provided: specified keys get their value, missing keys get 0.0\n\n        Examples:\n\n            &gt;&gt;&gt; # No weights specified - equal weights\n            &gt;&gt;&gt; normalize_weights(None, ['demand', 'solar', 'wind'])\n            {'demand': 1.0, 'solar': 1.0, 'wind': 1.0}\n\n            &gt;&gt;&gt; # Partial weights - missing get 0.0\n            &gt;&gt;&gt; normalize_weights({'demand': 2.0, 'solar': 1.5}, ['demand', 'solar', 'wind'])\n            {'demand': 2.0, 'solar': 1.5, 'wind': 0.0}\n\n            &gt;&gt;&gt; # All weights specified\n            &gt;&gt;&gt; normalize_weights({'demand': 2.0, 'solar': 1.0, 'wind': 0.5}, ['demand', 'solar'])\n            {'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n        \"\"\"\n        if weights is None:\n            return {key: 1.0 for key in keys}\n        return {key: weights.get(key, 0.0) for key in keys}\n</code></pre>"},{"location":"api/objectives/#energy_repset.score_components.base_score_component.ScoreComponent.prepare","title":"prepare  <code>abstractmethod</code>","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute state needed before scoring selections.</p> <p>This method is called once before evaluating any combinations. Use it to compute reference statistics, duration curves, or other data derived from the full dataset that will be compared against selections.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>ProblemContext containing raw data, features, and metadata.</p> required Note <p>This method should store computed state as instance attributes for use in score().</p> Source code in <code>energy_repset/score_components/base_score_component.py</code> <pre><code>@abstractmethod\ndef prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute state needed before scoring selections.\n\n    This method is called once before evaluating any combinations. Use it\n    to compute reference statistics, duration curves, or other data derived\n    from the full dataset that will be compared against selections.\n\n    Args:\n        context: ProblemContext containing raw data, features, and metadata.\n\n    Note:\n        This method should store computed state as instance attributes for\n        use in score().\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/objectives/#energy_repset.score_components.base_score_component.ScoreComponent.score","title":"score  <code>abstractmethod</code>","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute the component score for a candidate selection.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice labels forming the candidate selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar score. Lower is better for direction='min', higher is better</p> <code>float</code> <p>for direction='max'.</p> Note <p>This method is called many times during search. Precompute expensive operations in prepare() to avoid redundant calculations.</p> Source code in <code>energy_repset/score_components/base_score_component.py</code> <pre><code>@abstractmethod\ndef score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute the component score for a candidate selection.\n\n    Args:\n        combination: Tuple of slice labels forming the candidate selection.\n\n    Returns:\n        Scalar score. Lower is better for direction='min', higher is better\n        for direction='max'.\n\n    Note:\n        This method is called many times during search. Precompute expensive\n        operations in prepare() to avoid redundant calculations.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/representation/","title":"Representation Models","text":""},{"location":"api/representation/#energy_repset.representation.RepresentationModel","title":"RepresentationModel","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for representation models (Pillar R).</p> <p>Defines how selected representative periods represent the full dataset by calculating responsibility weights. The model is first fitted to learn about the entire dataset, then the weigh() method calculates weights for specific selections.</p> <p>Different models implement different weighting strategies: - Uniform: Equal weights (e.g., 365/k for yearly data) - Cluster-based: Weights proportional to cluster sizes - Blended: Soft assignment where each period is a weighted mix of representatives</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class UniformWeights(RepresentationModel):\n...     def fit(self, context: ProblemContext):\n...         self.n_total = len(context.slicer.slices)\n...\n...     def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n...         weight = self.n_total / len(combination)\n...         return {slice_id: weight for slice_id in combination}\n...\n&gt;&gt;&gt; model = UniformWeights()\n&gt;&gt;&gt; model.fit(context)\n&gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n&gt;&gt;&gt; print(weights)  # {0: 91.25, 3: 91.25, 6: 91.25, 9: 91.25} for 365 days, k=4\n</code></pre> <pre><code>&gt;&gt;&gt; class ClusterSizeWeights(RepresentationModel):\n...     def fit(self, context: ProblemContext):\n...         from sklearn.cluster import KMeans\n...         self.kmeans = KMeans(n_clusters=4)\n...         self.kmeans.fit(context.df_features)\n...\n...     def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n...         labels = self.kmeans.labels_\n...         weights = {}\n...         for i, slice_id in enumerate(combination):\n...             cluster_size = (labels == i).sum()\n...             weights[slice_id] = cluster_size\n...         return weights\n...\n&gt;&gt;&gt; model = ClusterSizeWeights()\n&gt;&gt;&gt; model.fit(context)\n&gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n&gt;&gt;&gt; print(weights)  # Weights proportional to cluster membership\n</code></pre> Source code in <code>energy_repset/representation/representation.py</code> <pre><code>class RepresentationModel(ABC):\n    \"\"\"Base class for representation models (Pillar R).\n\n    Defines how selected representative periods represent the full dataset by\n    calculating responsibility weights. The model is first fitted to learn about\n    the entire dataset, then the weigh() method calculates weights for specific\n    selections.\n\n    Different models implement different weighting strategies:\n    - Uniform: Equal weights (e.g., 365/k for yearly data)\n    - Cluster-based: Weights proportional to cluster sizes\n    - Blended: Soft assignment where each period is a weighted mix of representatives\n\n    Examples:\n        &gt;&gt;&gt; class UniformWeights(RepresentationModel):\n        ...     def fit(self, context: ProblemContext):\n        ...         self.n_total = len(context.slicer.slices)\n        ...\n        ...     def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n        ...         weight = self.n_total / len(combination)\n        ...         return {slice_id: weight for slice_id in combination}\n        ...\n        &gt;&gt;&gt; model = UniformWeights()\n        &gt;&gt;&gt; model.fit(context)\n        &gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n        &gt;&gt;&gt; print(weights)  # {0: 91.25, 3: 91.25, 6: 91.25, 9: 91.25} for 365 days, k=4\n\n        &gt;&gt;&gt; class ClusterSizeWeights(RepresentationModel):\n        ...     def fit(self, context: ProblemContext):\n        ...         from sklearn.cluster import KMeans\n        ...         self.kmeans = KMeans(n_clusters=4)\n        ...         self.kmeans.fit(context.df_features)\n        ...\n        ...     def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n        ...         labels = self.kmeans.labels_\n        ...         weights = {}\n        ...         for i, slice_id in enumerate(combination):\n        ...             cluster_size = (labels == i).sum()\n        ...             weights[slice_id] = cluster_size\n        ...         return weights\n        ...\n        &gt;&gt;&gt; model = ClusterSizeWeights()\n        &gt;&gt;&gt; model.fit(context)\n        &gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n        &gt;&gt;&gt; print(weights)  # Weights proportional to cluster membership\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, context: 'ProblemContext'):\n        \"\"\"Fit the representation model to the full dataset.\n\n        This method performs any necessary pre-computation based on the full set\n        of candidate slices (e.g., storing the feature matrix, fitting clustering\n        models, computing distance matrices).\n\n        Args:\n            context: The problem context with df_features populated. Feature\n                engineering must be run before calling this method.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def weigh(\n        self,\n        combination: SliceCombination\n    ) -&gt; Union[Dict[Hashable, float], pd.DataFrame]:\n        \"\"\"Calculate representation weights for a given selection.\n\n        This method should only be called after the model has been fitted.\n\n        Args:\n            combination: Tuple of selected slice identifiers for which to\n                calculate representation weights.\n\n        Returns:\n            The calculated weights, either as a dictionary mapping each selected\n            slice to its weight, or as a DataFrame for more complex weight\n            structures (e.g., blended models where each original period has\n            weights across multiple representatives).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.RepresentationModel.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(context: 'ProblemContext')\n</code></pre> <p>Fit the representation model to the full dataset.</p> <p>This method performs any necessary pre-computation based on the full set of candidate slices (e.g., storing the feature matrix, fitting clustering models, computing distance matrices).</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'ProblemContext'</code> <p>The problem context with df_features populated. Feature engineering must be run before calling this method.</p> required Source code in <code>energy_repset/representation/representation.py</code> <pre><code>@abstractmethod\ndef fit(self, context: 'ProblemContext'):\n    \"\"\"Fit the representation model to the full dataset.\n\n    This method performs any necessary pre-computation based on the full set\n    of candidate slices (e.g., storing the feature matrix, fitting clustering\n    models, computing distance matrices).\n\n    Args:\n        context: The problem context with df_features populated. Feature\n            engineering must be run before calling this method.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.RepresentationModel.weigh","title":"weigh  <code>abstractmethod</code>","text":"<pre><code>weigh(combination: SliceCombination) -&gt; dict[Hashable, float] | DataFrame\n</code></pre> <p>Calculate representation weights for a given selection.</p> <p>This method should only be called after the model has been fitted.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of selected slice identifiers for which to calculate representation weights.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, float] | DataFrame</code> <p>The calculated weights, either as a dictionary mapping each selected</p> <code>dict[Hashable, float] | DataFrame</code> <p>slice to its weight, or as a DataFrame for more complex weight</p> <code>dict[Hashable, float] | DataFrame</code> <p>structures (e.g., blended models where each original period has</p> <code>dict[Hashable, float] | DataFrame</code> <p>weights across multiple representatives).</p> Source code in <code>energy_repset/representation/representation.py</code> <pre><code>@abstractmethod\ndef weigh(\n    self,\n    combination: SliceCombination\n) -&gt; Union[Dict[Hashable, float], pd.DataFrame]:\n    \"\"\"Calculate representation weights for a given selection.\n\n    This method should only be called after the model has been fitted.\n\n    Args:\n        combination: Tuple of selected slice identifiers for which to\n            calculate representation weights.\n\n    Returns:\n        The calculated weights, either as a dictionary mapping each selected\n        slice to its weight, or as a DataFrame for more complex weight\n        structures (e.g., blended models where each original period has\n        weights across multiple representatives).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.UniformRepresentationModel","title":"UniformRepresentationModel","text":"<p>               Bases: <code>RepresentationModel</code></p> <p>Assigns equal weights to all selected representatives.</p> <p>The simplest representation model where each selected period gets weight 1/k. This is appropriate when you want each representative to contribute equally to downstream modeling, regardless of how many original periods it represents.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = UniformRepresentationModel()\n&gt;&gt;&gt; model.fit(context)\n&gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n&gt;&gt;&gt; print(weights)\n    {0: 0.25, 3: 0.25, 6: 0.25, 9: 0.25}\n\n&gt;&gt;&gt; # For yearly data with k=4 months, each month represents ~91 days\n&gt;&gt;&gt; # Weights sum to 1.0 for normalized analysis\n</code></pre> Source code in <code>energy_repset/representation/uniform.py</code> <pre><code>class UniformRepresentationModel(RepresentationModel):\n    \"\"\"Assigns equal weights to all selected representatives.\n\n    The simplest representation model where each selected period gets weight\n    1/k. This is appropriate when you want each representative to contribute\n    equally to downstream modeling, regardless of how many original periods\n    it represents.\n\n    Examples:\n\n        &gt;&gt;&gt; model = UniformRepresentationModel()\n        &gt;&gt;&gt; model.fit(context)\n        &gt;&gt;&gt; weights = model.weigh((0, 3, 6, 9))\n        &gt;&gt;&gt; print(weights)\n            {0: 0.25, 3: 0.25, 6: 0.25, 9: 0.25}\n\n        &gt;&gt;&gt; # For yearly data with k=4 months, each month represents ~91 days\n        &gt;&gt;&gt; # Weights sum to 1.0 for normalized analysis\n    \"\"\"\n\n    def fit(self, context: ProblemContext):\n        \"\"\"No fitting required for uniform weighting.\n\n        Args:\n            context: Problem context (unused but required by protocol).\n        \"\"\"\n        pass\n\n    def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n        \"\"\"Calculate uniform weights (1/k for each selected period).\n\n        Args:\n            combination: Tuple of selected slice identifiers.\n\n        Returns:\n            Dictionary mapping each slice ID to its weight (1/k).\n        \"\"\"\n        if not combination:\n            return {}\n        weight = 1.0 / len(combination)\n        return {label: weight for label in combination}\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.UniformRepresentationModel.fit","title":"fit","text":"<pre><code>fit(context: ProblemContext)\n</code></pre> <p>No fitting required for uniform weighting.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context (unused but required by protocol).</p> required Source code in <code>energy_repset/representation/uniform.py</code> <pre><code>def fit(self, context: ProblemContext):\n    \"\"\"No fitting required for uniform weighting.\n\n    Args:\n        context: Problem context (unused but required by protocol).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.UniformRepresentationModel.weigh","title":"weigh","text":"<pre><code>weigh(combination: SliceCombination) -&gt; dict[Hashable, float]\n</code></pre> <p>Calculate uniform weights (1/k for each selected period).</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of selected slice identifiers.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, float]</code> <p>Dictionary mapping each slice ID to its weight (1/k).</p> Source code in <code>energy_repset/representation/uniform.py</code> <pre><code>def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n    \"\"\"Calculate uniform weights (1/k for each selected period).\n\n    Args:\n        combination: Tuple of selected slice identifiers.\n\n    Returns:\n        Dictionary mapping each slice ID to its weight (1/k).\n    \"\"\"\n    if not combination:\n        return {}\n    weight = 1.0 / len(combination)\n    return {label: weight for label in combination}\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.KMedoidsClustersizeRepresentation","title":"KMedoidsClustersizeRepresentation","text":"<p>               Bases: <code>RepresentationModel</code></p> <p>Assigns weights based on k-medoids cluster sizes (hard assignment).</p> <p>This representation model performs virtual k-medoids clustering where the selected periods are enforced as medoids (cluster centers). Each candidate period is assigned to its nearest medoid, and weights are calculated as the proportion of periods assigned to each medoid.</p> <p>The weights reflect how many original periods each representative is responsible for, making this appropriate when representatives should be weighted by their \"sphere of influence\" in feature space.</p> <p>Attributes:</p> Name Type Description <code>all_features_</code> <p>Feature matrix for all candidate periods (set during fit).</p> <code>all_slice_labels_</code> <p>Labels for all candidate periods (set during fit).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = KMedoidsClustersizeRepresentation()\n&gt;&gt;&gt; model.fit(context)  # context has 12 monthly candidates\n&gt;&gt;&gt; weights = model.weigh((Period('2024-01', 'M'), Period('2024-06', 'M')))\n&gt;&gt;&gt; print(weights)\n    {Period('2024-01', 'M'): 0.583, Period('2024-06', 'M'): 0.417}\n&gt;&gt;&gt; # Jan represents 7 months, Jun represents 5 months\n</code></pre> Source code in <code>energy_repset/representation/k_medoids_clustersize.py</code> <pre><code>class KMedoidsClustersizeRepresentation(RepresentationModel):\n    \"\"\"Assigns weights based on k-medoids cluster sizes (hard assignment).\n\n    This representation model performs virtual k-medoids clustering where the\n    selected periods are enforced as medoids (cluster centers). Each candidate\n    period is assigned to its nearest medoid, and weights are calculated as\n    the proportion of periods assigned to each medoid.\n\n    The weights reflect how many original periods each representative is\n    responsible for, making this appropriate when representatives should be\n    weighted by their \"sphere of influence\" in feature space.\n\n    Attributes:\n        all_features_: Feature matrix for all candidate periods (set during fit).\n        all_slice_labels_: Labels for all candidate periods (set during fit).\n\n    Examples:\n\n        &gt;&gt;&gt; model = KMedoidsClustersizeRepresentation()\n        &gt;&gt;&gt; model.fit(context)  # context has 12 monthly candidates\n        &gt;&gt;&gt; weights = model.weigh((Period('2024-01', 'M'), Period('2024-06', 'M')))\n        &gt;&gt;&gt; print(weights)\n            {Period('2024-01', 'M'): 0.583, Period('2024-06', 'M'): 0.417}\n        &gt;&gt;&gt; # Jan represents 7 months, Jun represents 5 months\n    \"\"\"\n\n    def fit(self, context: ProblemContext):\n        \"\"\"Store the full feature matrix for later clustering.\n\n        Args:\n            context: Problem context containing df_features and candidates.\n        \"\"\"\n        self.all_features_ = context.df_features\n        self.all_slice_labels_ = context.slicer.unique_slices(context.df_raw.index)\n\n    def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n        \"\"\"Calculate weights based on cluster sizes from hard assignment.\n\n        Performs virtual k-medoids clustering where:\n        1. Selected periods are enforced as medoids\n        2. Each candidate is assigned to its nearest medoid (Euclidean distance)\n        3. Weight = (cluster size) / (total candidates)\n\n        Args:\n            combination: Tuple of selected slice identifiers.\n\n        Returns:\n            Dictionary mapping each slice ID to its weight (proportion of\n            candidates assigned to it).\n\n        Raises:\n            ValueError: If combination contains slices not in the feature matrix.\n        \"\"\"\n        if not combination:\n            return {}\n\n        # Extract feature vectors for selected medoids\n        medoid_indices = []\n        for slice_label in combination:\n            if slice_label not in self.all_slice_labels_:\n                raise ValueError(f\"Slice {slice_label} not found in candidates\")\n            medoid_indices.append(self.all_slice_labels_.index(slice_label))\n\n        medoid_features = self.all_features_.iloc[medoid_indices].values\n        all_features = self.all_features_.values\n\n        # Compute pairwise distances: shape (n_candidates, k_medoids)\n        distances = cdist(all_features, medoid_features, metric='euclidean')\n\n        # Hard assignment: each candidate assigned to nearest medoid\n        assignments = np.argmin(distances, axis=1)\n\n        # Count cluster sizes\n        cluster_sizes = np.bincount(assignments, minlength=len(combination))\n\n        # Calculate weights as proportions\n        total_candidates = len(self.all_slice_labels_)\n        weights = {\n            slice_label: float(cluster_sizes[i]) / total_candidates\n            for i, slice_label in enumerate(combination)\n        }\n\n        return weights\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.KMedoidsClustersizeRepresentation.fit","title":"fit","text":"<pre><code>fit(context: ProblemContext)\n</code></pre> <p>Store the full feature matrix for later clustering.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing df_features and candidates.</p> required Source code in <code>energy_repset/representation/k_medoids_clustersize.py</code> <pre><code>def fit(self, context: ProblemContext):\n    \"\"\"Store the full feature matrix for later clustering.\n\n    Args:\n        context: Problem context containing df_features and candidates.\n    \"\"\"\n    self.all_features_ = context.df_features\n    self.all_slice_labels_ = context.slicer.unique_slices(context.df_raw.index)\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.KMedoidsClustersizeRepresentation.weigh","title":"weigh","text":"<pre><code>weigh(combination: SliceCombination) -&gt; dict[Hashable, float]\n</code></pre> <p>Calculate weights based on cluster sizes from hard assignment.</p> <p>Performs virtual k-medoids clustering where: 1. Selected periods are enforced as medoids 2. Each candidate is assigned to its nearest medoid (Euclidean distance) 3. Weight = (cluster size) / (total candidates)</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of selected slice identifiers.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, float]</code> <p>Dictionary mapping each slice ID to its weight (proportion of</p> <code>dict[Hashable, float]</code> <p>candidates assigned to it).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If combination contains slices not in the feature matrix.</p> Source code in <code>energy_repset/representation/k_medoids_clustersize.py</code> <pre><code>def weigh(self, combination: SliceCombination) -&gt; Dict[Hashable, float]:\n    \"\"\"Calculate weights based on cluster sizes from hard assignment.\n\n    Performs virtual k-medoids clustering where:\n    1. Selected periods are enforced as medoids\n    2. Each candidate is assigned to its nearest medoid (Euclidean distance)\n    3. Weight = (cluster size) / (total candidates)\n\n    Args:\n        combination: Tuple of selected slice identifiers.\n\n    Returns:\n        Dictionary mapping each slice ID to its weight (proportion of\n        candidates assigned to it).\n\n    Raises:\n        ValueError: If combination contains slices not in the feature matrix.\n    \"\"\"\n    if not combination:\n        return {}\n\n    # Extract feature vectors for selected medoids\n    medoid_indices = []\n    for slice_label in combination:\n        if slice_label not in self.all_slice_labels_:\n            raise ValueError(f\"Slice {slice_label} not found in candidates\")\n        medoid_indices.append(self.all_slice_labels_.index(slice_label))\n\n    medoid_features = self.all_features_.iloc[medoid_indices].values\n    all_features = self.all_features_.values\n\n    # Compute pairwise distances: shape (n_candidates, k_medoids)\n    distances = cdist(all_features, medoid_features, metric='euclidean')\n\n    # Hard assignment: each candidate assigned to nearest medoid\n    assignments = np.argmin(distances, axis=1)\n\n    # Count cluster sizes\n    cluster_sizes = np.bincount(assignments, minlength=len(combination))\n\n    # Calculate weights as proportions\n    total_candidates = len(self.all_slice_labels_)\n    weights = {\n        slice_label: float(cluster_sizes[i]) / total_candidates\n        for i, slice_label in enumerate(combination)\n    }\n\n    return weights\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel","title":"BlendedRepresentationModel","text":"<p>               Bases: <code>RepresentationModel</code></p> <p>Assigns weights using a blended representation (R_soft).</p> <p>Each original slice in the full dataset is represented as a unique weighted combination of all the selected representatives. This is found by solving a small optimization problem for each original slice.</p> <p>The output is a DataFrame where rows are the original slice labels, columns are the selected representative labels, and values are the weights.</p> Source code in <code>energy_repset/representation/blended.py</code> <pre><code>class BlendedRepresentationModel(RepresentationModel):\n    \"\"\"\n    Assigns weights using a blended representation (R_soft).\n\n    Each original slice in the full dataset is represented as a unique\n    weighted combination of all the selected representatives. This is found by\n    solving a small optimization problem for each original slice.\n\n    The output is a DataFrame where rows are the original slice labels,\n    columns are the selected representative labels, and values are the weights.\n    \"\"\"\n\n    def __init__(self, blend_type: str = 'convex'):\n        \"\"\"\n        Parameters\n        ----------\n        blend_type : str, optional\n            The type of blend to perform. 'convex' is the most common,\n            ensuring weights are non-negative and sum to 1.\n            (default is 'convex')\n        \"\"\"\n        if blend_type != 'convex':\n            raise NotImplementedError(\"Only 'convex' blend type is currently supported.\")\n        self.blend_type = blend_type\n\n    def fit(self, context: 'ProblemContext'):\n        \"\"\"Stores the full feature matrix for later use.\"\"\"\n        self.all_features_ = context.df_features\n\n    def weigh(self, combination: SliceCombination) -&gt; pd.DataFrame:\n        if not combination:\n            return pd.DataFrame()\n\n        all_features = self.all_features_\n        rep_features = all_features.loc[list(combination)]\n\n        weight_results = {}\n\n        # 2. Loop through every original slice in the full dataset.\n        for original_label, original_vec in all_features.iterrows():\n            # 3. For each one, solve an optimization problem to find the best blend.\n            # Objective: minimize || original_vec - sum(weights * rep_vecs) ||^2\n            def objective_func(weights):\n                blended_vec = np.dot(weights, rep_features.values)\n                return np.sum((original_vec.values - blended_vec) ** 2)\n\n            # Initial guess: uniform weights\n            initial_weights = np.ones(len(combination)) / len(combination)\n\n            # Constraints and bounds for a convex blend\n            constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0})\n            bounds = [(0, 1) for _ in range(len(combination))]\n\n            # 4. Solve for the optimal weights for this specific original_slice.\n            result = minimize(\n                objective_func,\n                initial_weights,\n                method='SLSQP',\n                bounds=bounds,\n                constraints=constraints\n            )\n\n            weight_results[original_label] = result.x\n\n        # 5. Assemble the results into a final DataFrame and return.\n        blended_weights_df = pd.DataFrame.from_dict(\n            weight_results,\n            orient='index',\n            columns=combination\n        )\n\n        return blended_weights_df\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel.__init__","title":"__init__","text":"<pre><code>__init__(blend_type: str = 'convex')\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel.__init__--parameters","title":"Parameters","text":"<p>blend_type : str, optional     The type of blend to perform. 'convex' is the most common,     ensuring weights are non-negative and sum to 1.     (default is 'convex')</p> Source code in <code>energy_repset/representation/blended.py</code> <pre><code>def __init__(self, blend_type: str = 'convex'):\n    \"\"\"\n    Parameters\n    ----------\n    blend_type : str, optional\n        The type of blend to perform. 'convex' is the most common,\n        ensuring weights are non-negative and sum to 1.\n        (default is 'convex')\n    \"\"\"\n    if blend_type != 'convex':\n        raise NotImplementedError(\"Only 'convex' blend type is currently supported.\")\n    self.blend_type = blend_type\n</code></pre>"},{"location":"api/representation/#energy_repset.representation.BlendedRepresentationModel.fit","title":"fit","text":"<pre><code>fit(context: 'ProblemContext')\n</code></pre> <p>Stores the full feature matrix for later use.</p> Source code in <code>energy_repset/representation/blended.py</code> <pre><code>def fit(self, context: 'ProblemContext'):\n    \"\"\"Stores the full feature matrix for later use.\"\"\"\n    self.all_features_ = context.df_features\n</code></pre>"},{"location":"api/score_components/","title":"Score Components","text":""},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity","title":"WassersteinFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Measures distribution similarity using 1D Wasserstein distance per variable.</p> <p>Computes the Earth Mover's Distance between the full dataset's distribution and the selected subset's distribution for each variable. Distances are normalized by the interquartile range (IQR) to make them scale-invariant and comparable across variables.</p> <p>Lower scores indicate better distribution matching. This component is particularly effective for preserving statistical properties of the data.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights for prioritizing certain variables in the score. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Equal weights (default)\n&gt;&gt;&gt; component = WassersteinFidelity()\n&gt;&gt;&gt; component.prepare(context)\n&gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n&gt;&gt;&gt; print(f\"Wasserstein distance: {score:.3f}\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With variable-specific weights\n&gt;&gt;&gt; component = WassersteinFidelity(\n...     variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n... )\n&gt;&gt;&gt; component.prepare(context)\n&gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n&gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n</code></pre> Source code in <code>energy_repset/score_components/wasserstein_fidelity.py</code> <pre><code>class WassersteinFidelity(ScoreComponent):\n    \"\"\"Measures distribution similarity using 1D Wasserstein distance per variable.\n\n    Computes the Earth Mover's Distance between the full dataset's distribution\n    and the selected subset's distribution for each variable. Distances are\n    normalized by the interquartile range (IQR) to make them scale-invariant\n    and comparable across variables.\n\n    Lower scores indicate better distribution matching. This component is\n    particularly effective for preserving statistical properties of the data.\n\n    Args:\n        variable_weights: Optional per-variable weights for prioritizing certain\n            variables in the score. If None, all variables weighted equally (1.0).\n            If specified, missing variables get weight 0.0.\n\n    Examples:\n        &gt;&gt;&gt; # Equal weights (default)\n        &gt;&gt;&gt; component = WassersteinFidelity()\n        &gt;&gt;&gt; component.prepare(context)\n        &gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n        &gt;&gt;&gt; print(f\"Wasserstein distance: {score:.3f}\")\n\n        &gt;&gt;&gt; # With variable-specific weights\n        &gt;&gt;&gt; component = WassersteinFidelity(\n        ...     variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n        ... )\n        &gt;&gt;&gt; component.prepare(context)\n        &gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n        &gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n    \"\"\"\n\n    def __init__(self, variable_weights: Dict[str, float] | None = None) -&gt; None:\n        \"\"\"Initialize Wasserstein fidelity component.\n\n        Args:\n            variable_weights: Optional per-variable weights. If None, all\n                variables weighted equally (1.0). If specified, missing\n                variables get weight 0.0.\n        \"\"\"\n        self.name = \"wasserstein\"\n        self.direction = \"min\"\n        self._requested_weights = variable_weights\n\n        self.df: pd.DataFrame = None\n        self.labels = None\n        self.vars = None\n        self.iqr = None\n        self.variable_weights: Dict[str, float] = None\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute reference distributions and normalization factors.\n\n        Args:\n            context: Problem context with raw time-series data.\n        \"\"\"\n        df = context.df_raw.copy()\n        slicer = context.slicer\n\n        self.df = df\n        self.labels = slicer.labels_for_index(df.index)\n        self.vars = list(df.columns)\n        self.iqr = (df.quantile(0.75) - df.quantile(0.25)).replace(0, 1.0)\n\n        self.variable_weights = self._default_weight_normalization(self._requested_weights, self.vars)\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute normalized Wasserstein distance between full and selection.\n\n        Args:\n            combination: Slice identifiers forming the selection.\n\n        Returns:\n            Sum of per-variable Wasserstein distances, each normalized by IQR\n            and weighted according to variable_weights. Lower is better.\n        \"\"\"\n        sel_mask = pd.Index(self.labels).isin(combination)\n        sel = self.df.loc[sel_mask]\n        s = 0.0\n        for v in self.vars:\n            s += self.variable_weights[v] * (wasserstein_distance(self.df[v].values, sel[v].values) / float(self.iqr[v]))\n        return float(s)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity.__init__","title":"__init__","text":"<pre><code>__init__(variable_weights: Dict[str, float] | None = None) -&gt; None\n</code></pre> <p>Initialize Wasserstein fidelity component.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> Source code in <code>energy_repset/score_components/wasserstein_fidelity.py</code> <pre><code>def __init__(self, variable_weights: Dict[str, float] | None = None) -&gt; None:\n    \"\"\"Initialize Wasserstein fidelity component.\n\n    Args:\n        variable_weights: Optional per-variable weights. If None, all\n            variables weighted equally (1.0). If specified, missing\n            variables get weight 0.0.\n    \"\"\"\n    self.name = \"wasserstein\"\n    self.direction = \"min\"\n    self._requested_weights = variable_weights\n\n    self.df: pd.DataFrame = None\n    self.labels = None\n    self.vars = None\n    self.iqr = None\n    self.variable_weights: Dict[str, float] = None\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute reference distributions and normalization factors.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with raw time-series data.</p> required Source code in <code>energy_repset/score_components/wasserstein_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute reference distributions and normalization factors.\n\n    Args:\n        context: Problem context with raw time-series data.\n    \"\"\"\n    df = context.df_raw.copy()\n    slicer = context.slicer\n\n    self.df = df\n    self.labels = slicer.labels_for_index(df.index)\n    self.vars = list(df.columns)\n    self.iqr = (df.quantile(0.75) - df.quantile(0.25)).replace(0, 1.0)\n\n    self.variable_weights = self._default_weight_normalization(self._requested_weights, self.vars)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.WassersteinFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute normalized Wasserstein distance between full and selection.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sum of per-variable Wasserstein distances, each normalized by IQR</p> <code>float</code> <p>and weighted according to variable_weights. Lower is better.</p> Source code in <code>energy_repset/score_components/wasserstein_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute normalized Wasserstein distance between full and selection.\n\n    Args:\n        combination: Slice identifiers forming the selection.\n\n    Returns:\n        Sum of per-variable Wasserstein distances, each normalized by IQR\n        and weighted according to variable_weights. Lower is better.\n    \"\"\"\n    sel_mask = pd.Index(self.labels).isin(combination)\n    sel = self.df.loc[sel_mask]\n    s = 0.0\n    for v in self.vars:\n        s += self.variable_weights[v] * (wasserstein_distance(self.df[v].values, sel[v].values) / float(self.iqr[v]))\n    return float(s)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity","title":"CorrelationFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Preserves cross-variable correlation structure using Frobenius norm.</p> <p>Measures how well the selection preserves the correlation structure between variables by comparing the full dataset's correlation matrix with the selection's correlation matrix. Uses relative Frobenius norm of the difference matrix.</p> <p>Lower scores indicate better preservation of variable relationships. This component is important for downstream modeling tasks that depend on realistic co-occurrence patterns (e.g., solar and wind generation).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; component = CorrelationFidelity()\n&gt;&gt;&gt; component.prepare(context)\n&gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n&gt;&gt;&gt; print(f\"Correlation mismatch: {score:.3f}\")\n# 0.0 would be perfect preservation, 1.0+ indicates poor preservation\n</code></pre> <pre><code>&gt;&gt;&gt; # Combine with Wasserstein in an ObjectiveSet\n&gt;&gt;&gt; from energy_repset import ObjectiveSet, ObjectiveSpec\n&gt;&gt;&gt; objectives = ObjectiveSet([\n...     ObjectiveSpec('wasserstein', WassersteinFidelity(), weight=1.0),\n...     ObjectiveSpec('correlation', CorrelationFidelity(), weight=1.0)\n... ])\n</code></pre> Source code in <code>energy_repset/score_components/correlation_fidelity.py</code> <pre><code>class CorrelationFidelity(ScoreComponent):\n    \"\"\"Preserves cross-variable correlation structure using Frobenius norm.\n\n    Measures how well the selection preserves the correlation structure between\n    variables by comparing the full dataset's correlation matrix with the\n    selection's correlation matrix. Uses relative Frobenius norm of the\n    difference matrix.\n\n    Lower scores indicate better preservation of variable relationships. This\n    component is important for downstream modeling tasks that depend on\n    realistic co-occurrence patterns (e.g., solar and wind generation).\n\n    Examples:\n        &gt;&gt;&gt; component = CorrelationFidelity()\n        &gt;&gt;&gt; component.prepare(context)\n        &gt;&gt;&gt; score = component.score((0, 3, 6, 9))\n        &gt;&gt;&gt; print(f\"Correlation mismatch: {score:.3f}\")\n        # 0.0 would be perfect preservation, 1.0+ indicates poor preservation\n\n        &gt;&gt;&gt; # Combine with Wasserstein in an ObjectiveSet\n        &gt;&gt;&gt; from energy_repset import ObjectiveSet, ObjectiveSpec\n        &gt;&gt;&gt; objectives = ObjectiveSet([\n        ...     ObjectiveSpec('wasserstein', WassersteinFidelity(), weight=1.0),\n        ...     ObjectiveSpec('correlation', CorrelationFidelity(), weight=1.0)\n        ... ])\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize correlation fidelity component.\"\"\"\n        self.name = \"correlation\"\n        self.direction = \"min\"\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute full dataset's correlation matrix.\n\n        Args:\n            context: Problem context with raw time-series data.\n        \"\"\"\n        df = context.df_raw.copy()\n        slicer = context.slicer\n        self.df = df\n        self.labels = slicer.labels_for_index(df.index)\n        self.full_corr = df.corr()\n\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute relative Frobenius norm of correlation matrix difference.\n\n        Args:\n            combination: Slice identifiers forming the selection.\n\n        Returns:\n            Relative Frobenius norm ||C_full - C_sel||_F / ||C_full||_F where\n            C denotes correlation matrices. Lower is better (0 = perfect match).\n        \"\"\"\n        sel = self.df.loc[pd.Index(self.labels).isin(combination)]\n        diff = self.full_corr - sel.corr()\n        num = float(np.linalg.norm(diff.values, ord=\"fro\"))\n        den = float(np.linalg.norm(self.full_corr.values, ord=\"fro\")) + 1e-12\n        return num / den\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize correlation fidelity component.</p> Source code in <code>energy_repset/score_components/correlation_fidelity.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize correlation fidelity component.\"\"\"\n    self.name = \"correlation\"\n    self.direction = \"min\"\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute full dataset's correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with raw time-series data.</p> required Source code in <code>energy_repset/score_components/correlation_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute full dataset's correlation matrix.\n\n    Args:\n        context: Problem context with raw time-series data.\n    \"\"\"\n    df = context.df_raw.copy()\n    slicer = context.slicer\n    self.df = df\n    self.labels = slicer.labels_for_index(df.index)\n    self.full_corr = df.corr()\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CorrelationFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute relative Frobenius norm of correlation matrix difference.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Relative Frobenius norm ||C_full - C_sel||_F / ||C_full||_F where</p> <code>float</code> <p>C denotes correlation matrices. Lower is better (0 = perfect match).</p> Source code in <code>energy_repset/score_components/correlation_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute relative Frobenius norm of correlation matrix difference.\n\n    Args:\n        combination: Slice identifiers forming the selection.\n\n    Returns:\n        Relative Frobenius norm ||C_full - C_sel||_F / ||C_full||_F where\n        C denotes correlation matrices. Lower is better (0 = perfect match).\n    \"\"\"\n    sel = self.df.loc[pd.Index(self.labels).isin(combination)]\n    diff = self.full_corr - sel.corr()\n    num = float(np.linalg.norm(diff.values, ord=\"fro\"))\n    den = float(np.linalg.norm(self.full_corr.values, ord=\"fro\")) + 1e-12\n    return num / den\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity","title":"DiurnalFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Measures how well the selection preserves hourly (diurnal) patterns.</p> <p>Compares the mean hourly profiles between the full dataset and the selected subset. This is useful for applications where intraday patterns matter (e.g., electricity demand profiles, solar generation curves).</p> <p>The score is the normalized mean squared error between the full and selected hour-of-day profiles, averaged across all variables and hours.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DiurnalFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add diurnal fidelity to your objective set\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'diurnal': (1.0, DiurnalFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # For hourly data, this ensures selected periods\n&gt;&gt;&gt; # preserve the typical daily load shape\n</code></pre> Source code in <code>energy_repset/score_components/diurnal_fidelity.py</code> <pre><code>class DiurnalFidelity(ScoreComponent):\n    \"\"\"Measures how well the selection preserves hourly (diurnal) patterns.\n\n    Compares the mean hourly profiles between the full dataset and the\n    selected subset. This is useful for applications where intraday patterns\n    matter (e.g., electricity demand profiles, solar generation curves).\n\n    The score is the normalized mean squared error between the full and\n    selected hour-of-day profiles, averaged across all variables and hours.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import DiurnalFidelity\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Add diurnal fidelity to your objective set\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'diurnal': (1.0, DiurnalFidelity())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # For hourly data, this ensures selected periods\n        &gt;&gt;&gt; # preserve the typical daily load shape\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize diurnal fidelity component.\"\"\"\n        self.name = \"diurnal\"\n        self.direction = \"min\"\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute the full dataset's mean hourly profile.\n\n        Args:\n            context: Problem context containing raw time-series data.\n        \"\"\"\n        df = context.df_raw.copy()\n        slicer = context.slicer\n        self.df = df\n        self.labels = slicer.labels_for_index(df.index)\n        self.full = df.groupby(df.index.hour).mean(numeric_only=True)\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute normalized MSE between full and selection diurnal profiles.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Normalized mean squared error across all variables and hours.\n            Lower values indicate better preservation of diurnal patterns.\n        \"\"\"\n        sel = self.df.loc[pd.Index(self.labels).isin(combination)]\n        sub = sel.groupby(sel.index.hour).mean(numeric_only=True)\n        a, b = self.full.align(sub, join=\"inner\", axis=0)\n        num = float(((a - b).pow(2)).mean().mean())\n        den = float(a.pow(2).mean().mean()) + 1e-12\n        return num / den\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize diurnal fidelity component.</p> Source code in <code>energy_repset/score_components/diurnal_fidelity.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize diurnal fidelity component.\"\"\"\n    self.name = \"diurnal\"\n    self.direction = \"min\"\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute the full dataset's mean hourly profile.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required Source code in <code>energy_repset/score_components/diurnal_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute the full dataset's mean hourly profile.\n\n    Args:\n        context: Problem context containing raw time-series data.\n    \"\"\"\n    df = context.df_raw.copy()\n    slicer = context.slicer\n    self.df = df\n    self.labels = slicer.labels_for_index(df.index)\n    self.full = df.groupby(df.index.hour).mean(numeric_only=True)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute normalized MSE between full and selection diurnal profiles.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Normalized mean squared error across all variables and hours.</p> <code>float</code> <p>Lower values indicate better preservation of diurnal patterns.</p> Source code in <code>energy_repset/score_components/diurnal_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute normalized MSE between full and selection diurnal profiles.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Normalized mean squared error across all variables and hours.\n        Lower values indicate better preservation of diurnal patterns.\n    \"\"\"\n    sel = self.df.loc[pd.Index(self.labels).isin(combination)]\n    sub = sel.groupby(sel.index.hour).mean(numeric_only=True)\n    a, b = self.full.align(sub, join=\"inner\", axis=0)\n    num = float(((a - b).pow(2)).mean().mean())\n    den = float(a.pow(2).mean().mean()) + 1e-12\n    return num / den\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity","title":"DurationCurveFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Matches duration curves using quantile approximation and IQR normalization.</p> <p>Measures how well the selection preserves the statistical distribution of each variable by comparing quantiles of the full and selected data. This is more computationally efficient than NRMSEFidelity for large datasets since it compares a fixed number of quantiles rather than full sorted arrays.</p> <p>Uses IQR (interquartile range) normalization instead of mean normalization, making it more robust to outliers.</p> <p>Parameters:</p> Name Type Description Default <code>n_quantiles</code> <code>int</code> <p>Number of quantiles to compute for duration curve approximation. Default is 101 (0%, 1%, ..., 100%).</p> <code>101</code> <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights for prioritizing certain variables in the score. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DurationCurveFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: 101 quantiles (0%, 1%, ..., 100%)\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'duration': (1.0, DurationCurveFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Coarser approximation for faster computation\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'duration': (1.0, DurationCurveFidelity(n_quantiles=21))\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With variable weights for prioritizing specific variables\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'duration': (1.0, DurationCurveFidelity(\n...         n_quantiles=101,\n...         variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n...     ))\n... })\n&gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n</code></pre> Source code in <code>energy_repset/score_components/duration_curve_fidelity.py</code> <pre><code>class DurationCurveFidelity(ScoreComponent):\n    \"\"\"Matches duration curves using quantile approximation and IQR normalization.\n\n    Measures how well the selection preserves the statistical distribution\n    of each variable by comparing quantiles of the full and selected data.\n    This is more computationally efficient than NRMSEFidelity for large\n    datasets since it compares a fixed number of quantiles rather than\n    full sorted arrays.\n\n    Uses IQR (interquartile range) normalization instead of mean normalization,\n    making it more robust to outliers.\n\n    Args:\n        n_quantiles: Number of quantiles to compute for duration curve\n            approximation. Default is 101 (0%, 1%, ..., 100%).\n        variable_weights: Optional per-variable weights for prioritizing certain\n            variables in the score. If None, all variables weighted equally (1.0).\n            If specified, missing variables get weight 0.0.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import DurationCurveFidelity\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Default: 101 quantiles (0%, 1%, ..., 100%)\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'duration': (1.0, DurationCurveFidelity())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Coarser approximation for faster computation\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'duration': (1.0, DurationCurveFidelity(n_quantiles=21))\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With variable weights for prioritizing specific variables\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'duration': (1.0, DurationCurveFidelity(\n        ...         n_quantiles=101,\n        ...         variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n        ...     ))\n        ... })\n        &gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n    \"\"\"\n\n    def __init__(\n        self,\n        n_quantiles: int = 101,\n        variable_weights: Dict[str, float] | None = None\n    ) -&gt; None:\n        \"\"\"Initialize duration curve fidelity component.\n\n        Args:\n            n_quantiles: Number of quantiles for duration curve approximation.\n            variable_weights: Optional per-variable weights. If None, all\n                variables weighted equally (1.0). If specified, missing\n                variables get weight 0.0.\n        \"\"\"\n        self.name = \"nrmse_duration_curve\"\n        self.direction = \"min\"\n        self.n_quantiles = n_quantiles\n        self._requested_weights = variable_weights\n        self.variable_weights: Dict[str, float] = None\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute quantiles and normalization factors for full dataset.\n\n        Args:\n            context: Problem context containing raw time-series data.\n        \"\"\"\n        df = context.df_raw\n        self.df = df\n        self.labels = context.slicer.labels_for_index(df.index)\n        self.vars = list(df.columns)\n\n        self.variable_weights = self._default_weight_normalization(self._requested_weights, self.vars)\n\n        self.quantiles = np.linspace(0, 1, self.n_quantiles)\n        self.full_quantiles = self.df.quantile(self.quantiles)\n        self.iqr = (df.quantile(0.75) - df.quantile(0.25)).replace(0, 1.0)\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute sum of per-variable NRMSE for quantile-based duration curves.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Weighted sum of per-variable NRMSE values using IQR normalization.\n            Returns infinity if the selection is empty.\n        \"\"\"\n        sel_mask = pd.Index(self.labels).isin(combination)\n        if not sel_mask.any():\n            return np.inf\n        sel = self.df.loc[sel_mask]\n\n        sel_quantiles = sel.quantile(self.quantiles)\n\n        total_nrmse = 0.0\n        for v in self.vars:\n            squared_errors = (self.full_quantiles[v].values - sel_quantiles[v].values) ** 2\n            rmse = np.sqrt(squared_errors.mean())\n            total_nrmse += self.variable_weights[v] * (rmse / float(self.iqr[v]))\n\n        return float(total_nrmse)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity.__init__","title":"__init__","text":"<pre><code>__init__(n_quantiles: int = 101, variable_weights: Dict[str, float] | None = None) -&gt; None\n</code></pre> <p>Initialize duration curve fidelity component.</p> <p>Parameters:</p> Name Type Description Default <code>n_quantiles</code> <code>int</code> <p>Number of quantiles for duration curve approximation.</p> <code>101</code> <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> Source code in <code>energy_repset/score_components/duration_curve_fidelity.py</code> <pre><code>def __init__(\n    self,\n    n_quantiles: int = 101,\n    variable_weights: Dict[str, float] | None = None\n) -&gt; None:\n    \"\"\"Initialize duration curve fidelity component.\n\n    Args:\n        n_quantiles: Number of quantiles for duration curve approximation.\n        variable_weights: Optional per-variable weights. If None, all\n            variables weighted equally (1.0). If specified, missing\n            variables get weight 0.0.\n    \"\"\"\n    self.name = \"nrmse_duration_curve\"\n    self.direction = \"min\"\n    self.n_quantiles = n_quantiles\n    self._requested_weights = variable_weights\n    self.variable_weights: Dict[str, float] = None\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute quantiles and normalization factors for full dataset.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required Source code in <code>energy_repset/score_components/duration_curve_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute quantiles and normalization factors for full dataset.\n\n    Args:\n        context: Problem context containing raw time-series data.\n    \"\"\"\n    df = context.df_raw\n    self.df = df\n    self.labels = context.slicer.labels_for_index(df.index)\n    self.vars = list(df.columns)\n\n    self.variable_weights = self._default_weight_normalization(self._requested_weights, self.vars)\n\n    self.quantiles = np.linspace(0, 1, self.n_quantiles)\n    self.full_quantiles = self.df.quantile(self.quantiles)\n    self.iqr = (df.quantile(0.75) - df.quantile(0.25)).replace(0, 1.0)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DurationCurveFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute sum of per-variable NRMSE for quantile-based duration curves.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Weighted sum of per-variable NRMSE values using IQR normalization.</p> <code>float</code> <p>Returns infinity if the selection is empty.</p> Source code in <code>energy_repset/score_components/duration_curve_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute sum of per-variable NRMSE for quantile-based duration curves.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Weighted sum of per-variable NRMSE values using IQR normalization.\n        Returns infinity if the selection is empty.\n    \"\"\"\n    sel_mask = pd.Index(self.labels).isin(combination)\n    if not sel_mask.any():\n        return np.inf\n    sel = self.df.loc[sel_mask]\n\n    sel_quantiles = sel.quantile(self.quantiles)\n\n    total_nrmse = 0.0\n    for v in self.vars:\n        squared_errors = (self.full_quantiles[v].values - sel_quantiles[v].values) ** 2\n        rmse = np.sqrt(squared_errors.mean())\n        total_nrmse += self.variable_weights[v] * (rmse / float(self.iqr[v]))\n\n    return float(total_nrmse)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity","title":"NRMSEFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Matches duration curves using interpolation and NRMSE.</p> <p>Measures how well the selection preserves the statistical distribution of each variable by comparing full and selected duration curves (sorted value profiles). The selection's duration curve is interpolated to match the full curve's length, then NRMSE is computed.</p> <p>This approach uses the full sorted arrays and is accurate but can be computationally expensive for very large datasets. For efficiency with large data, consider DurationCurveFidelity which uses quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights for prioritizing certain variables in the score. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import NRMSEFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic usage with equal variable weights\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'nrmse': (1.0, NRMSEFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prioritize specific variables\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'nrmse': (1.0, NRMSEFidelity(\n...         variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n...     ))\n... })\n&gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n</code></pre> Source code in <code>energy_repset/score_components/nrmse_fidelity.py</code> <pre><code>class NRMSEFidelity(ScoreComponent):\n    \"\"\"Matches duration curves using interpolation and NRMSE.\n\n    Measures how well the selection preserves the statistical distribution\n    of each variable by comparing full and selected duration curves (sorted\n    value profiles). The selection's duration curve is interpolated to match\n    the full curve's length, then NRMSE is computed.\n\n    This approach uses the full sorted arrays and is accurate but can be\n    computationally expensive for very large datasets. For efficiency with\n    large data, consider DurationCurveFidelity which uses quantiles.\n\n    Args:\n        variable_weights: Optional per-variable weights for prioritizing certain\n            variables in the score. If None, all variables weighted equally (1.0).\n            If specified, missing variables get weight 0.0.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import NRMSEFidelity\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Basic usage with equal variable weights\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'nrmse': (1.0, NRMSEFidelity())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Prioritize specific variables\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'nrmse': (1.0, NRMSEFidelity(\n        ...         variable_weights={'demand': 2.0, 'solar': 1.0, 'wind': 0.5}\n        ...     ))\n        ... })\n        &gt;&gt;&gt; # demand has 2x impact, solar 1x, wind 0.5x, other variables 0x\n    \"\"\"\n\n    def __init__(self, variable_weights: Dict[str, float] | None = None) -&gt; None:\n        \"\"\"Initialize NRMSE fidelity component.\n\n        Args:\n            variable_weights: Optional per-variable weights. If None, all\n                variables weighted equally (1.0). If specified, missing\n                variables get weight 0.0.\n        \"\"\"\n        self.name = \"nrmse\"\n        self.direction = \"min\"\n        self._requested_weights = variable_weights\n        self.variable_weights: Dict[str, float] = None\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute full duration curves and normalization factors.\n\n        Args:\n            context: Problem context containing raw time-series data.\n        \"\"\"\n        df = context.df_raw\n        self.df = df\n        self.labels = context.slicer.labels_for_index(df.index)\n        self.vars = list(df.columns)\n\n        self.variable_weights = self._default_weight_normalization(self._requested_weights, self.vars)\n\n        self.full_curves = {\n            v: np.sort(df[v].values)[::-1] for v in self.vars\n        }\n        self.full_means = {\n            v: np.mean(df[v].values) for v in self.vars\n        }\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute sum of per-variable NRMSE for duration curves.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Weighted sum of per-variable NRMSE values. Returns infinity\n            if the selection is empty.\n        \"\"\"\n        sel_mask = pd.Index(self.labels).isin(combination)\n        if not sel_mask.any():\n            return np.inf\n\n        sel = self.df.loc[sel_mask]\n        s = 0.0\n\n        for v in self.vars:\n            full_curve = self.full_curves[v]\n            sel_curve = np.sort(sel[v].values)[::-1]\n\n            if len(sel_curve) == 0:\n                continue\n\n            # Interpolate selection's duration curve to match full length\n            x_full = np.linspace(0, 1, len(full_curve))\n            x_sel = np.linspace(0, 1, len(sel_curve))\n            resampled_sel_curve = np.interp(x_full, x_sel, sel_curve)\n\n            # Calculate RMSE\n            mse = np.mean((full_curve - resampled_sel_curve) ** 2)\n            rmse = np.sqrt(mse)\n\n            # Normalize by mean\n            mean_val = self.full_means[v]\n            nrmse = rmse / (mean_val + 1e-12)\n\n            s += self.variable_weights[v] * nrmse\n\n        return float(s)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity.__init__","title":"__init__","text":"<pre><code>__init__(variable_weights: Dict[str, float] | None = None) -&gt; None\n</code></pre> <p>Initialize NRMSE fidelity component.</p> <p>Parameters:</p> Name Type Description Default <code>variable_weights</code> <code>Dict[str, float] | None</code> <p>Optional per-variable weights. If None, all variables weighted equally (1.0). If specified, missing variables get weight 0.0.</p> <code>None</code> Source code in <code>energy_repset/score_components/nrmse_fidelity.py</code> <pre><code>def __init__(self, variable_weights: Dict[str, float] | None = None) -&gt; None:\n    \"\"\"Initialize NRMSE fidelity component.\n\n    Args:\n        variable_weights: Optional per-variable weights. If None, all\n            variables weighted equally (1.0). If specified, missing\n            variables get weight 0.0.\n    \"\"\"\n    self.name = \"nrmse\"\n    self.direction = \"min\"\n    self._requested_weights = variable_weights\n    self.variable_weights: Dict[str, float] = None\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute full duration curves and normalization factors.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required Source code in <code>energy_repset/score_components/nrmse_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute full duration curves and normalization factors.\n\n    Args:\n        context: Problem context containing raw time-series data.\n    \"\"\"\n    df = context.df_raw\n    self.df = df\n    self.labels = context.slicer.labels_for_index(df.index)\n    self.vars = list(df.columns)\n\n    self.variable_weights = self._default_weight_normalization(self._requested_weights, self.vars)\n\n    self.full_curves = {\n        v: np.sort(df[v].values)[::-1] for v in self.vars\n    }\n    self.full_means = {\n        v: np.mean(df[v].values) for v in self.vars\n    }\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.NRMSEFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute sum of per-variable NRMSE for duration curves.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Weighted sum of per-variable NRMSE values. Returns infinity</p> <code>float</code> <p>if the selection is empty.</p> Source code in <code>energy_repset/score_components/nrmse_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute sum of per-variable NRMSE for duration curves.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Weighted sum of per-variable NRMSE values. Returns infinity\n        if the selection is empty.\n    \"\"\"\n    sel_mask = pd.Index(self.labels).isin(combination)\n    if not sel_mask.any():\n        return np.inf\n\n    sel = self.df.loc[sel_mask]\n    s = 0.0\n\n    for v in self.vars:\n        full_curve = self.full_curves[v]\n        sel_curve = np.sort(sel[v].values)[::-1]\n\n        if len(sel_curve) == 0:\n            continue\n\n        # Interpolate selection's duration curve to match full length\n        x_full = np.linspace(0, 1, len(full_curve))\n        x_sel = np.linspace(0, 1, len(sel_curve))\n        resampled_sel_curve = np.interp(x_full, x_sel, sel_curve)\n\n        # Calculate RMSE\n        mse = np.mean((full_curve - resampled_sel_curve) ** 2)\n        rmse = np.sqrt(mse)\n\n        # Normalize by mean\n        mean_val = self.full_means[v]\n        nrmse = rmse / (mean_val + 1e-12)\n\n        s += self.variable_weights[v] * nrmse\n\n    return float(s)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity","title":"DTWFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Measures representation quality using Dynamic Time Warping distance.</p> <p>Computes the average DTW distance from each unselected slice to its nearest representative in the selection. This is analogous to inertia in k-medoids clustering but uses DTW instead of Euclidean distance.</p> <p>DTW allows temporal alignment, making it suitable for time-series where similar patterns may be shifted in time (e.g., seasonal load profiles with varying peak times).</p> <p>Requires the <code>tslearn</code> package for DTW computation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DTWFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use DTW for time-series with temporal shifts\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'dtw': (1.0, DTWFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Good for multi-day periods with similar but shifted patterns\n&gt;&gt;&gt; # e.g., weeks with similar load but peak occurring at different times\n</code></pre> Note <p>This requires <code>tslearn</code> to be installed:     pip install tslearn</p> Source code in <code>energy_repset/score_components/dtw_fidelity.py</code> <pre><code>class DTWFidelity(ScoreComponent):\n    \"\"\"Measures representation quality using Dynamic Time Warping distance.\n\n    Computes the average DTW distance from each unselected slice to its\n    nearest representative in the selection. This is analogous to inertia\n    in k-medoids clustering but uses DTW instead of Euclidean distance.\n\n    DTW allows temporal alignment, making it suitable for time-series where\n    similar patterns may be shifted in time (e.g., seasonal load profiles\n    with varying peak times).\n\n    Requires the `tslearn` package for DTW computation.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import DTWFidelity\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use DTW for time-series with temporal shifts\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'dtw': (1.0, DTWFidelity())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Good for multi-day periods with similar but shifted patterns\n        &gt;&gt;&gt; # e.g., weeks with similar load but peak occurring at different times\n\n    Note:\n        This requires `tslearn` to be installed:\n            pip install tslearn\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize DTW fidelity component.\"\"\"\n        self.name = \"dtw\"\n        self.direction = \"min\"\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute per-slice time-series data.\n\n        Args:\n            context: Problem context containing raw time-series data.\n        \"\"\"\n        df = context.df_raw\n        self.slices = {\n            label: group.values\n            for label, group in df.groupby(context.slicer.labels_for_index(df.index))\n        }\n        self.all_labels = set(self.slices.keys())\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute average DTW distance from unselected to selected slices.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Average DTW distance from each unselected slice to its nearest\n            representative. Returns 0.0 if all slices are selected or\n            none are selected.\n\n        Raises:\n            ImportError: If tslearn is not installed.\n        \"\"\"\n        from tslearn.metrics import dtw\n        selected_labels = set(combination)\n        unselected_labels = self.all_labels - selected_labels\n\n        if not selected_labels or not unselected_labels:\n            return 0.0\n\n        selected_series = [self.slices[lbl] for lbl in selected_labels]\n        total_dist = 0.0\n\n        for lbl in unselected_labels:\n            unselected_series = self.slices[lbl]\n            min_dist = np.inf\n            for sel_series in selected_series:\n                dist = dtw(unselected_series, sel_series)\n                if dist &lt; min_dist:\n                    min_dist = dist\n            total_dist += min_dist\n\n        return total_dist / len(unselected_labels)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize DTW fidelity component.</p> Source code in <code>energy_repset/score_components/dtw_fidelity.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize DTW fidelity component.\"\"\"\n    self.name = \"dtw\"\n    self.direction = \"min\"\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute per-slice time-series data.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required Source code in <code>energy_repset/score_components/dtw_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute per-slice time-series data.\n\n    Args:\n        context: Problem context containing raw time-series data.\n    \"\"\"\n    df = context.df_raw\n    self.slices = {\n        label: group.values\n        for label, group in df.groupby(context.slicer.labels_for_index(df.index))\n    }\n    self.all_labels = set(self.slices.keys())\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DTWFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute average DTW distance from unselected to selected slices.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average DTW distance from each unselected slice to its nearest</p> <code>float</code> <p>representative. Returns 0.0 if all slices are selected or</p> <code>float</code> <p>none are selected.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If tslearn is not installed.</p> Source code in <code>energy_repset/score_components/dtw_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute average DTW distance from unselected to selected slices.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Average DTW distance from each unselected slice to its nearest\n        representative. Returns 0.0 if all slices are selected or\n        none are selected.\n\n    Raises:\n        ImportError: If tslearn is not installed.\n    \"\"\"\n    from tslearn.metrics import dtw\n    selected_labels = set(combination)\n    unselected_labels = self.all_labels - selected_labels\n\n    if not selected_labels or not unselected_labels:\n        return 0.0\n\n    selected_series = [self.slices[lbl] for lbl in selected_labels]\n    total_dist = 0.0\n\n    for lbl in unselected_labels:\n        unselected_series = self.slices[lbl]\n        min_dist = np.inf\n        for sel_series in selected_series:\n            dist = dtw(unselected_series, sel_series)\n            if dist &lt; min_dist:\n                min_dist = dist\n        total_dist += min_dist\n\n    return total_dist / len(unselected_labels)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity","title":"DiurnalDTWFidelity","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Preserves hourly patterns using Dynamic Time Warping on diurnal profiles.</p> <p>Combines the concepts of DiurnalFidelity and DTWFidelity: compares hour-of-day aggregated profiles between full and selected data, but uses DTW distance instead of MSE to allow for temporal flexibility.</p> <p>This is useful when you want to preserve the general shape of hourly patterns but allow for some temporal shifting (e.g., load profiles with similar shapes but shifted peak hours).</p> <p>Uses a custom DTW implementation (no external dependencies), normalized by the standard deviation of the full diurnal profile.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DiurnalDTWFidelity\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Preserve diurnal patterns with temporal flexibility\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'diurnal_dtw': (1.0, DiurnalDTWFidelity())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Useful when hourly patterns are important but exact timing\n&gt;&gt;&gt; # alignment is not critical (e.g., shifted daily load curves)\n</code></pre> Note <p>Unlike DTWFidelity, this does not require tslearn since it uses a custom DTW implementation on aggregated hourly profiles.</p> Source code in <code>energy_repset/score_components/diurnal_dtw_fidelity.py</code> <pre><code>class DiurnalDTWFidelity(ScoreComponent):\n    \"\"\"Preserves hourly patterns using Dynamic Time Warping on diurnal profiles.\n\n    Combines the concepts of DiurnalFidelity and DTWFidelity: compares\n    hour-of-day aggregated profiles between full and selected data, but\n    uses DTW distance instead of MSE to allow for temporal flexibility.\n\n    This is useful when you want to preserve the general shape of hourly\n    patterns but allow for some temporal shifting (e.g., load profiles\n    with similar shapes but shifted peak hours).\n\n    Uses a custom DTW implementation (no external dependencies), normalized\n    by the standard deviation of the full diurnal profile.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import DiurnalDTWFidelity\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Preserve diurnal patterns with temporal flexibility\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'diurnal_dtw': (1.0, DiurnalDTWFidelity())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Useful when hourly patterns are important but exact timing\n        &gt;&gt;&gt; # alignment is not critical (e.g., shifted daily load curves)\n\n    Note:\n        Unlike DTWFidelity, this does not require tslearn since it uses\n        a custom DTW implementation on aggregated hourly profiles.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize diurnal DTW fidelity component.\"\"\"\n        self.name = \"diurnal_dtw\"\n        self.direction = \"min\"\n\n    def _dtw_distance(self, s1: np.ndarray, s2: np.ndarray) -&gt; float:\n        \"\"\"Compute Dynamic Time Warping distance between two 1D arrays.\n\n        Args:\n            s1: First time series.\n            s2: Second time series.\n\n        Returns:\n            DTW distance between s1 and s2.\n        \"\"\"\n        n, m = len(s1), len(s2)\n        dtw_matrix = np.full((n + 1, m + 1), np.inf)\n        dtw_matrix[0, 0] = 0.0\n\n        for i in range(1, n + 1):\n            for j in range(1, m + 1):\n                cost = abs(s1[i - 1] - s2[j - 1])\n                last_min = min(dtw_matrix[i - 1, j], dtw_matrix[i, j - 1], dtw_matrix[i - 1, j - 1])\n                dtw_matrix[i, j] = cost + last_min\n\n        return dtw_matrix[n, m]\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Precompute full dataset's diurnal profile and normalization factors.\n\n        Args:\n            context: Problem context containing raw time-series data.\n        \"\"\"\n        df = context.df_raw\n        self.df = df\n        self.labels = context.slicer.labels_for_index(df.index)\n        self.vars = list(df.columns)\n        self.full_diurnal = df.groupby(df.index.hour).mean(numeric_only=True)\n        self.norm_factor = self.full_diurnal.std().replace(0, 1.0)\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute sum of per-variable normalized DTW distances for diurnal profiles.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Sum of per-variable DTW distances between full and selected\n            diurnal profiles, normalized by standard deviation. Returns\n            infinity if the selection is empty.\n        \"\"\"\n        sel_mask = pd.Index(self.labels).isin(combination)\n        if not sel_mask.any():\n            return np.inf\n\n        sel = self.df.loc[sel_mask]\n        sel_diurnal = sel.groupby(sel.index.hour).mean(numeric_only=True)\n\n        full_aligned, sel_aligned = self.full_diurnal.align(sel_diurnal, join=\"inner\", axis=0)\n        if full_aligned.empty:\n            return np.inf\n\n        total_dtw_dist = 0.0\n        for v in self.vars:\n            if v in full_aligned and v in sel_aligned:\n                full_profile = full_aligned[v].values\n                sel_profile = sel_aligned[v].values\n                dist = self._dtw_distance(full_profile, sel_profile)\n                total_dtw_dist += dist / float(self.norm_factor[v])\n\n        return float(total_dtw_dist)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize diurnal DTW fidelity component.</p> Source code in <code>energy_repset/score_components/diurnal_dtw_fidelity.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize diurnal DTW fidelity component.\"\"\"\n    self.name = \"diurnal_dtw\"\n    self.direction = \"min\"\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Precompute full dataset's diurnal profile and normalization factors.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context containing raw time-series data.</p> required Source code in <code>energy_repset/score_components/diurnal_dtw_fidelity.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Precompute full dataset's diurnal profile and normalization factors.\n\n    Args:\n        context: Problem context containing raw time-series data.\n    \"\"\"\n    df = context.df_raw\n    self.df = df\n    self.labels = context.slicer.labels_for_index(df.index)\n    self.vars = list(df.columns)\n    self.full_diurnal = df.groupby(df.index.hour).mean(numeric_only=True)\n    self.norm_factor = self.full_diurnal.std().replace(0, 1.0)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiurnalDTWFidelity.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute sum of per-variable normalized DTW distances for diurnal profiles.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sum of per-variable DTW distances between full and selected</p> <code>float</code> <p>diurnal profiles, normalized by standard deviation. Returns</p> <code>float</code> <p>infinity if the selection is empty.</p> Source code in <code>energy_repset/score_components/diurnal_dtw_fidelity.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute sum of per-variable normalized DTW distances for diurnal profiles.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Sum of per-variable DTW distances between full and selected\n        diurnal profiles, normalized by standard deviation. Returns\n        infinity if the selection is empty.\n    \"\"\"\n    sel_mask = pd.Index(self.labels).isin(combination)\n    if not sel_mask.any():\n        return np.inf\n\n    sel = self.df.loc[sel_mask]\n    sel_diurnal = sel.groupby(sel.index.hour).mean(numeric_only=True)\n\n    full_aligned, sel_aligned = self.full_diurnal.align(sel_diurnal, join=\"inner\", axis=0)\n    if full_aligned.empty:\n        return np.inf\n\n    total_dtw_dist = 0.0\n    for v in self.vars:\n        if v in full_aligned and v in sel_aligned:\n            full_profile = full_aligned[v].values\n            sel_profile = sel_aligned[v].values\n            dist = self._dtw_distance(full_profile, sel_profile)\n            total_dtw_dist += dist / float(self.norm_factor[v])\n\n    return float(total_dtw_dist)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward","title":"DiversityReward","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Rewards selections with diverse, mutually distant representative periods.</p> <p>Computes the average pairwise Euclidean distance between selected slice features in feature space. Higher diversity can help ensure the selection covers a wider range of conditions, avoiding redundant representatives.</p> <p>This is particularly useful when combined with fidelity objectives to balance accuracy with coverage.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import DiversityReward\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Encourage diverse representatives\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'diversity': (0.3, DiversityReward())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Combine with fidelity for balanced selection\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'fidelity': (1.0, WassersteinFidelity()),\n...     'diversity': (0.2, DiversityReward())\n... })\n</code></pre> Source code in <code>energy_repset/score_components/diversity_reward.py</code> <pre><code>class DiversityReward(ScoreComponent):\n    \"\"\"Rewards selections with diverse, mutually distant representative periods.\n\n    Computes the average pairwise Euclidean distance between selected slice\n    features in feature space. Higher diversity can help ensure the selection\n    covers a wider range of conditions, avoiding redundant representatives.\n\n    This is particularly useful when combined with fidelity objectives to\n    balance accuracy with coverage.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import DiversityReward\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Encourage diverse representatives\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'diversity': (0.3, DiversityReward())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Combine with fidelity for balanced selection\n        &gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'fidelity': (1.0, WassersteinFidelity()),\n        ...     'diversity': (0.2, DiversityReward())\n        ... })\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize diversity reward component.\"\"\"\n        self.name = \"diversity\"\n        self.direction = \"max\"\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Store the feature matrix for pairwise distance computation.\n\n        Args:\n            context: Problem context with computed features.\n        \"\"\"\n        self.features = context.df_features.copy()\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute mean pairwise Euclidean distance among selected features.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Average pairwise distance in feature space. Returns 0.0 if\n            fewer than two slices are selected.\n        \"\"\"\n        X = self.features.loc[list(combination)].values\n        if X.shape[0] &lt; 2:\n            return 0.0\n        n = X.shape[0]\n        dsum = 0.0\n        cnt = 0\n        for i in range(n):\n            for j in range(i + 1, n):\n                dsum += float(np.linalg.norm(X[i] - X[j]))\n                cnt += 1\n        return dsum / cnt\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize diversity reward component.</p> Source code in <code>energy_repset/score_components/diversity_reward.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize diversity reward component.\"\"\"\n    self.name = \"diversity\"\n    self.direction = \"max\"\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Store the feature matrix for pairwise distance computation.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with computed features.</p> required Source code in <code>energy_repset/score_components/diversity_reward.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Store the feature matrix for pairwise distance computation.\n\n    Args:\n        context: Problem context with computed features.\n    \"\"\"\n    self.features = context.df_features.copy()\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.DiversityReward.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute mean pairwise Euclidean distance among selected features.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average pairwise distance in feature space. Returns 0.0 if</p> <code>float</code> <p>fewer than two slices are selected.</p> Source code in <code>energy_repset/score_components/diversity_reward.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute mean pairwise Euclidean distance among selected features.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Average pairwise distance in feature space. Returns 0.0 if\n        fewer than two slices are selected.\n    \"\"\"\n    X = self.features.loc[list(combination)].values\n    if X.shape[0] &lt; 2:\n        return 0.0\n    n = X.shape[0]\n    dsum = 0.0\n    cnt = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            dsum += float(np.linalg.norm(X[i] - X[j]))\n            cnt += 1\n    return dsum / cnt\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance","title":"CentroidBalance","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Penalizes selections whose centroid deviates from the global center.</p> <p>Computes the Euclidean distance between the centroid of selected slice features and the origin (global center in standardized feature space).</p> <p>This objective ensures the selection doesn't systematically bias toward extreme conditions, maintaining balance around typical conditions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import CentroidBalance\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Penalize selections biased toward extreme periods\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'balance': (0.5, CentroidBalance())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Used in examples/ex2_feature_space.py to maintain balanced selections\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'fidelity': (1.0, WassersteinFidelity()),\n...     'balance': (0.3, CentroidBalance())\n... })\n</code></pre> Source code in <code>energy_repset/score_components/centroid_balance.py</code> <pre><code>class CentroidBalance(ScoreComponent):\n    \"\"\"Penalizes selections whose centroid deviates from the global center.\n\n    Computes the Euclidean distance between the centroid of selected slice\n    features and the origin (global center in standardized feature space).\n\n    This objective ensures the selection doesn't systematically bias toward\n    extreme conditions, maintaining balance around typical conditions.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import CentroidBalance\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Penalize selections biased toward extreme periods\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'balance': (0.5, CentroidBalance())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Used in examples/ex2_feature_space.py to maintain balanced selections\n        &gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'fidelity': (1.0, WassersteinFidelity()),\n        ...     'balance': (0.3, CentroidBalance())\n        ... })\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize centroid balance component.\"\"\"\n        self.name = \"centroid_balance\"\n        self.direction = \"min\"\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Store the feature matrix for centroid computation.\n\n        Args:\n            context: Problem context with computed features (should be\n                standardized for meaningful centroid distances).\n        \"\"\"\n        self.features = context.df_features.copy()\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute distance from selection centroid to global center.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            Euclidean distance from the selection's feature centroid to\n            the origin. Lower values indicate more balanced selections.\n        \"\"\"\n        X = self.features.loc[list(combination)].values\n        mu = X.mean(axis=0)\n        return float(np.linalg.norm(mu))\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize centroid balance component.</p> Source code in <code>energy_repset/score_components/centroid_balance.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize centroid balance component.\"\"\"\n    self.name = \"centroid_balance\"\n    self.direction = \"min\"\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Store the feature matrix for centroid computation.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with computed features (should be standardized for meaningful centroid distances).</p> required Source code in <code>energy_repset/score_components/centroid_balance.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Store the feature matrix for centroid computation.\n\n    Args:\n        context: Problem context with computed features (should be\n            standardized for meaningful centroid distances).\n    \"\"\"\n    self.features = context.df_features.copy()\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CentroidBalance.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute distance from selection centroid to global center.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Euclidean distance from the selection's feature centroid to</p> <code>float</code> <p>the origin. Lower values indicate more balanced selections.</p> Source code in <code>energy_repset/score_components/centroid_balance.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute distance from selection centroid to global center.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        Euclidean distance from the selection's feature centroid to\n        the origin. Lower values indicate more balanced selections.\n    \"\"\"\n    X = self.features.loc[list(combination)].values\n    mu = X.mean(axis=0)\n    return float(np.linalg.norm(mu))\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance","title":"CoverageBalance","text":"<p>               Bases: <code>ScoreComponent</code></p> <p>Promotes balanced coverage by encouraging uniform responsibility.</p> <p>Uses RBF (Radial Basis Function) kernel-based soft assignment to compute how much \"responsibility\" each selected representative has for covering all candidate slices. Penalizes selections where some representatives cover many slices while others cover few.</p> <p>This is conceptually similar to cluster balance in k-medoids, ensuring no representative is over- or under-utilized.</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>RBF kernel sharpness parameter (higher = sharper assignments). Default is 1.0.</p> <code>1.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.score_components import CoverageBalance\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Ensure balanced coverage with default sharpness\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'coverage': (0.5, CoverageBalance())\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sharper assignments (more cluster-like behavior)\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'coverage': (0.5, CoverageBalance(gamma=2.0))\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Softer assignments (smoother transitions)\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'coverage': (0.5, CoverageBalance(gamma=0.5))\n... })\n</code></pre> Source code in <code>energy_repset/score_components/coverage_balance.py</code> <pre><code>class CoverageBalance(ScoreComponent):\n    \"\"\"Promotes balanced coverage by encouraging uniform responsibility.\n\n    Uses RBF (Radial Basis Function) kernel-based soft assignment to compute\n    how much \"responsibility\" each selected representative has for covering\n    all candidate slices. Penalizes selections where some representatives\n    cover many slices while others cover few.\n\n    This is conceptually similar to cluster balance in k-medoids, ensuring\n    no representative is over- or under-utilized.\n\n    Args:\n        gamma: RBF kernel sharpness parameter (higher = sharper assignments).\n            Default is 1.0.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.score_components import CoverageBalance\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Ensure balanced coverage with default sharpness\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'coverage': (0.5, CoverageBalance())\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sharper assignments (more cluster-like behavior)\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'coverage': (0.5, CoverageBalance(gamma=2.0))\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Softer assignments (smoother transitions)\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'coverage': (0.5, CoverageBalance(gamma=0.5))\n        ... })\n    \"\"\"\n\n    def __init__(self, gamma: float = 1.0) -&gt; None:\n        \"\"\"Initialize coverage balance component.\n\n        Args:\n            gamma: RBF kernel sharpness. Higher values create sharper\n                cluster-like assignments.\n        \"\"\"\n        self.name = \"coverage_balance\"\n        self.direction = \"min\"\n        self.gamma = gamma\n\n    def prepare(self, context: ProblemContext) -&gt; None:\n        \"\"\"Store feature matrix for responsibility computation.\n\n        Args:\n            context: Problem context with computed features.\n        \"\"\"\n        self.features = context.df_features.copy()\n        self.all_X = np.nan_to_num(self.features.values, nan=0.0)\n\n    def _responsibilities(self, combination: SliceCombination) -&gt; np.ndarray:\n        \"\"\"Compute soft assignment responsibilities using RBF kernel.\n\n        Args:\n            combination: Tuple of slice identifiers.\n\n        Returns:\n            Array of responsibility weights for each selected slice,\n            summing to 1.0.\n        \"\"\"\n        sel_X = self.features.loc[list(combination)].values\n        # Compute squared distances: (n_all, n_sel)\n        d2 = ((self.all_X[:, None, :] - sel_X[None, :, :]) ** 2).sum(axis=2)\n        # RBF kernel weights\n        K = np.exp(-self.gamma * d2)\n        # Responsibility = sum of weights across all slices\n        mass = K.sum(axis=0)\n        if mass.sum() &lt;= 0:\n            return np.ones(len(combination)) / len(combination)\n        return mass / mass.sum()\n\n    def score(self, combination: SliceCombination) -&gt; float:\n        \"\"\"Compute L2 deviation of responsibilities from uniform distribution.\n\n        Args:\n            combination: Tuple of slice identifiers forming the selection.\n\n        Returns:\n            L2 norm of (responsibilities - uniform). Zero indicates perfectly\n            balanced coverage; higher values indicate imbalance.\n        \"\"\"\n        r = self._responsibilities(combination)\n        u = np.ones_like(r) / len(r)\n        return float(np.linalg.norm(r - u))\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance.__init__","title":"__init__","text":"<pre><code>__init__(gamma: float = 1.0) -&gt; None\n</code></pre> <p>Initialize coverage balance component.</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>RBF kernel sharpness. Higher values create sharper cluster-like assignments.</p> <code>1.0</code> Source code in <code>energy_repset/score_components/coverage_balance.py</code> <pre><code>def __init__(self, gamma: float = 1.0) -&gt; None:\n    \"\"\"Initialize coverage balance component.\n\n    Args:\n        gamma: RBF kernel sharpness. Higher values create sharper\n            cluster-like assignments.\n    \"\"\"\n    self.name = \"coverage_balance\"\n    self.direction = \"min\"\n    self.gamma = gamma\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance.prepare","title":"prepare","text":"<pre><code>prepare(context: ProblemContext) -&gt; None\n</code></pre> <p>Store feature matrix for responsibility computation.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with computed features.</p> required Source code in <code>energy_repset/score_components/coverage_balance.py</code> <pre><code>def prepare(self, context: ProblemContext) -&gt; None:\n    \"\"\"Store feature matrix for responsibility computation.\n\n    Args:\n        context: Problem context with computed features.\n    \"\"\"\n    self.features = context.df_features.copy()\n    self.all_X = np.nan_to_num(self.features.values, nan=0.0)\n</code></pre>"},{"location":"api/score_components/#energy_repset.score_components.CoverageBalance.score","title":"score","text":"<pre><code>score(combination: SliceCombination) -&gt; float\n</code></pre> <p>Compute L2 deviation of responsibilities from uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>SliceCombination</code> <p>Tuple of slice identifiers forming the selection.</p> required <p>Returns:</p> Type Description <code>float</code> <p>L2 norm of (responsibilities - uniform). Zero indicates perfectly</p> <code>float</code> <p>balanced coverage; higher values indicate imbalance.</p> Source code in <code>energy_repset/score_components/coverage_balance.py</code> <pre><code>def score(self, combination: SliceCombination) -&gt; float:\n    \"\"\"Compute L2 deviation of responsibilities from uniform distribution.\n\n    Args:\n        combination: Tuple of slice identifiers forming the selection.\n\n    Returns:\n        L2 norm of (responsibilities - uniform). Zero indicates perfectly\n        balanced coverage; higher values indicate imbalance.\n    \"\"\"\n    r = self._responsibilities(combination)\n    u = np.ones_like(r) / len(r)\n    return float(np.linalg.norm(r - u))\n</code></pre>"},{"location":"api/search_algorithms/","title":"Search Algorithms","text":""},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SearchAlgorithm","title":"SearchAlgorithm","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all selection search algorithms (Pillar A).</p> <p>Defines the interface for algorithms that find optimal representative subsets. The algorithm's sole responsibility is to take a problem context and find the best selection of k items based on its internal logic and objective function.</p> <p>Different workflow types implement this protocol differently: - Generate-and-Test: Generates candidates, evaluates with ObjectiveSet, selects best - Constructive: Builds solution iteratively (e.g., k-means clustering) - Direct Optimization: Formulates and solves as single optimization problem (e.g., MILP)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class SimpleExhaustiveSearch(SearchAlgorithm):\n...     def __init__(self, objective_set: ObjectiveSet, selection_policy: SelectionPolicy, k: int):\n...         self.objective_set = objective_set\n...         self.selection_policy = selection_policy\n...         self.k = k\n...\n...     def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n...         # Generate all k-combinations\n...         from itertools import combinations\n...         all_combis = list(combinations(context.slicer.slices, self.k))\n...\n...         # Score each combination\n...         scored_combis = []\n...         for combi in all_combis:\n...             scores = self.objective_set.evaluate(context, combi)\n...             scored_combis.append((combi, scores))\n...\n...         # Select best according to policy\n...         best_combi, best_scores = self.selection_policy.select(scored_combis)\n...\n...         return RepSetResult(\n...             selection=best_combi,\n...             weights={s: 1/self.k for s in best_combi},\n...             scores=best_scores\n...         )\n...\n&gt;&gt;&gt; algorithm = SimpleExhaustiveSearch(objective_set, policy, k=4)\n&gt;&gt;&gt; result = algorithm.find_selection(context)\n&gt;&gt;&gt; print(result.selection)  # e.g., (0, 3, 6, 9) - selected slice IDs\n</code></pre> Source code in <code>energy_repset/search_algorithms/search_algorithm.py</code> <pre><code>class SearchAlgorithm(ABC):\n    \"\"\"Base class for all selection search algorithms (Pillar A).\n\n    Defines the interface for algorithms that find optimal representative subsets.\n    The algorithm's sole responsibility is to take a problem context and find the\n    best selection of k items based on its internal logic and objective function.\n\n    Different workflow types implement this protocol differently:\n    - Generate-and-Test: Generates candidates, evaluates with ObjectiveSet, selects best\n    - Constructive: Builds solution iteratively (e.g., k-means clustering)\n    - Direct Optimization: Formulates and solves as single optimization problem (e.g., MILP)\n\n    Examples:\n        &gt;&gt;&gt; class SimpleExhaustiveSearch(SearchAlgorithm):\n        ...     def __init__(self, objective_set: ObjectiveSet, selection_policy: SelectionPolicy, k: int):\n        ...         self.objective_set = objective_set\n        ...         self.selection_policy = selection_policy\n        ...         self.k = k\n        ...\n        ...     def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        ...         # Generate all k-combinations\n        ...         from itertools import combinations\n        ...         all_combis = list(combinations(context.slicer.slices, self.k))\n        ...\n        ...         # Score each combination\n        ...         scored_combis = []\n        ...         for combi in all_combis:\n        ...             scores = self.objective_set.evaluate(context, combi)\n        ...             scored_combis.append((combi, scores))\n        ...\n        ...         # Select best according to policy\n        ...         best_combi, best_scores = self.selection_policy.select(scored_combis)\n        ...\n        ...         return RepSetResult(\n        ...             selection=best_combi,\n        ...             weights={s: 1/self.k for s in best_combi},\n        ...             scores=best_scores\n        ...         )\n        ...\n        &gt;&gt;&gt; algorithm = SimpleExhaustiveSearch(objective_set, policy, k=4)\n        &gt;&gt;&gt; result = algorithm.find_selection(context)\n        &gt;&gt;&gt; print(result.selection)  # e.g., (0, 3, 6, 9) - selected slice IDs\n    \"\"\"\n\n    @abstractmethod\n    def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        \"\"\"Find the best subset of k representative periods.\n\n        Args:\n            context: The problem context with df_features populated (feature\n                engineering must be run before calling this method).\n\n        Returns:\n            A RepSetResult containing the selected slice identifiers, their\n            representation weights, and objective scores.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SearchAlgorithm.find_selection","title":"find_selection  <code>abstractmethod</code>","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Find the best subset of k representative periods.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>The problem context with df_features populated (feature engineering must be run before calling this method).</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>A RepSetResult containing the selected slice identifiers, their</p> <code>RepSetResult</code> <p>representation weights, and objective scores.</p> Source code in <code>energy_repset/search_algorithms/search_algorithm.py</code> <pre><code>@abstractmethod\ndef find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n    \"\"\"Find the best subset of k representative periods.\n\n    Args:\n        context: The problem context with df_features populated (feature\n            engineering must be run before calling this method).\n\n    Returns:\n        A RepSetResult containing the selected slice identifiers, their\n        representation weights, and objective scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenSearchAlgorithm","title":"ObjectiveDrivenSearchAlgorithm","text":"<p>               Bases: <code>SearchAlgorithm</code>, <code>ABC</code></p> <p>Base class for search algorithms guided by external objective functions.</p> <p>Provides a common structure for algorithms that rely on a user-defined ObjectiveSet to score candidates and a SelectionPolicy to choose the best. This pattern separates the search strategy from the objective function, enabling flexible algorithm design.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet, ObjectiveSpec\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; from energy_repset.selection_policies import WeightedSumPolicy\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'wasserstein': (1.0, WassersteinFidelity()),\n... })\n&gt;&gt;&gt; policy = WeightedSumPolicy()\n&gt;&gt;&gt; # See ObjectiveDrivenCombinatorialSearchAlgorithm for concrete usage\n</code></pre> Source code in <code>energy_repset/search_algorithms/objective_driven.py</code> <pre><code>class ObjectiveDrivenSearchAlgorithm(SearchAlgorithm, ABC):\n    \"\"\"Base class for search algorithms guided by external objective functions.\n\n    Provides a common structure for algorithms that rely on a user-defined\n    ObjectiveSet to score candidates and a SelectionPolicy to choose the best.\n    This pattern separates the search strategy from the objective function,\n    enabling flexible algorithm design.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet, ObjectiveSpec\n        &gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n        &gt;&gt;&gt; from energy_repset.selection_policies import WeightedSumPolicy\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'wasserstein': (1.0, WassersteinFidelity()),\n        ... })\n        &gt;&gt;&gt; policy = WeightedSumPolicy()\n        &gt;&gt;&gt; # See ObjectiveDrivenCombinatorialSearchAlgorithm for concrete usage\n    \"\"\"\n    def __init__(\n            self,\n            objective_set: ObjectiveSet,\n            selection_policy: SelectionPolicy,\n    ):\n        \"\"\"Initialize objective-driven search algorithm.\n\n        Args:\n            objective_set: Collection of score components defining quality metrics.\n            selection_policy: Strategy for selecting best combination from scored\n                candidates (e.g., weighted sum, Pareto dominance).\n        \"\"\"\n        self.objective_set = objective_set\n        self.selection_policy = selection_policy\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenSearchAlgorithm.__init__","title":"__init__","text":"<pre><code>__init__(objective_set: ObjectiveSet, selection_policy: SelectionPolicy)\n</code></pre> <p>Initialize objective-driven search algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>objective_set</code> <code>ObjectiveSet</code> <p>Collection of score components defining quality metrics.</p> required <code>selection_policy</code> <code>SelectionPolicy</code> <p>Strategy for selecting best combination from scored candidates (e.g., weighted sum, Pareto dominance).</p> required Source code in <code>energy_repset/search_algorithms/objective_driven.py</code> <pre><code>def __init__(\n        self,\n        objective_set: ObjectiveSet,\n        selection_policy: SelectionPolicy,\n):\n    \"\"\"Initialize objective-driven search algorithm.\n\n    Args:\n        objective_set: Collection of score components defining quality metrics.\n        selection_policy: Strategy for selecting best combination from scored\n            candidates (e.g., weighted sum, Pareto dominance).\n    \"\"\"\n    self.objective_set = objective_set\n    self.selection_policy = selection_policy\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm","title":"ObjectiveDrivenCombinatorialSearchAlgorithm","text":"<p>               Bases: <code>ObjectiveDrivenSearchAlgorithm</code></p> <p>Generate-and-test search using a combination generator (Workflow Type 1).</p> <p>Generates candidate combinations using a CombinationGenerator, scores each with the ObjectiveSet, and selects the best according to the SelectionPolicy. This is the canonical implementation of the Generate-and-Test workflow.</p> <p>Supports exhaustive search (all k-combinations) and constrained generation (e.g., seasonal quotas). Displays progress with tqdm and stores all evaluations in diagnostics for analysis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet, ObjectiveSpec,\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen,\n&gt;&gt;&gt; from energy_repset.selection_policies import WeightedSumPolicy\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity, CorrelationFidelity\n&gt;&gt;&gt; objectives = ObjectiveSet({\n...     'wasserstein': (1.0, WassersteinFidelity()),\n...     'correlation': (0.5, CorrelationFidelity())\n... })\n&gt;&gt;&gt; policy = WeightedSumPolicy()\n&gt;&gt;&gt; generator = ExhaustiveCombiGen(k=4)\n&gt;&gt;&gt; algorithm = ObjectiveDrivenCombinatorialSearchAlgorithm(\n...     objective_set=objectives,\n...     selection_policy=policy,\n...     combination_generator=generator\n... )\n&gt;&gt;&gt; result = algorithm.find_selection(context, k=4)\n&gt;&gt;&gt; print(result.selection)  # Best 4-month selection\n&gt;&gt;&gt; print(result.diagnostics['evaluations_df'])  # All scored combinations\n</code></pre> Source code in <code>energy_repset/search_algorithms/objective_driven.py</code> <pre><code>class ObjectiveDrivenCombinatorialSearchAlgorithm(ObjectiveDrivenSearchAlgorithm):\n    \"\"\"Generate-and-test search using a combination generator (Workflow Type 1).\n\n    Generates candidate combinations using a CombinationGenerator, scores each\n    with the ObjectiveSet, and selects the best according to the SelectionPolicy.\n    This is the canonical implementation of the Generate-and-Test workflow.\n\n    Supports exhaustive search (all k-combinations) and constrained generation\n    (e.g., seasonal quotas). Displays progress with tqdm and stores all\n    evaluations in diagnostics for analysis.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet, ObjectiveSpec,\n        &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen,\n        &gt;&gt;&gt; from energy_repset.selection_policies import WeightedSumPolicy\n        &gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity, CorrelationFidelity\n        &gt;&gt;&gt; objectives = ObjectiveSet({\n        ...     'wasserstein': (1.0, WassersteinFidelity()),\n        ...     'correlation': (0.5, CorrelationFidelity())\n        ... })\n        &gt;&gt;&gt; policy = WeightedSumPolicy()\n        &gt;&gt;&gt; generator = ExhaustiveCombiGen(k=4)\n        &gt;&gt;&gt; algorithm = ObjectiveDrivenCombinatorialSearchAlgorithm(\n        ...     objective_set=objectives,\n        ...     selection_policy=policy,\n        ...     combination_generator=generator\n        ... )\n        &gt;&gt;&gt; result = algorithm.find_selection(context, k=4)\n        &gt;&gt;&gt; print(result.selection)  # Best 4-month selection\n        &gt;&gt;&gt; print(result.diagnostics['evaluations_df'])  # All scored combinations\n    \"\"\"\n    def __init__(\n            self,\n            objective_set: ObjectiveSet,\n            selection_policy: SelectionPolicy,\n            combination_generator: CombinationGenerator,\n    ):\n        \"\"\"Initialize combinatorial search algorithm.\n\n        Args:\n            objective_set: Collection of score components defining quality metrics.\n            selection_policy: Strategy for selecting the best combination.\n            combination_generator: Defines which combinations to evaluate\n                (e.g., all combinations, seasonal constraints).\n        \"\"\"\n        super().__init__(objective_set, selection_policy)\n        self.combination_generator = combination_generator\n        self._all_scores_df: pd.DataFrame | None = None\n\n    def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        \"\"\"Find optimal selection by exhaustively scoring generated combinations.\n\n        Args:\n            context: Problem context with df_features populated.\n\n        Returns:\n            RepSetResult with the winning selection, scores, representatives,\n            and diagnostics containing evaluations_df with all scored combinations.\n        \"\"\"\n        from tqdm import tqdm\n        import pandas as pd\n\n        self.objective_set.prepare(context)\n\n        unique_slices = context.get_unique_slices()\n        iterator = tqdm(\n            self.combination_generator.generate(unique_slices),\n            desc='Iterating over combinations',\n            total=self.combination_generator.count(unique_slices)\n        )\n\n        rows = []\n        for combi in iterator:\n            metrics = self.objective_set.evaluate(combi, context)\n            rec = {\n                \"slices\": combi,\n                \"label\": \", \".join(str(s) for s in combi)\n            }\n            rec.update(metrics)\n            rows.append(rec)\n\n        evaluations_df = pd.DataFrame(rows)\n        self._all_scores_df = evaluations_df.copy()\n\n        winning_combination = self.selection_policy.select_best(evaluations_df, self.objective_set)\n        slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n        result = RepSetResult(\n            context=context,\n            selection_space='subset',\n            selection=winning_combination,\n            scores=self.objective_set.evaluate(winning_combination, context),\n            representatives={s: context.df_raw.iloc[slice_labels == s] for s in winning_combination},\n            diagnostics={'evaluations_df': evaluations_df}\n        )\n        return result\n\n    def get_all_scores(self) -&gt; pd.DataFrame:\n        \"\"\"Return DataFrame of all evaluated combinations with scores.\n\n        Returns:\n            DataFrame with columns: slices, label, score_comp_1, score_comp_2, ...\n\n        Raises:\n            ValueError: If find_selection() has not been called yet.\n        \"\"\"\n        import pandas as pd\n\n        if self._all_scores_df is None:\n            raise ValueError(\"No scores available. Call find_selection() first.\")\n        return self._all_scores_df.copy()\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm.__init__","title":"__init__","text":"<pre><code>__init__(objective_set: ObjectiveSet, selection_policy: SelectionPolicy, combination_generator: CombinationGenerator)\n</code></pre> <p>Initialize combinatorial search algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>objective_set</code> <code>ObjectiveSet</code> <p>Collection of score components defining quality metrics.</p> required <code>selection_policy</code> <code>SelectionPolicy</code> <p>Strategy for selecting the best combination.</p> required <code>combination_generator</code> <code>CombinationGenerator</code> <p>Defines which combinations to evaluate (e.g., all combinations, seasonal constraints).</p> required Source code in <code>energy_repset/search_algorithms/objective_driven.py</code> <pre><code>def __init__(\n        self,\n        objective_set: ObjectiveSet,\n        selection_policy: SelectionPolicy,\n        combination_generator: CombinationGenerator,\n):\n    \"\"\"Initialize combinatorial search algorithm.\n\n    Args:\n        objective_set: Collection of score components defining quality metrics.\n        selection_policy: Strategy for selecting the best combination.\n        combination_generator: Defines which combinations to evaluate\n            (e.g., all combinations, seasonal constraints).\n    \"\"\"\n    super().__init__(objective_set, selection_policy)\n    self.combination_generator = combination_generator\n    self._all_scores_df: pd.DataFrame | None = None\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm.find_selection","title":"find_selection","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Find optimal selection by exhaustively scoring generated combinations.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with df_features populated.</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>RepSetResult with the winning selection, scores, representatives,</p> <code>RepSetResult</code> <p>and diagnostics containing evaluations_df with all scored combinations.</p> Source code in <code>energy_repset/search_algorithms/objective_driven.py</code> <pre><code>def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n    \"\"\"Find optimal selection by exhaustively scoring generated combinations.\n\n    Args:\n        context: Problem context with df_features populated.\n\n    Returns:\n        RepSetResult with the winning selection, scores, representatives,\n        and diagnostics containing evaluations_df with all scored combinations.\n    \"\"\"\n    from tqdm import tqdm\n    import pandas as pd\n\n    self.objective_set.prepare(context)\n\n    unique_slices = context.get_unique_slices()\n    iterator = tqdm(\n        self.combination_generator.generate(unique_slices),\n        desc='Iterating over combinations',\n        total=self.combination_generator.count(unique_slices)\n    )\n\n    rows = []\n    for combi in iterator:\n        metrics = self.objective_set.evaluate(combi, context)\n        rec = {\n            \"slices\": combi,\n            \"label\": \", \".join(str(s) for s in combi)\n        }\n        rec.update(metrics)\n        rows.append(rec)\n\n    evaluations_df = pd.DataFrame(rows)\n    self._all_scores_df = evaluations_df.copy()\n\n    winning_combination = self.selection_policy.select_best(evaluations_df, self.objective_set)\n    slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n    result = RepSetResult(\n        context=context,\n        selection_space='subset',\n        selection=winning_combination,\n        scores=self.objective_set.evaluate(winning_combination, context),\n        representatives={s: context.df_raw.iloc[slice_labels == s] for s in winning_combination},\n        diagnostics={'evaluations_df': evaluations_df}\n    )\n    return result\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.ObjectiveDrivenCombinatorialSearchAlgorithm.get_all_scores","title":"get_all_scores","text":"<pre><code>get_all_scores() -&gt; DataFrame\n</code></pre> <p>Return DataFrame of all evaluated combinations with scores.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: slices, label, score_comp_1, score_comp_2, ...</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If find_selection() has not been called yet.</p> Source code in <code>energy_repset/search_algorithms/objective_driven.py</code> <pre><code>def get_all_scores(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame of all evaluated combinations with scores.\n\n    Returns:\n        DataFrame with columns: slices, label, score_comp_1, score_comp_2, ...\n\n    Raises:\n        ValueError: If find_selection() has not been called yet.\n    \"\"\"\n    import pandas as pd\n\n    if self._all_scores_df is None:\n        raise ValueError(\"No scores available. Call find_selection() first.\")\n    return self._all_scores_df.copy()\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.HullClusteringSearch","title":"HullClusteringSearch","text":"<p>               Bases: <code>SearchAlgorithm</code></p> <p>Farthest-point greedy hull clustering (Neustroev et al., 2025).</p> <p>Implements the greedy convex/conic hull clustering algorithm from Neustroev et al. (2025).  At each iteration the algorithm selects the data point furthest from the current hull, i.e. the point with maximum projection error onto the hull spanned by the already- selected representatives.  The first representative is the point furthest from the dataset mean.</p> <p>This farthest-point strategy naturally selects extreme/boundary periods first, producing a hull that spans the data well.</p> <p>The algorithm leaves <code>weights=None</code> in the result so that an external <code>RepresentationModel</code> (typically <code>BlendedRepresentationModel</code>) can compute the final soft-assignment weights.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of representative periods to select.</p> required <code>hull_type</code> <code>Literal['convex', 'conic']</code> <p>Type of projection constraint. <code>'convex'</code> enforces non-negative weights that sum to 1. <code>'conic'</code> enforces only non-negativity.</p> <code>'convex'</code> References <p>G. Neustroev, D. A. Tejada-Arango, G. Morales-Espana, M. M. de Weerdt. \"Hull Clustering with Blended Representative Periods for Energy System Optimization Models.\" arXiv:2508.21641, 2025.</p> <p>Examples:</p> <p>Basic usage with blended representation:</p> <pre><code>&gt;&gt;&gt; from energy_repset.search_algorithms import HullClusteringSearch\n&gt;&gt;&gt; from energy_repset.representation import BlendedRepresentationModel\n&gt;&gt;&gt; search = HullClusteringSearch(k=4, hull_type='convex')\n&gt;&gt;&gt; repr_model = BlendedRepresentationModel(blend_type='convex')\n</code></pre> Source code in <code>energy_repset/search_algorithms/hull_clustering.py</code> <pre><code>class HullClusteringSearch(SearchAlgorithm):\n    \"\"\"Farthest-point greedy hull clustering (Neustroev et al., 2025).\n\n    Implements the greedy convex/conic hull clustering algorithm from\n    Neustroev et al. (2025).  At each iteration the algorithm selects\n    the data point **furthest from the current hull**, i.e. the point\n    with maximum projection error onto the hull spanned by the already-\n    selected representatives.  The first representative is the point\n    furthest from the dataset mean.\n\n    This farthest-point strategy naturally selects extreme/boundary\n    periods first, producing a hull that spans the data well.\n\n    The algorithm leaves ``weights=None`` in the result so that an external\n    ``RepresentationModel`` (typically ``BlendedRepresentationModel``) can\n    compute the final soft-assignment weights.\n\n    Args:\n        k: Number of representative periods to select.\n        hull_type: Type of projection constraint. ``'convex'`` enforces\n            non-negative weights that sum to 1. ``'conic'`` enforces only\n            non-negativity.\n\n    References:\n        G. Neustroev, D. A. Tejada-Arango, G. Morales-Espana,\n        M. M. de Weerdt. \"Hull Clustering with Blended Representative\n        Periods for Energy System Optimization Models.\"\n        arXiv:2508.21641, 2025.\n\n    Examples:\n        Basic usage with blended representation:\n\n        &gt;&gt;&gt; from energy_repset.search_algorithms import HullClusteringSearch\n        &gt;&gt;&gt; from energy_repset.representation import BlendedRepresentationModel\n        &gt;&gt;&gt; search = HullClusteringSearch(k=4, hull_type='convex')\n        &gt;&gt;&gt; repr_model = BlendedRepresentationModel(blend_type='convex')\n    \"\"\"\n\n    def __init__(self, k: int, hull_type: Literal['convex', 'conic'] = 'convex'):\n        \"\"\"Initialize Hull Clustering search.\n\n        Args:\n            k: Number of hull vertices (representative periods) to select.\n            hull_type: Projection type. ``'convex'`` requires weights &gt;= 0 and\n                sum(weights) == 1. ``'conic'`` requires only weights &gt;= 0.\n        \"\"\"\n        self.k = k\n        self.hull_type = hull_type\n\n    def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        \"\"\"Find k hull vertices via farthest-point greedy selection.\n\n        The algorithm (Algorithm 2 in Neustroev et al.):\n\n        1. Select the point furthest from the dataset mean.\n        2. For iterations 2..k, compute the projection error (hull\n           distance) for every remaining point and select the one with\n           the **maximum** error.\n\n        Args:\n            context: Problem context with ``df_features`` populated.\n\n        Returns:\n            RepSetResult with the selected hull vertices, ``weights=None``\n            (to be filled by an external representation model), and the\n            final projection error in ``scores``.\n        \"\"\"\n        Z = context.df_features.values\n        labels = list(context.df_features.index)\n        N = Z.shape[0]\n\n        selected_idx, final_error = self._greedy_farthest_point(Z)\n\n        selection = tuple(labels[i] for i in selected_idx)\n\n        slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n        representatives = {\n            s: context.df_raw.loc[slice_labels == s] for s in selection\n        }\n\n        return RepSetResult(\n            context=context,\n            selection_space='subset',\n            selection=selection,\n            scores={'projection_error': final_error},\n            representatives=representatives,\n            weights=None,\n        )\n\n    def _greedy_farthest_point(\n        self, Z: np.ndarray\n    ) -&gt; tuple[list[int], float]:\n        \"\"\"Run the farthest-point greedy hull clustering.\n\n        Args:\n            Z: Feature matrix (N x p).\n\n        Returns:\n            Tuple of (selected indices, total projection error).\n        \"\"\"\n        N = Z.shape[0]\n\n        first_idx = self._init_furthest_from_mean(Z)\n        selected_idx = [first_idx]\n        remaining = set(range(N)) - {first_idx}\n\n        hull_dists = np.full(N, np.inf)\n        self._update_hull_distances(Z, selected_idx, remaining, hull_dists)\n\n        for _ in range(self.k - 1):\n            best = max(remaining, key=lambda i: hull_dists[i])\n            selected_idx.append(best)\n            remaining.discard(best)\n\n            if remaining:\n                self._update_hull_distances(\n                    Z, selected_idx, remaining, hull_dists\n                )\n\n        final_error = sum(hull_dists[i] for i in range(N) if i not in selected_idx)\n        return selected_idx, float(final_error)\n\n    @staticmethod\n    def _init_furthest_from_mean(Z: np.ndarray) -&gt; int:\n        \"\"\"Select the point furthest from the dataset mean.\n\n        Args:\n            Z: Feature matrix (N x p).\n\n        Returns:\n            Index of the point with maximum squared distance to the mean.\n        \"\"\"\n        mean_z = Z.mean(axis=0)\n        dists = np.sum((Z - mean_z) ** 2, axis=1)\n        return int(np.argmax(dists))\n\n    def _update_hull_distances(\n        self,\n        Z: np.ndarray,\n        selected_idx: list[int],\n        remaining: set[int],\n        hull_dists: np.ndarray,\n    ) -&gt; None:\n        \"\"\"Recompute hull distances for remaining points.\n\n        Args:\n            Z: Feature matrix (N x p).\n            selected_idx: Currently selected hull vertex indices.\n            remaining: Set of indices still available for selection.\n            hull_dists: Array to update in-place with new hull distances.\n        \"\"\"\n        Z_sel = Z[selected_idx]\n        for i in remaining:\n            hull_dists[i] = self._projection_error(Z[i], Z_sel)\n\n    def _projection_error(self, z: np.ndarray, Z_sel: np.ndarray) -&gt; float:\n        \"\"\"Compute projection error for a single point onto the hull.\n\n        Args:\n            z: Feature vector for one period (length p).\n            Z_sel: Feature matrix of selected hull vertices (k_current x p).\n\n        Returns:\n            Squared L2 projection error.\n        \"\"\"\n        if self.hull_type == 'conic':\n            return self._projection_error_conic(z, Z_sel)\n        return self._projection_error_convex(z, Z_sel)\n\n    def _projection_error_conic(\n        self, z: np.ndarray, Z_sel: np.ndarray\n    ) -&gt; float:\n        \"\"\"Conic projection: min_w ||z - Z_sel^T @ w||^2, w &gt;= 0.\"\"\"\n        w, _ = nnls(Z_sel.T, z)\n        reconstruction = Z_sel.T @ w\n        return float(np.sum((z - reconstruction) ** 2))\n\n    def _projection_error_convex(\n        self, z: np.ndarray, Z_sel: np.ndarray\n    ) -&gt; float:\n        \"\"\"Convex projection: min_w ||z - w @ Z_sel||^2, w &gt;= 0, sum(w) = 1.\"\"\"\n        k_sel = Z_sel.shape[0]\n        if k_sel == 1:\n            return float(np.sum((z - Z_sel[0]) ** 2))\n\n        def objective(w):\n            return np.sum((z - w @ Z_sel) ** 2)\n\n        w0 = np.ones(k_sel) / k_sel\n        bounds = [(0.0, 1.0)] * k_sel\n        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n\n        result = minimize(\n            objective, w0, method='SLSQP',\n            bounds=bounds, constraints=constraints,\n            options={'ftol': 1e-10, 'maxiter': 200},\n        )\n        return float(result.fun)\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.HullClusteringSearch.__init__","title":"__init__","text":"<pre><code>__init__(k: int, hull_type: Literal['convex', 'conic'] = 'convex')\n</code></pre> <p>Initialize Hull Clustering search.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of hull vertices (representative periods) to select.</p> required <code>hull_type</code> <code>Literal['convex', 'conic']</code> <p>Projection type. <code>'convex'</code> requires weights &gt;= 0 and sum(weights) == 1. <code>'conic'</code> requires only weights &gt;= 0.</p> <code>'convex'</code> Source code in <code>energy_repset/search_algorithms/hull_clustering.py</code> <pre><code>def __init__(self, k: int, hull_type: Literal['convex', 'conic'] = 'convex'):\n    \"\"\"Initialize Hull Clustering search.\n\n    Args:\n        k: Number of hull vertices (representative periods) to select.\n        hull_type: Projection type. ``'convex'`` requires weights &gt;= 0 and\n            sum(weights) == 1. ``'conic'`` requires only weights &gt;= 0.\n    \"\"\"\n    self.k = k\n    self.hull_type = hull_type\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.HullClusteringSearch.find_selection","title":"find_selection","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Find k hull vertices via farthest-point greedy selection.</p> <p>The algorithm (Algorithm 2 in Neustroev et al.):</p> <ol> <li>Select the point furthest from the dataset mean.</li> <li>For iterations 2..k, compute the projection error (hull    distance) for every remaining point and select the one with    the maximum error.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with <code>df_features</code> populated.</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>RepSetResult with the selected hull vertices, <code>weights=None</code></p> <code>RepSetResult</code> <p>(to be filled by an external representation model), and the</p> <code>RepSetResult</code> <p>final projection error in <code>scores</code>.</p> Source code in <code>energy_repset/search_algorithms/hull_clustering.py</code> <pre><code>def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n    \"\"\"Find k hull vertices via farthest-point greedy selection.\n\n    The algorithm (Algorithm 2 in Neustroev et al.):\n\n    1. Select the point furthest from the dataset mean.\n    2. For iterations 2..k, compute the projection error (hull\n       distance) for every remaining point and select the one with\n       the **maximum** error.\n\n    Args:\n        context: Problem context with ``df_features`` populated.\n\n    Returns:\n        RepSetResult with the selected hull vertices, ``weights=None``\n        (to be filled by an external representation model), and the\n        final projection error in ``scores``.\n    \"\"\"\n    Z = context.df_features.values\n    labels = list(context.df_features.index)\n    N = Z.shape[0]\n\n    selected_idx, final_error = self._greedy_farthest_point(Z)\n\n    selection = tuple(labels[i] for i in selected_idx)\n\n    slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n    representatives = {\n        s: context.df_raw.loc[slice_labels == s] for s in selection\n    }\n\n    return RepSetResult(\n        context=context,\n        selection_space='subset',\n        selection=selection,\n        scores={'projection_error': final_error},\n        representatives=representatives,\n        weights=None,\n    )\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.CTPCSearch","title":"CTPCSearch","text":"<p>               Bases: <code>SearchAlgorithm</code></p> <p>Chronological Time-Period Clustering with contiguity constraint.</p> <p>Implements hierarchical agglomerative clustering where only temporally adjacent periods may merge, producing k contiguous time segments. Based on Pineda &amp; Morales (2018).</p> <p>The algorithm computes weights as the fraction of time covered by each segment, so the external representation model is skipped when the result is used in <code>RepSetExperiment.run()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of contiguous time segments to produce.</p> required <code>linkage</code> <code>Literal['ward', 'complete', 'average', 'single']</code> <p>Linkage criterion for agglomerative clustering. One of <code>'ward'</code>, <code>'complete'</code>, <code>'average'</code>, or <code>'single'</code>.</p> <code>'ward'</code> References <p>S. Pineda, J. M. Morales. \"Chronological Time-Period Clustering for Optimal Capacity Expansion Planning With Storage.\" IEEE Trans. Power Syst., 33(6), 7162--7170, 2018.</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; from energy_repset.search_algorithms import CTPCSearch\n&gt;&gt;&gt; search = CTPCSearch(k=4, linkage='ward')\n&gt;&gt;&gt; result = search.find_selection(feature_context)\n&gt;&gt;&gt; result.selection  # Tuple of medoid labels\n&gt;&gt;&gt; result.weights    # Dict mapping labels to time fractions\n</code></pre> Source code in <code>energy_repset/search_algorithms/ctpc.py</code> <pre><code>class CTPCSearch(SearchAlgorithm):\n    \"\"\"Chronological Time-Period Clustering with contiguity constraint.\n\n    Implements hierarchical agglomerative clustering where only temporally\n    adjacent periods may merge, producing k contiguous time segments.\n    Based on Pineda &amp; Morales (2018).\n\n    The algorithm computes weights as the fraction of time covered by each\n    segment, so the external representation model is skipped when the result\n    is used in ``RepSetExperiment.run()``.\n\n    Args:\n        k: Number of contiguous time segments to produce.\n        linkage: Linkage criterion for agglomerative clustering. One of\n            ``'ward'``, ``'complete'``, ``'average'``, or ``'single'``.\n\n    References:\n        S. Pineda, J. M. Morales. \"Chronological Time-Period Clustering\n        for Optimal Capacity Expansion Planning With Storage.\"\n        IEEE Trans. Power Syst., 33(6), 7162--7170, 2018.\n\n    Examples:\n        Basic usage:\n\n        &gt;&gt;&gt; from energy_repset.search_algorithms import CTPCSearch\n        &gt;&gt;&gt; search = CTPCSearch(k=4, linkage='ward')\n        &gt;&gt;&gt; result = search.find_selection(feature_context)\n        &gt;&gt;&gt; result.selection  # Tuple of medoid labels\n        &gt;&gt;&gt; result.weights    # Dict mapping labels to time fractions\n    \"\"\"\n\n    def __init__(\n        self,\n        k: int,\n        linkage: Literal['ward', 'complete', 'average', 'single'] = 'ward',\n    ):\n        \"\"\"Initialize CTPC search.\n\n        Args:\n            k: Number of contiguous clusters to produce.\n            linkage: Agglomerative linkage criterion.\n        \"\"\"\n        self.k = k\n        self.linkage = linkage\n\n    def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        \"\"\"Run contiguity-constrained hierarchical clustering.\n\n        Args:\n            context: Problem context with ``df_features`` populated. Slices\n                must be naturally ordered by time (which they are when coming\n                from ``TimeSlicer``).\n\n        Returns:\n            RepSetResult with medoid (or centroid) labels as the selection,\n            pre-computed weights (segment size fractions), and within-cluster\n            sum of squares in ``scores``.\n        \"\"\"\n        Z = context.df_features.values\n        labels = list(context.df_features.index)\n        N = Z.shape[0]\n\n        connectivity = self._build_connectivity(N)\n\n        clustering = AgglomerativeClustering(\n            n_clusters=self.k,\n            connectivity=connectivity,\n            linkage=self.linkage,\n        )\n        cluster_labels = clustering.fit_predict(Z)\n\n        selection, weights, wcss, diagnostics = self._extract_results(\n            Z, labels, cluster_labels\n        )\n\n        slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n        representatives = {\n            s: context.df_raw.loc[slice_labels == s] for s in selection\n        }\n\n        return RepSetResult(\n            context=context,\n            selection_space='chronological',\n            selection=selection,\n            scores={'wcss': wcss},\n            representatives=representatives,\n            weights=weights,\n            diagnostics=diagnostics,\n        )\n\n    def _build_connectivity(self, n: int) -&gt; np.ndarray:\n        \"\"\"Build tridiagonal connectivity matrix for n slices.\n\n        Args:\n            n: Number of time slices.\n\n        Returns:\n            Sparse-like (n x n) binary adjacency matrix connecting only\n            temporally adjacent slices.\n        \"\"\"\n        off_diag = np.ones(n - 1)\n        return diags([off_diag, np.ones(n), off_diag], [-1, 0, 1]).toarray()\n\n    def _extract_results(\n        self,\n        Z: np.ndarray,\n        labels: list,\n        cluster_labels: np.ndarray,\n    ) -&gt; tuple:\n        \"\"\"Extract selection, weights, WCSS, and diagnostics from clustering.\n\n        Args:\n            Z: Feature matrix (N x p).\n            labels: Slice labels aligned with rows of Z.\n            cluster_labels: Cluster assignment for each slice (length N).\n\n        Returns:\n            Tuple of (selection, weights, wcss, diagnostics).\n        \"\"\"\n        unique_clusters = sorted(set(cluster_labels))\n        N = len(labels)\n        wcss = 0.0\n\n        selected_labels = []\n        weight_dict: Dict = {}\n        segment_info: list[Dict[str, Any]] = []\n\n        for c in unique_clusters:\n            mask = cluster_labels == c\n            indices = np.where(mask)[0]\n            cluster_Z = Z[indices]\n            centroid = cluster_Z.mean(axis=0)\n\n            cluster_wcss = np.sum((cluster_Z - centroid) ** 2)\n            wcss += cluster_wcss\n\n            dists = np.sum((cluster_Z - centroid) ** 2, axis=1)\n            medoid_local = int(np.argmin(dists))\n            rep_idx = indices[medoid_local]\n            rep_label = labels[rep_idx]\n\n            fraction = len(indices) / N\n            selected_labels.append(rep_label)\n            weight_dict[rep_label] = fraction\n\n            segment_info.append({\n                'cluster': c,\n                'start': labels[indices[0]],\n                'end': labels[indices[-1]],\n                'size': len(indices),\n                'representative': rep_label,\n            })\n\n        segment_info.sort(key=lambda seg: seg['start'])\n\n        selection = tuple(selected_labels)\n        diagnostics = {\n            'cluster_labels': cluster_labels.tolist(),\n            'segments': segment_info,\n        }\n\n        return selection, weight_dict, float(wcss), diagnostics\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.CTPCSearch.__init__","title":"__init__","text":"<pre><code>__init__(k: int, linkage: Literal['ward', 'complete', 'average', 'single'] = 'ward')\n</code></pre> <p>Initialize CTPC search.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of contiguous clusters to produce.</p> required <code>linkage</code> <code>Literal['ward', 'complete', 'average', 'single']</code> <p>Agglomerative linkage criterion.</p> <code>'ward'</code> Source code in <code>energy_repset/search_algorithms/ctpc.py</code> <pre><code>def __init__(\n    self,\n    k: int,\n    linkage: Literal['ward', 'complete', 'average', 'single'] = 'ward',\n):\n    \"\"\"Initialize CTPC search.\n\n    Args:\n        k: Number of contiguous clusters to produce.\n        linkage: Agglomerative linkage criterion.\n    \"\"\"\n    self.k = k\n    self.linkage = linkage\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.CTPCSearch.find_selection","title":"find_selection","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Run contiguity-constrained hierarchical clustering.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with <code>df_features</code> populated. Slices must be naturally ordered by time (which they are when coming from <code>TimeSlicer</code>).</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>RepSetResult with medoid (or centroid) labels as the selection,</p> <code>RepSetResult</code> <p>pre-computed weights (segment size fractions), and within-cluster</p> <code>RepSetResult</code> <p>sum of squares in <code>scores</code>.</p> Source code in <code>energy_repset/search_algorithms/ctpc.py</code> <pre><code>def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n    \"\"\"Run contiguity-constrained hierarchical clustering.\n\n    Args:\n        context: Problem context with ``df_features`` populated. Slices\n            must be naturally ordered by time (which they are when coming\n            from ``TimeSlicer``).\n\n    Returns:\n        RepSetResult with medoid (or centroid) labels as the selection,\n        pre-computed weights (segment size fractions), and within-cluster\n        sum of squares in ``scores``.\n    \"\"\"\n    Z = context.df_features.values\n    labels = list(context.df_features.index)\n    N = Z.shape[0]\n\n    connectivity = self._build_connectivity(N)\n\n    clustering = AgglomerativeClustering(\n        n_clusters=self.k,\n        connectivity=connectivity,\n        linkage=self.linkage,\n    )\n    cluster_labels = clustering.fit_predict(Z)\n\n    selection, weights, wcss, diagnostics = self._extract_results(\n        Z, labels, cluster_labels\n    )\n\n    slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n    representatives = {\n        s: context.df_raw.loc[slice_labels == s] for s in selection\n    }\n\n    return RepSetResult(\n        context=context,\n        selection_space='chronological',\n        selection=selection,\n        scores={'wcss': wcss},\n        representatives=representatives,\n        weights=weights,\n        diagnostics=diagnostics,\n    )\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SnippetSearch","title":"SnippetSearch","text":"<p>               Bases: <code>SearchAlgorithm</code></p> <p>Greedy p-median selection of multi-day representative subsequences.</p> <p>Implements a greedy approximation of the Snippet algorithm from Anderson et al. (2024).  Selects k sliding-window subsequences of <code>period_length_days</code> days each, minimizing the total day-level distance across the full time horizon.</p> <p>Each candidate subsequence contains <code>period_length_days</code> daily profile snippets. The distance from any day to a candidate is the minimum squared Euclidean distance to any of its constituent daily snippets. The greedy selection picks the candidate with the greatest total cost reduction at each iteration.</p> <p>The original paper solves the selection as a MILP; this implementation uses a greedy p-median heuristic which provides a (1 - 1/e) approximation guarantee.</p> <p>Requires <code>context.slicer.unit == 'day'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of representative subsequences to select.</p> required <code>period_length_days</code> <code>int</code> <p>Length of each candidate subsequence in days.</p> <code>7</code> <code>step_days</code> <code>int</code> <p>Stride between consecutive sliding-window candidates.</p> <code>1</code> References <p>O. Anderson, N. Yu, K. Oikonomou, D. Wu. \"On the Selection of Intermediate Length Representative Periods for Capacity Expansion.\" arXiv:2401.02888, 2024.</p> <p>Examples:</p> <p>Basic usage with daily slicing:</p> <pre><code>&gt;&gt;&gt; from energy_repset.search_algorithms import SnippetSearch\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; slicer = TimeSlicer(unit='day')\n&gt;&gt;&gt; search = SnippetSearch(k=8, period_length_days=7, step_days=1)\n</code></pre> Source code in <code>energy_repset/search_algorithms/snippet.py</code> <pre><code>class SnippetSearch(SearchAlgorithm):\n    \"\"\"Greedy p-median selection of multi-day representative subsequences.\n\n    Implements a greedy approximation of the Snippet algorithm from\n    Anderson et al. (2024).  Selects k sliding-window subsequences of\n    ``period_length_days`` days each, minimizing the total day-level\n    distance across the full time horizon.\n\n    Each candidate subsequence contains ``period_length_days`` daily profile\n    snippets. The distance from any day to a candidate is the minimum\n    squared Euclidean distance to any of its constituent daily snippets.\n    The greedy selection picks the candidate with the greatest total cost\n    reduction at each iteration.\n\n    The original paper solves the selection as a MILP; this implementation\n    uses a greedy p-median heuristic which provides a (1 - 1/e)\n    approximation guarantee.\n\n    Requires ``context.slicer.unit == 'day'``.\n\n    Args:\n        k: Number of representative subsequences to select.\n        period_length_days: Length of each candidate subsequence in days.\n        step_days: Stride between consecutive sliding-window candidates.\n\n    References:\n        O. Anderson, N. Yu, K. Oikonomou, D. Wu. \"On the Selection of\n        Intermediate Length Representative Periods for Capacity Expansion.\"\n        arXiv:2401.02888, 2024.\n\n    Examples:\n        Basic usage with daily slicing:\n\n        &gt;&gt;&gt; from energy_repset.search_algorithms import SnippetSearch\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt; slicer = TimeSlicer(unit='day')\n        &gt;&gt;&gt; search = SnippetSearch(k=8, period_length_days=7, step_days=1)\n    \"\"\"\n\n    def __init__(\n        self, k: int, period_length_days: int = 7, step_days: int = 1,\n    ):\n        \"\"\"Initialize Snippet search.\n\n        Args:\n            k: Number of representative subsequences to select.\n            period_length_days: Number of days in each candidate subsequence.\n            step_days: Stride between consecutive candidate start positions.\n        \"\"\"\n        self.k = k\n        self.period_length_days = period_length_days\n        self.step_days = step_days\n\n    def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        \"\"\"Find k representative subsequences via greedy p-median selection.\n\n        Args:\n            context: Problem context. Must have ``slicer.unit == 'day'``.\n                Feature engineering should provide daily profile vectors in\n                ``df_features``, but the algorithm can also build profiles\n                from ``df_raw`` directly.\n\n        Returns:\n            RepSetResult with selected starting-day labels, pre-computed\n            weights (fraction of days assigned), and total distance score.\n\n        Raises:\n            ValueError: If ``context.slicer.unit`` is not ``'day'``.\n        \"\"\"\n        if context.slicer.unit != 'day':\n            raise ValueError(\n                f\"SnippetSearch requires daily slicing (unit='day'), \"\n                f\"got unit='{context.slicer.unit}'.\"\n            )\n\n        daily_profiles = self._build_daily_profiles(context)\n        N = daily_profiles.shape[0]\n        day_labels = list(context.df_features.index)\n        L = self.period_length_days\n\n        candidates = self._generate_candidates(N, L, self.step_days)\n        if len(candidates) &lt; self.k:\n            raise ValueError(\n                f\"Only {len(candidates)} candidate subsequences available, \"\n                f\"but k={self.k} requested. Reduce k or period_length_days.\"\n            )\n\n        dist_matrix = self._compute_distance_matrix(daily_profiles, candidates)\n\n        selected_candidates, per_day_min = self._greedy_select(\n            dist_matrix, self.k\n        )\n\n        assignments = np.argmin(\n            dist_matrix[:, selected_candidates], axis=1\n        )\n\n        selection_labels = []\n        weight_dict = {}\n        for local_idx, cand_idx in enumerate(selected_candidates):\n            start_day = candidates[cand_idx][0]\n            label = day_labels[start_day]\n            selection_labels.append(label)\n            n_assigned = int(np.sum(assignments == local_idx))\n            weight_dict[label] = n_assigned / N\n\n        selection = tuple(selection_labels)\n        total_distance = float(np.sum(per_day_min))\n\n        slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n        representatives = {}\n        for label in selection:\n            start_idx = day_labels.index(label)\n            end_idx = min(start_idx + L, N)\n            period_labels = day_labels[start_idx:end_idx]\n            mask = slice_labels.isin(set(period_labels))\n            representatives[label] = context.df_raw.loc[mask]\n\n        return RepSetResult(\n            context=context,\n            selection_space='subset',\n            selection=selection,\n            scores={'total_distance': total_distance},\n            representatives=representatives,\n            weights=weight_dict,\n            diagnostics={\n                'assignments': assignments.tolist(),\n                'candidate_starts': [\n                    day_labels[candidates[c][0]] for c in selected_candidates\n                ],\n            },\n        )\n\n    def _build_daily_profiles(self, context: ProblemContext) -&gt; np.ndarray:\n        \"\"\"Build daily profile vectors from context features.\n\n        Args:\n            context: Problem context with ``df_features`` populated.\n\n        Returns:\n            Array of shape (N_days, n_features) with one row per day.\n        \"\"\"\n        return context.df_features.values\n\n    def _generate_candidates(\n        self, n_days: int, length: int, step: int\n    ) -&gt; list[list[int]]:\n        \"\"\"Generate sliding-window candidate subsequences.\n\n        Args:\n            n_days: Total number of days.\n            length: Number of days per subsequence.\n            step: Stride between consecutive candidates.\n\n        Returns:\n            List of candidates, each a list of day indices.\n        \"\"\"\n        candidates = []\n        start = 0\n        while start + length &lt;= n_days:\n            candidates.append(list(range(start, start + length)))\n            start += step\n        return candidates\n\n    def _compute_distance_matrix(\n        self, profiles: np.ndarray, candidates: list[list[int]]\n    ) -&gt; np.ndarray:\n        \"\"\"Compute distance from each day to each candidate.\n\n        For each (day, candidate) pair, the distance is the minimum Euclidean\n        distance between the day's profile and any of the candidate's daily\n        snippet profiles.\n\n        Args:\n            profiles: Daily profile matrix (N x p).\n            candidates: List of candidate subsequences (each a list of day indices).\n\n        Returns:\n            Distance matrix of shape (N, C) where C is the number of candidates.\n        \"\"\"\n        N = profiles.shape[0]\n        C = len(candidates)\n        dist_matrix = np.empty((N, C))\n\n        for j, cand_days in enumerate(candidates):\n            cand_profiles = profiles[cand_days]\n            for i in range(N):\n                diffs = cand_profiles - profiles[i]\n                dists = np.sum(diffs ** 2, axis=1)\n                dist_matrix[i, j] = np.min(dists)\n\n        return dist_matrix\n\n    def _greedy_select(\n        self, dist_matrix: np.ndarray, k: int\n    ) -&gt; tuple[list[int], np.ndarray]:\n        \"\"\"Greedy p-median selection of k candidates.\n\n        At each iteration, selects the candidate that most reduces the total\n        per-day minimum distance.\n\n        Args:\n            dist_matrix: Distance matrix (N x C).\n            k: Number of candidates to select.\n\n        Returns:\n            Tuple of (selected candidate indices, per-day minimum distances).\n        \"\"\"\n        N = dist_matrix.shape[0]\n        per_day_min = np.full(N, np.inf)\n        selected: list[int] = []\n\n        for _ in range(k):\n            best_candidate = -1\n            best_reduction = -np.inf\n\n            for j in range(dist_matrix.shape[1]):\n                if j in selected:\n                    continue\n                new_min = np.minimum(per_day_min, dist_matrix[:, j])\n                reduction = np.sum(per_day_min) - np.sum(new_min)\n                if reduction &gt; best_reduction:\n                    best_reduction = reduction\n                    best_candidate = j\n\n            selected.append(best_candidate)\n            per_day_min = np.minimum(per_day_min, dist_matrix[:, best_candidate])\n\n        return selected, per_day_min\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SnippetSearch.__init__","title":"__init__","text":"<pre><code>__init__(k: int, period_length_days: int = 7, step_days: int = 1)\n</code></pre> <p>Initialize Snippet search.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of representative subsequences to select.</p> required <code>period_length_days</code> <code>int</code> <p>Number of days in each candidate subsequence.</p> <code>7</code> <code>step_days</code> <code>int</code> <p>Stride between consecutive candidate start positions.</p> <code>1</code> Source code in <code>energy_repset/search_algorithms/snippet.py</code> <pre><code>def __init__(\n    self, k: int, period_length_days: int = 7, step_days: int = 1,\n):\n    \"\"\"Initialize Snippet search.\n\n    Args:\n        k: Number of representative subsequences to select.\n        period_length_days: Number of days in each candidate subsequence.\n        step_days: Stride between consecutive candidate start positions.\n    \"\"\"\n    self.k = k\n    self.period_length_days = period_length_days\n    self.step_days = step_days\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.SnippetSearch.find_selection","title":"find_selection","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Find k representative subsequences via greedy p-median selection.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context. Must have <code>slicer.unit == 'day'</code>. Feature engineering should provide daily profile vectors in <code>df_features</code>, but the algorithm can also build profiles from <code>df_raw</code> directly.</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>RepSetResult with selected starting-day labels, pre-computed</p> <code>RepSetResult</code> <p>weights (fraction of days assigned), and total distance score.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>context.slicer.unit</code> is not <code>'day'</code>.</p> Source code in <code>energy_repset/search_algorithms/snippet.py</code> <pre><code>def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n    \"\"\"Find k representative subsequences via greedy p-median selection.\n\n    Args:\n        context: Problem context. Must have ``slicer.unit == 'day'``.\n            Feature engineering should provide daily profile vectors in\n            ``df_features``, but the algorithm can also build profiles\n            from ``df_raw`` directly.\n\n    Returns:\n        RepSetResult with selected starting-day labels, pre-computed\n        weights (fraction of days assigned), and total distance score.\n\n    Raises:\n        ValueError: If ``context.slicer.unit`` is not ``'day'``.\n    \"\"\"\n    if context.slicer.unit != 'day':\n        raise ValueError(\n            f\"SnippetSearch requires daily slicing (unit='day'), \"\n            f\"got unit='{context.slicer.unit}'.\"\n        )\n\n    daily_profiles = self._build_daily_profiles(context)\n    N = daily_profiles.shape[0]\n    day_labels = list(context.df_features.index)\n    L = self.period_length_days\n\n    candidates = self._generate_candidates(N, L, self.step_days)\n    if len(candidates) &lt; self.k:\n        raise ValueError(\n            f\"Only {len(candidates)} candidate subsequences available, \"\n            f\"but k={self.k} requested. Reduce k or period_length_days.\"\n        )\n\n    dist_matrix = self._compute_distance_matrix(daily_profiles, candidates)\n\n    selected_candidates, per_day_min = self._greedy_select(\n        dist_matrix, self.k\n    )\n\n    assignments = np.argmin(\n        dist_matrix[:, selected_candidates], axis=1\n    )\n\n    selection_labels = []\n    weight_dict = {}\n    for local_idx, cand_idx in enumerate(selected_candidates):\n        start_day = candidates[cand_idx][0]\n        label = day_labels[start_day]\n        selection_labels.append(label)\n        n_assigned = int(np.sum(assignments == local_idx))\n        weight_dict[label] = n_assigned / N\n\n    selection = tuple(selection_labels)\n    total_distance = float(np.sum(per_day_min))\n\n    slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n    representatives = {}\n    for label in selection:\n        start_idx = day_labels.index(label)\n        end_idx = min(start_idx + L, N)\n        period_labels = day_labels[start_idx:end_idx]\n        mask = slice_labels.isin(set(period_labels))\n        representatives[label] = context.df_raw.loc[mask]\n\n    return RepSetResult(\n        context=context,\n        selection_space='subset',\n        selection=selection,\n        scores={'total_distance': total_distance},\n        representatives=representatives,\n        weights=weight_dict,\n        diagnostics={\n            'assignments': assignments.tolist(),\n            'candidate_starts': [\n                day_labels[candidates[c][0]] for c in selected_candidates\n            ],\n        },\n    )\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.KMedoidsSearch","title":"KMedoidsSearch","text":"<p>               Bases: <code>SearchAlgorithm</code></p> <p>K-medoids clustering for representative subset selection.</p> <p>Wraps <code>sklearn_extra.cluster.KMedoids</code> to partition feature-space slices into k clusters and select the medoid of each cluster as a representative period. Weights are computed as the fraction of slices assigned to each cluster.</p> <p>This is a constructive (Workflow Type 2) algorithm: it has its own internal objective and does not require an external <code>ObjectiveSet</code>. The <code>RepresentationModel</code> is skipped by <code>RepSetExperiment.run()</code> because weights are pre-computed.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of clusters / representative periods.</p> required <code>metric</code> <code>str</code> <p>Distance metric for k-medoids (default <code>'euclidean'</code>).</p> <code>'euclidean'</code> <code>method</code> <code>str</code> <p>K-medoids algorithm variant. <code>'alternate'</code> (default) or <code>'pam'</code> (Partitioning Around Medoids, slower but optimal).</p> <code>'alternate'</code> <code>init</code> <code>str</code> <p>Initialization method (default <code>'k-medoids++'</code>).</p> <code>'k-medoids++'</code> <code>random_state</code> <code>int | None</code> <p>Seed for reproducibility.</p> <code>None</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>300</code> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; from energy_repset.search_algorithms import KMedoidsSearch\n&gt;&gt;&gt; search = KMedoidsSearch(k=4, random_state=42)\n&gt;&gt;&gt; result = search.find_selection(feature_context)\n&gt;&gt;&gt; result.selection  # Tuple of medoid labels\n&gt;&gt;&gt; result.weights    # Dict mapping labels to cluster-size fractions\n</code></pre> Source code in <code>energy_repset/search_algorithms/clustering.py</code> <pre><code>class KMedoidsSearch(SearchAlgorithm):\n    \"\"\"K-medoids clustering for representative subset selection.\n\n    Wraps ``sklearn_extra.cluster.KMedoids`` to partition feature-space\n    slices into k clusters and select the medoid of each cluster as a\n    representative period. Weights are computed as the fraction of slices\n    assigned to each cluster.\n\n    This is a constructive (Workflow Type 2) algorithm: it has its own\n    internal objective and does not require an external ``ObjectiveSet``.\n    The ``RepresentationModel`` is skipped by ``RepSetExperiment.run()``\n    because weights are pre-computed.\n\n    Args:\n        k: Number of clusters / representative periods.\n        metric: Distance metric for k-medoids (default ``'euclidean'``).\n        method: K-medoids algorithm variant. ``'alternate'`` (default) or\n            ``'pam'`` (Partitioning Around Medoids, slower but optimal).\n        init: Initialization method (default ``'k-medoids++'``).\n        random_state: Seed for reproducibility.\n        max_iter: Maximum number of iterations.\n\n    Examples:\n        Basic usage:\n\n        &gt;&gt;&gt; from energy_repset.search_algorithms import KMedoidsSearch\n        &gt;&gt;&gt; search = KMedoidsSearch(k=4, random_state=42)\n        &gt;&gt;&gt; result = search.find_selection(feature_context)\n        &gt;&gt;&gt; result.selection  # Tuple of medoid labels\n        &gt;&gt;&gt; result.weights    # Dict mapping labels to cluster-size fractions\n    \"\"\"\n\n    def __init__(\n        self,\n        k: int,\n        metric: str = 'euclidean',\n        method: str = 'alternate',\n        init: str = 'k-medoids++',\n        random_state: int | None = None,\n        max_iter: int = 300,\n    ):\n        \"\"\"Initialize k-medoids clustering search.\n\n        Args:\n            k: Number of clusters to produce.\n            metric: Distance metric passed to ``KMedoids``.\n            method: Algorithm variant (``'alternate'`` or ``'pam'``).\n            init: Medoid initialization strategy.\n            random_state: Random seed for reproducibility.\n            max_iter: Maximum iterations for convergence.\n        \"\"\"\n        self.k = k\n        self.metric = metric\n        self.method = method\n        self.init = init\n        self.random_state = random_state\n        self.max_iter = max_iter\n\n    def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n        \"\"\"Run k-medoids clustering on the feature space.\n\n        Args:\n            context: Problem context with ``df_features`` populated.\n\n        Returns:\n            RepSetResult with medoid labels as the selection, pre-computed\n            cluster-size-proportional weights, and WCSS (Within-Cluster Sum\n            of Squares) in ``scores``.\n        \"\"\"\n        Z = context.df_features.values\n        labels = list(context.df_features.index)\n\n        model = KMedoids(\n            n_clusters=self.k,\n            metric=self.metric,\n            method=self.method,\n            init=self.init,\n            random_state=self.random_state,\n            max_iter=self.max_iter,\n        )\n        model.fit(Z)\n\n        selection, weights, wcss, diagnostics = self._extract_results(\n            Z, labels, model.labels_, model.medoid_indices_\n        )\n        diagnostics['inertia'] = float(model.inertia_)\n        diagnostics['n_iter'] = int(model.n_iter_)\n\n        slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n        representatives = {\n            s: context.df_raw.loc[slice_labels == s] for s in selection\n        }\n\n        return RepSetResult(\n            context=context,\n            selection_space='subset',\n            selection=selection,\n            scores={'wcss': wcss},\n            representatives=representatives,\n            weights=weights,\n            diagnostics=diagnostics,\n        )\n\n    def _extract_results(\n        self,\n        Z: np.ndarray,\n        labels: list,\n        cluster_labels: np.ndarray,\n        medoid_indices: np.ndarray,\n    ) -&gt; tuple:\n        \"\"\"Extract selection, weights, WCSS, and diagnostics from clustering.\n\n        Args:\n            Z: Feature matrix (N x p).\n            labels: Slice labels aligned with rows of Z.\n            cluster_labels: Cluster assignment for each slice (length N).\n            medoid_indices: Row indices of medoids in Z.\n\n        Returns:\n            Tuple of (selection, weights, wcss, diagnostics).\n        \"\"\"\n        unique_clusters = sorted(set(cluster_labels))\n        N = len(labels)\n        wcss = 0.0\n\n        selected_labels = []\n        weight_dict: Dict[Any, float] = {}\n        cluster_info: list[Dict[str, Any]] = []\n\n        for c in unique_clusters:\n            mask = cluster_labels == c\n            indices = np.where(mask)[0]\n            cluster_Z = Z[indices]\n            centroid = cluster_Z.mean(axis=0)\n\n            cluster_wcss = float(np.sum((cluster_Z - centroid) ** 2))\n            wcss += cluster_wcss\n\n            medoid_idx = medoid_indices[c]\n            rep_label = labels[medoid_idx]\n\n            fraction = len(indices) / N\n            selected_labels.append(rep_label)\n            weight_dict[rep_label] = fraction\n\n            member_labels = [labels[i] for i in indices]\n            cluster_info.append({\n                'cluster': int(c),\n                'medoid': rep_label,\n                'size': len(indices),\n                'members': member_labels,\n            })\n\n        selection = tuple(selected_labels)\n        diagnostics = {\n            'cluster_labels': cluster_labels.tolist(),\n            'cluster_info': cluster_info,\n        }\n\n        return selection, weight_dict, float(wcss), diagnostics\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.KMedoidsSearch.__init__","title":"__init__","text":"<pre><code>__init__(k: int, metric: str = 'euclidean', method: str = 'alternate', init: str = 'k-medoids++', random_state: int | None = None, max_iter: int = 300)\n</code></pre> <p>Initialize k-medoids clustering search.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of clusters to produce.</p> required <code>metric</code> <code>str</code> <p>Distance metric passed to <code>KMedoids</code>.</p> <code>'euclidean'</code> <code>method</code> <code>str</code> <p>Algorithm variant (<code>'alternate'</code> or <code>'pam'</code>).</p> <code>'alternate'</code> <code>init</code> <code>str</code> <p>Medoid initialization strategy.</p> <code>'k-medoids++'</code> <code>random_state</code> <code>int | None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations for convergence.</p> <code>300</code> Source code in <code>energy_repset/search_algorithms/clustering.py</code> <pre><code>def __init__(\n    self,\n    k: int,\n    metric: str = 'euclidean',\n    method: str = 'alternate',\n    init: str = 'k-medoids++',\n    random_state: int | None = None,\n    max_iter: int = 300,\n):\n    \"\"\"Initialize k-medoids clustering search.\n\n    Args:\n        k: Number of clusters to produce.\n        metric: Distance metric passed to ``KMedoids``.\n        method: Algorithm variant (``'alternate'`` or ``'pam'``).\n        init: Medoid initialization strategy.\n        random_state: Random seed for reproducibility.\n        max_iter: Maximum iterations for convergence.\n    \"\"\"\n    self.k = k\n    self.metric = metric\n    self.method = method\n    self.init = init\n    self.random_state = random_state\n    self.max_iter = max_iter\n</code></pre>"},{"location":"api/search_algorithms/#energy_repset.search_algorithms.KMedoidsSearch.find_selection","title":"find_selection","text":"<pre><code>find_selection(context: ProblemContext) -&gt; RepSetResult\n</code></pre> <p>Run k-medoids clustering on the feature space.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>Problem context with <code>df_features</code> populated.</p> required <p>Returns:</p> Type Description <code>RepSetResult</code> <p>RepSetResult with medoid labels as the selection, pre-computed</p> <code>RepSetResult</code> <p>cluster-size-proportional weights, and WCSS (Within-Cluster Sum</p> <code>RepSetResult</code> <p>of Squares) in <code>scores</code>.</p> Source code in <code>energy_repset/search_algorithms/clustering.py</code> <pre><code>def find_selection(self, context: ProblemContext) -&gt; RepSetResult:\n    \"\"\"Run k-medoids clustering on the feature space.\n\n    Args:\n        context: Problem context with ``df_features`` populated.\n\n    Returns:\n        RepSetResult with medoid labels as the selection, pre-computed\n        cluster-size-proportional weights, and WCSS (Within-Cluster Sum\n        of Squares) in ``scores``.\n    \"\"\"\n    Z = context.df_features.values\n    labels = list(context.df_features.index)\n\n    model = KMedoids(\n        n_clusters=self.k,\n        metric=self.metric,\n        method=self.method,\n        init=self.init,\n        random_state=self.random_state,\n        max_iter=self.max_iter,\n    )\n    model.fit(Z)\n\n    selection, weights, wcss, diagnostics = self._extract_results(\n        Z, labels, model.labels_, model.medoid_indices_\n    )\n    diagnostics['inertia'] = float(model.inertia_)\n    diagnostics['n_iter'] = int(model.n_iter_)\n\n    slice_labels = context.slicer.labels_for_index(context.df_raw.index)\n    representatives = {\n        s: context.df_raw.loc[slice_labels == s] for s in selection\n    }\n\n    return RepSetResult(\n        context=context,\n        selection_space='subset',\n        selection=selection,\n        scores={'wcss': wcss},\n        representatives=representatives,\n        weights=weights,\n        diagnostics=diagnostics,\n    )\n</code></pre>"},{"location":"api/selection_policies/","title":"Selection Policies","text":""},{"location":"api/selection_policies/#energy_repset.selection_policies.SelectionPolicy","title":"SelectionPolicy","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for selection policies that choose the best combination.</p> <p>Selection policies define the strategy for choosing the winning combination from a set of scored candidates. Different policies implement different trade-offs between competing objectives (e.g., weighted sum vs. Pareto).</p> <p>This is a key component of the Generate-and-Test workflow where the SearchAlgorithm generates candidates, the ObjectiveSet scores them, and the SelectionPolicy picks the winner.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # See WeightedSumPolicy and ParetoUtopiaPolicy for concrete examples\n&gt;&gt;&gt; class SimpleMinPolicy(SelectionPolicy):\n...     def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet):\n...         # Just pick the row with minimum of first objective\n...         first_obj = list(objective_set.component_meta().keys())[0]\n...         best_row = evaluations_df.loc[evaluations_df[first_obj].idxmin()]\n...         return tuple(best_row['slices'])\n</code></pre> Source code in <code>energy_repset/selection_policies/policy.py</code> <pre><code>class SelectionPolicy(ABC):\n    \"\"\"Base class for selection policies that choose the best combination.\n\n    Selection policies define the strategy for choosing the winning combination\n    from a set of scored candidates. Different policies implement different\n    trade-offs between competing objectives (e.g., weighted sum vs. Pareto).\n\n    This is a key component of the Generate-and-Test workflow where the\n    SearchAlgorithm generates candidates, the ObjectiveSet scores them, and\n    the SelectionPolicy picks the winner.\n\n    Examples:\n        &gt;&gt;&gt; # See WeightedSumPolicy and ParetoUtopiaPolicy for concrete examples\n        &gt;&gt;&gt; class SimpleMinPolicy(SelectionPolicy):\n        ...     def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet):\n        ...         # Just pick the row with minimum of first objective\n        ...         first_obj = list(objective_set.component_meta().keys())[0]\n        ...         best_row = evaluations_df.loc[evaluations_df[first_obj].idxmin()]\n        ...         return tuple(best_row['slices'])\n    \"\"\"\n\n    @abstractmethod\n    def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n        \"\"\"Select the best combination from scored candidates.\n\n        Args:\n            evaluations_df: DataFrame where each row is a candidate combination\n                with columns 'slices' (the combination tuple) and score columns\n                for each objective component.\n            objective_set: Provides metadata about score components (direction,\n                weights, etc.) needed for selection logic.\n\n        Returns:\n            Tuple of slice identifiers representing the winning combination.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.SelectionPolicy.select_best","title":"select_best  <code>abstractmethod</code>","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select the best combination from scored candidates.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations_df</code> <code>DataFrame</code> <p>DataFrame where each row is a candidate combination with columns 'slices' (the combination tuple) and score columns for each objective component.</p> required <code>objective_set</code> <code>ObjectiveSet</code> <p>Provides metadata about score components (direction, weights, etc.) needed for selection logic.</p> required <p>Returns:</p> Type Description <code>tuple[Hashable, ...]</code> <p>Tuple of slice identifiers representing the winning combination.</p> Source code in <code>energy_repset/selection_policies/policy.py</code> <pre><code>@abstractmethod\ndef select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n    \"\"\"Select the best combination from scored candidates.\n\n    Args:\n        evaluations_df: DataFrame where each row is a candidate combination\n            with columns 'slices' (the combination tuple) and score columns\n            for each objective component.\n        objective_set: Provides metadata about score components (direction,\n            weights, etc.) needed for selection logic.\n\n    Returns:\n        Tuple of slice identifiers representing the winning combination.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.PolicyOutcome","title":"PolicyOutcome  <code>dataclass</code>","text":"Source code in <code>energy_repset/selection_policies/policy.py</code> <pre><code>@dataclass(frozen=True)\nclass PolicyOutcome:\n    algorithm: SearchAlgorithm\n    selected: RepSetResult\n    scores_annotated: pd.DataFrame\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.WeightedSumPolicy","title":"WeightedSumPolicy","text":"<p>               Bases: <code>SelectionPolicy</code></p> <p>Selects the combination minimizing a weighted sum of objectives.</p> <p>Combines multiple objectives into a single scalar score using weighted averaging. Objectives are oriented for minimization (max objectives are negated), optionally normalized, then combined using weights from the ObjectiveSet (which can be overridden).</p> <p>This is the simplest multi-objective selection strategy and works well when relative importance of objectives is known.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from energy_repset import ObjectiveSet, ObjectiveSpec\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity, CorrelationFidelity\n&gt;&gt;&gt; # Default: use weights from ObjectiveSet\n&gt;&gt;&gt; policy = WeightedSumPolicy()\n&gt;&gt;&gt; objectives = ObjectiveSet([\n...     ObjectiveSpec('wasserstein', WassersteinFidelity(), weight=1.0),\n...     ObjectiveSpec('correlation', CorrelationFidelity(), weight=0.5)\n... ])\n&gt;&gt;&gt; # Final score = 1.0*wasserstein + 0.5*correlation\n</code></pre> <pre><code>&gt;&gt;&gt; # Override weights in policy\n&gt;&gt;&gt; policy = WeightedSumPolicy(\n...     overrides={'wasserstein': 2.0, 'correlation': 1.0}\n... )\n&gt;&gt;&gt; # Final score = 2.0*wasserstein + 1.0*correlation\n</code></pre> <pre><code>&gt;&gt;&gt; # With normalization to make objectives comparable\n&gt;&gt;&gt; policy = WeightedSumPolicy(\n...     normalization='robust_minmax',  # Scale to [0, 1] using 5th-95th percentiles\n...     tie_breakers=('wasserstein',),  # Break ties by wasserstein\n...     tie_dirs=('min',)\n... )\n</code></pre> Source code in <code>energy_repset/selection_policies/weighted_sum.py</code> <pre><code>class WeightedSumPolicy(SelectionPolicy):\n    \"\"\"Selects the combination minimizing a weighted sum of objectives.\n\n    Combines multiple objectives into a single scalar score using weighted\n    averaging. Objectives are oriented for minimization (max objectives are\n    negated), optionally normalized, then combined using weights from the\n    ObjectiveSet (which can be overridden).\n\n    This is the simplest multi-objective selection strategy and works well\n    when relative importance of objectives is known.\n\n    Examples:\n        &gt;&gt;&gt; from energy_repset import ObjectiveSet, ObjectiveSpec\n        &gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity, CorrelationFidelity\n        &gt;&gt;&gt; # Default: use weights from ObjectiveSet\n        &gt;&gt;&gt; policy = WeightedSumPolicy()\n        &gt;&gt;&gt; objectives = ObjectiveSet([\n        ...     ObjectiveSpec('wasserstein', WassersteinFidelity(), weight=1.0),\n        ...     ObjectiveSpec('correlation', CorrelationFidelity(), weight=0.5)\n        ... ])\n        &gt;&gt;&gt; # Final score = 1.0*wasserstein + 0.5*correlation\n\n        &gt;&gt;&gt; # Override weights in policy\n        &gt;&gt;&gt; policy = WeightedSumPolicy(\n        ...     overrides={'wasserstein': 2.0, 'correlation': 1.0}\n        ... )\n        &gt;&gt;&gt; # Final score = 2.0*wasserstein + 1.0*correlation\n\n        &gt;&gt;&gt; # With normalization to make objectives comparable\n        &gt;&gt;&gt; policy = WeightedSumPolicy(\n        ...     normalization='robust_minmax',  # Scale to [0, 1] using 5th-95th percentiles\n        ...     tie_breakers=('wasserstein',),  # Break ties by wasserstein\n        ...     tie_dirs=('min',)\n        ... )\n    \"\"\"\n    def __init__(\n            self,\n            overrides: Optional[Dict[str, float]] = None,\n            normalization: Normalization = \"none\",\n            tie_breakers: Tuple[str, ...] = (),\n            tie_dirs: Tuple[ScoreComponentDirection, ...] = (),\n    ) -&gt; None:\n        \"\"\"Initialize weighted sum policy.\n\n        Args:\n            overrides: Optional dict mapping objective names to weights,\n                overriding weights from ObjectiveSet.\n            normalization: How to normalize objectives before weighting:\n                - \"none\": No normalization\n                - \"robust_minmax\": Scale to [0, 1] using 5th-95th percentiles\n                - \"zscore_iqr\": Z-score using median and IQR\n            tie_breakers: Tuple of objective names to use for tie-breaking.\n            tie_dirs: Corresponding directions (\"min\" or \"max\") for tie-breakers.\n        \"\"\"\n        self.overrides = overrides or {}\n        self.normalization = normalization\n        self.tie_breakers = tie_breakers\n        self.tie_dirs = tie_dirs\n\n    def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n        \"\"\"Select combination with minimum weighted sum score.\n\n        Args:\n            evaluations_df: DataFrame with 'slices' column and objective scores.\n            objective_set: Provides component metadata (direction, weights).\n\n        Returns:\n            Tuple of slice identifiers with the lowest weighted sum score.\n        \"\"\"\n        df = evaluations_df.copy()\n        meta = objective_set.component_meta()\n        oriented = df[list(meta.keys())].copy()\n\n        # Orient all objectives for minimization\n        for name, m in meta.items():\n            if m[\"direction\"] == \"max\":\n                oriented[name] = -oriented[name]\n\n        # Normalize if requested\n        Z = self._normalize(oriented, mode=self.normalization)\n\n        # Compute weights (preferences from ObjectiveSet, overrides from strategy)\n        weights = {name: float(m[\"pref\"]) for name, m in meta.items()}\n        for k, v in self.overrides.items():\n            if k not in weights:\n                raise ValueError(f\"Unknown metric in overrides: {k}\")\n            weights[k] = float(v)\n\n        # Compute weighted sum scores\n        df[\"strategy_score\"] = sum(Z[name] * w for name, w in weights.items())\n\n        # Find best solution\n        best = df.sort_values(\"strategy_score\", ascending=True)\n        if len(best) &gt; 1 and len(self.tie_breakers) &gt; 0:\n            for col, d in zip(self.tie_breakers, self.tie_dirs):\n                best = best.sort_values(col, ascending=(d == \"min\"))\n\n        return tuple(best.iloc[0][\"slices\"])\n\n    def _normalize(self, Y: pd.DataFrame, mode: Normalization) -&gt; pd.DataFrame:\n        if mode == \"none\":\n            return Y\n        if mode == \"robust_minmax\":\n            q_lo = Y.quantile(0.05)\n            q_hi = Y.quantile(0.95)\n            denom = (q_hi - q_lo).replace(0, 1.0)\n            return ((Y - q_lo) / denom).clip(lower=0.0)\n        med = Y.median()\n        iqr = (Y.quantile(0.75) - Y.quantile(0.25)).replace(0, 1.0)\n        return (Y - med) / iqr\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.WeightedSumPolicy.__init__","title":"__init__","text":"<pre><code>__init__(overrides: dict[str, float] | None = None, normalization: Normalization = 'none', tie_breakers: tuple[str, ...] = (), tie_dirs: tuple[ScoreComponentDirection, ...] = ()) -&gt; None\n</code></pre> <p>Initialize weighted sum policy.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>dict[str, float] | None</code> <p>Optional dict mapping objective names to weights, overriding weights from ObjectiveSet.</p> <code>None</code> <code>normalization</code> <code>Normalization</code> <p>How to normalize objectives before weighting: - \"none\": No normalization - \"robust_minmax\": Scale to [0, 1] using 5th-95th percentiles - \"zscore_iqr\": Z-score using median and IQR</p> <code>'none'</code> <code>tie_breakers</code> <code>tuple[str, ...]</code> <p>Tuple of objective names to use for tie-breaking.</p> <code>()</code> <code>tie_dirs</code> <code>tuple[ScoreComponentDirection, ...]</code> <p>Corresponding directions (\"min\" or \"max\") for tie-breakers.</p> <code>()</code> Source code in <code>energy_repset/selection_policies/weighted_sum.py</code> <pre><code>def __init__(\n        self,\n        overrides: Optional[Dict[str, float]] = None,\n        normalization: Normalization = \"none\",\n        tie_breakers: Tuple[str, ...] = (),\n        tie_dirs: Tuple[ScoreComponentDirection, ...] = (),\n) -&gt; None:\n    \"\"\"Initialize weighted sum policy.\n\n    Args:\n        overrides: Optional dict mapping objective names to weights,\n            overriding weights from ObjectiveSet.\n        normalization: How to normalize objectives before weighting:\n            - \"none\": No normalization\n            - \"robust_minmax\": Scale to [0, 1] using 5th-95th percentiles\n            - \"zscore_iqr\": Z-score using median and IQR\n        tie_breakers: Tuple of objective names to use for tie-breaking.\n        tie_dirs: Corresponding directions (\"min\" or \"max\") for tie-breakers.\n    \"\"\"\n    self.overrides = overrides or {}\n    self.normalization = normalization\n    self.tie_breakers = tie_breakers\n    self.tie_dirs = tie_dirs\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.WeightedSumPolicy.select_best","title":"select_best","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select combination with minimum weighted sum score.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations_df</code> <code>DataFrame</code> <p>DataFrame with 'slices' column and objective scores.</p> required <code>objective_set</code> <code>ObjectiveSet</code> <p>Provides component metadata (direction, weights).</p> required <p>Returns:</p> Type Description <code>tuple[Hashable, ...]</code> <p>Tuple of slice identifiers with the lowest weighted sum score.</p> Source code in <code>energy_repset/selection_policies/weighted_sum.py</code> <pre><code>def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n    \"\"\"Select combination with minimum weighted sum score.\n\n    Args:\n        evaluations_df: DataFrame with 'slices' column and objective scores.\n        objective_set: Provides component metadata (direction, weights).\n\n    Returns:\n        Tuple of slice identifiers with the lowest weighted sum score.\n    \"\"\"\n    df = evaluations_df.copy()\n    meta = objective_set.component_meta()\n    oriented = df[list(meta.keys())].copy()\n\n    # Orient all objectives for minimization\n    for name, m in meta.items():\n        if m[\"direction\"] == \"max\":\n            oriented[name] = -oriented[name]\n\n    # Normalize if requested\n    Z = self._normalize(oriented, mode=self.normalization)\n\n    # Compute weights (preferences from ObjectiveSet, overrides from strategy)\n    weights = {name: float(m[\"pref\"]) for name, m in meta.items()}\n    for k, v in self.overrides.items():\n        if k not in weights:\n            raise ValueError(f\"Unknown metric in overrides: {k}\")\n        weights[k] = float(v)\n\n    # Compute weighted sum scores\n    df[\"strategy_score\"] = sum(Z[name] * w for name, w in weights.items())\n\n    # Find best solution\n    best = df.sort_values(\"strategy_score\", ascending=True)\n    if len(best) &gt; 1 and len(self.tie_breakers) &gt; 0:\n        for col, d in zip(self.tie_breakers, self.tie_dirs):\n            best = best.sort_values(col, ascending=(d == \"min\"))\n\n    return tuple(best.iloc[0][\"slices\"])\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoMaxMinStrategy","title":"ParetoMaxMinStrategy","text":"<p>               Bases: <code>ParetoUtopiaPolicy</code></p> Source code in <code>energy_repset/selection_policies/pareto.py</code> <pre><code>class ParetoMaxMinStrategy(ParetoUtopiaPolicy):\n\n    def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n        \"\"\"Select best solution using Pareto max-min approach.\"\"\"\n        df = evaluations_df.copy()\n        dirs = self._resolve_objectives_from_meta(objective_set.component_meta(), df)\n        feas = self._apply_constraints(df, self.fairness_constraints)\n        df[\"feasible\"] = feas\n        Y = df[list(dirs.keys())].copy()\n\n        # Orient all objectives for minimization\n        for c, d in dirs.items():\n            if d == \"max\":\n                Y[c] = -Y[c]\n\n        # Find Pareto front\n        pareto_mask = self._pareto_mask(Y[feas])\n        df[\"pareto\"] = False\n        df.loc[feas.index[feas].tolist(), \"pareto\"] = pareto_mask.values\n\n        # Store masks for diagnostics\n        self.pareto_mask = df[\"pareto\"].copy()\n        self.feasible_mask = df[\"feasible\"].copy()\n\n        # Normalize and compute max-min score\n        Z = self._normalize(Y, self.normalization)\n        ideal = Z[df[\"feasible\"]].min(axis=0)\n        slack = 1.0 - (Z - ideal.values)\n        df[\"maxmin_score\"] = slack.min(axis=1)\n\n        # Select from Pareto front\n        front = df[(df[\"feasible\"]) &amp; (df[\"pareto\"])]\n        if len(front) == 0:\n            front = df[df[\"feasible\"]] if df[\"feasible\"].any() else df\n\n        best = front.sort_values(\"maxmin_score\", ascending=False)\n        if len(best) &gt; 1 and len(self.tie_breakers) &gt; 0:\n            for col, d in zip(self.tie_breakers, self.tie_dirs):\n                best = best.sort_values(col, ascending=(d == \"min\"))\n\n        return tuple(best.iloc[0][\"slices\"])\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoMaxMinStrategy.select_best","title":"select_best","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select best solution using Pareto max-min approach.</p> Source code in <code>energy_repset/selection_policies/pareto.py</code> <pre><code>def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n    \"\"\"Select best solution using Pareto max-min approach.\"\"\"\n    df = evaluations_df.copy()\n    dirs = self._resolve_objectives_from_meta(objective_set.component_meta(), df)\n    feas = self._apply_constraints(df, self.fairness_constraints)\n    df[\"feasible\"] = feas\n    Y = df[list(dirs.keys())].copy()\n\n    # Orient all objectives for minimization\n    for c, d in dirs.items():\n        if d == \"max\":\n            Y[c] = -Y[c]\n\n    # Find Pareto front\n    pareto_mask = self._pareto_mask(Y[feas])\n    df[\"pareto\"] = False\n    df.loc[feas.index[feas].tolist(), \"pareto\"] = pareto_mask.values\n\n    # Store masks for diagnostics\n    self.pareto_mask = df[\"pareto\"].copy()\n    self.feasible_mask = df[\"feasible\"].copy()\n\n    # Normalize and compute max-min score\n    Z = self._normalize(Y, self.normalization)\n    ideal = Z[df[\"feasible\"]].min(axis=0)\n    slack = 1.0 - (Z - ideal.values)\n    df[\"maxmin_score\"] = slack.min(axis=1)\n\n    # Select from Pareto front\n    front = df[(df[\"feasible\"]) &amp; (df[\"pareto\"])]\n    if len(front) == 0:\n        front = df[df[\"feasible\"]] if df[\"feasible\"].any() else df\n\n    best = front.sort_values(\"maxmin_score\", ascending=False)\n    if len(best) &gt; 1 and len(self.tie_breakers) &gt; 0:\n        for col, d in zip(self.tie_breakers, self.tie_dirs):\n            best = best.sort_values(col, ascending=(d == \"min\"))\n\n    return tuple(best.iloc[0][\"slices\"])\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoUtopiaPolicy","title":"ParetoUtopiaPolicy","text":"<p>               Bases: <code>SelectionPolicy</code></p> Source code in <code>energy_repset/selection_policies/pareto.py</code> <pre><code>class ParetoUtopiaPolicy(SelectionPolicy):\n    def __init__(\n            self,\n            objectives: Optional[Dict[str, ScoreComponentDirection]] = None,\n            normalization: Normalization = \"robust_minmax\",\n            fairness_constraints: Optional[Dict[str, float]] = None,\n            distance: Literal[\"chebyshev\", \"euclidean\"] = \"chebyshev\",\n            tie_breakers: Tuple[str, ...] = (),\n            tie_dirs: Tuple[ScoreComponentDirection, ...] = (),\n            eps: float = 1e-9,\n    ) -&gt; None:\n        self.objectives = objectives\n        self.normalization = normalization\n        self.fairness_constraints = fairness_constraints or {}\n        self.distance = distance\n        self.tie_breakers = tie_breakers\n        self.tie_dirs = tie_dirs\n        self.eps = eps\n        self.pareto_mask: pd.Series | None = None\n        self.feasible_mask: pd.Series | None = None\n\n    def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n        \"\"\"Select best solution using Pareto utopia approach.\"\"\"\n        df = evaluations_df.copy()\n        dirs = self._resolve_objectives_from_meta(objective_set.component_meta(), df)\n        feas = self._apply_constraints(df, self.fairness_constraints)\n        df[\"feasible\"] = feas\n        Y = df[list(dirs.keys())].copy()\n\n        # Orient all objectives for minimization\n        for c, d in dirs.items():\n            if d == \"max\":\n                Y[c] = -Y[c]\n\n        # Find Pareto front\n        pareto_mask = self._pareto_mask(Y[feas])\n        df[\"pareto\"] = False\n        df.loc[feas.index[feas].tolist(), \"pareto\"] = pareto_mask.values\n\n        # Store masks for diagnostics\n        self.pareto_mask = df[\"pareto\"].copy()\n        self.feasible_mask = df[\"feasible\"].copy()\n\n        # Normalize and compute utopia distance\n        Z = self._normalize(Y, self.normalization)\n        ideal = Z[df[\"feasible\"]].min(axis=0)\n        dist = self._dist(Z, ideal, self.distance)\n        df[\"utopia_distance\"] = dist\n\n        # Select from Pareto front\n        front = df[(df[\"feasible\"]) &amp; (df[\"pareto\"])]\n        if len(front) == 0:\n            front = df[df[\"feasible\"]] if df[\"feasible\"].any() else df\n\n        best = front.sort_values(\"utopia_distance\", ascending=True)\n        if len(best) &gt; 1 and len(self.tie_breakers) &gt; 0:\n            for col, d in zip(self.tie_breakers, self.tie_dirs):\n                best = best.sort_values(col, ascending=(d == \"min\"))\n\n        return tuple(best.iloc[0][\"slices\"])\n\n    def _resolve_objectives(self, objective_set: ObjectiveSet, df: pd.DataFrame) -&gt; Dict[str, ScoreComponentDirection]:\n        \"\"\"Legacy method for backward compatibility.\"\"\"\n        if self.objectives is not None:\n            return self.objectives\n        meta = objective_set.component_meta()\n        return {name: info[\"direction\"] for name, info in meta.items() if name in df.columns}\n\n    def _resolve_objectives_from_meta(self, meta: Dict[str, Dict[str, any]], df: pd.DataFrame) -&gt; Dict[str, ScoreComponentDirection]:\n        \"\"\"Resolve objectives from component metadata.\"\"\"\n        if self.objectives is not None:\n            return self.objectives\n        return {name: info[\"direction\"] for name, info in meta.items() if name in df.columns}\n\n    def _apply_constraints(self, df: pd.DataFrame, cons: Dict[str, float]) -&gt; pd.Series:\n        if not cons:\n            return pd.Series(True, index=df.index)\n        mask = pd.Series(True, index=df.index)\n        for col, thr in cons.items():\n            if col not in df.columns:\n                raise ValueError(f\"Unknown constraint metric: {col}\")\n            mask &amp;= df[col] &lt;= thr\n        return mask\n\n    def _normalize(self, Y: pd.DataFrame, mode: Normalization) -&gt; pd.DataFrame:\n        if mode == \"robust_minmax\":\n            q_lo = Y.quantile(0.05)\n            q_hi = Y.quantile(0.95)\n            denom = (q_hi - q_lo).replace(0, 1.0)\n            return ((Y - q_lo) / denom).clip(lower=0.0)\n        if mode == \"zscore_iqr\":\n            med = Y.median()\n            iqr = (Y.quantile(0.75) - Y.quantile(0.25)).replace(0, 1.0)\n            return (Y - med) / iqr\n        return Y\n\n    def _dist(self, Z: pd.DataFrame, ideal: pd.Series, kind: str) -&gt; pd.Series:\n        D = (Z - ideal.values)\n        if kind == \"chebyshev\":\n            return D.abs().max(axis=1)\n        return np.sqrt((D.pow(2)).sum(axis=1))\n\n    def _pareto_mask(self, Y: pd.DataFrame) -&gt; pd.Series:\n        A = Y.values\n        n = A.shape[0]\n        mask = np.ones(n, dtype=bool)\n        for i in range(n):\n            if not mask[i]:\n                continue\n            for j in range(n):\n                if i == j:\n                    continue\n                if self._dominates(A[j], A[i]):\n                    mask[i] = False\n                    break\n        return pd.Series(mask, index=Y.index)\n\n    def _dominates(self, a: np.ndarray, b: np.ndarray) -&gt; bool:\n        return np.all(a &lt;= b + self.eps) and np.any(a &lt; b - self.eps)\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoUtopiaPolicy.select_best","title":"select_best","text":"<pre><code>select_best(evaluations_df: DataFrame, objective_set: ObjectiveSet) -&gt; tuple[Hashable, ...]\n</code></pre> <p>Select best solution using Pareto utopia approach.</p> Source code in <code>energy_repset/selection_policies/pareto.py</code> <pre><code>def select_best(self, evaluations_df: pd.DataFrame, objective_set: ObjectiveSet) -&gt; Tuple[Hashable, ...]:\n    \"\"\"Select best solution using Pareto utopia approach.\"\"\"\n    df = evaluations_df.copy()\n    dirs = self._resolve_objectives_from_meta(objective_set.component_meta(), df)\n    feas = self._apply_constraints(df, self.fairness_constraints)\n    df[\"feasible\"] = feas\n    Y = df[list(dirs.keys())].copy()\n\n    # Orient all objectives for minimization\n    for c, d in dirs.items():\n        if d == \"max\":\n            Y[c] = -Y[c]\n\n    # Find Pareto front\n    pareto_mask = self._pareto_mask(Y[feas])\n    df[\"pareto\"] = False\n    df.loc[feas.index[feas].tolist(), \"pareto\"] = pareto_mask.values\n\n    # Store masks for diagnostics\n    self.pareto_mask = df[\"pareto\"].copy()\n    self.feasible_mask = df[\"feasible\"].copy()\n\n    # Normalize and compute utopia distance\n    Z = self._normalize(Y, self.normalization)\n    ideal = Z[df[\"feasible\"]].min(axis=0)\n    dist = self._dist(Z, ideal, self.distance)\n    df[\"utopia_distance\"] = dist\n\n    # Select from Pareto front\n    front = df[(df[\"feasible\"]) &amp; (df[\"pareto\"])]\n    if len(front) == 0:\n        front = df[df[\"feasible\"]] if df[\"feasible\"].any() else df\n\n    best = front.sort_values(\"utopia_distance\", ascending=True)\n    if len(best) &gt; 1 and len(self.tie_breakers) &gt; 0:\n        for col, d in zip(self.tie_breakers, self.tie_dirs):\n            best = best.sort_values(col, ascending=(d == \"min\"))\n\n    return tuple(best.iloc[0][\"slices\"])\n</code></pre>"},{"location":"api/selection_policies/#energy_repset.selection_policies.ParetoOutcome","title":"ParetoOutcome  <code>dataclass</code>","text":"<p>               Bases: <code>PolicyOutcome</code></p> Source code in <code>energy_repset/selection_policies/pareto.py</code> <pre><code>@dataclass(frozen=True)\nclass ParetoOutcome(PolicyOutcome):\n    objectives: Dict[str, ScoreComponentDirection]\n    feasible_mask_col: str\n    pareto_mask_col: str\n    score_col: str\n</code></pre>"},{"location":"api/workflow/","title":"Workflow &amp; Experiment","text":""},{"location":"api/workflow/#energy_repset.workflow.Workflow","title":"Workflow  <code>dataclass</code>","text":"<p>A serializable object that defines a complete selection problem.</p> <p>This dataclass encapsulates all components needed to execute a representative subset selection workflow: feature engineering, search algorithm, and optionally a representation model.</p> <p>Attributes:</p> Name Type Description <code>feature_engineer</code> <code>FeatureEngineer</code> <p>Component that transforms raw time-series into features.</p> <code>search_algorithm</code> <code>SearchAlgorithm</code> <p>Algorithm that finds the optimal subset of k periods.</p> <code>representation_model</code> <code>RepresentationModel | None</code> <p>Model that calculates responsibility weights for selected periods. <code>None</code> when the search algorithm pre-computes its own weights (e.g. constructive algorithms like <code>KMedoidsSearch</code>).</p> <code>k</code> <code>RepresentationModel | None</code> <p>Number of representative periods to select.</p> <p>Examples:</p> <p>Define a complete workflow:</p> <pre><code>&gt;&gt;&gt; from energy_repset.workflow import Workflow\n&gt;&gt;&gt; from energy_repset.feature_engineering import StandardStatsFeatureEngineer\n&gt;&gt;&gt; from energy_repset.search_algorithms import ObjectiveDrivenCombinatorialSearchAlgorithm\n&gt;&gt;&gt; from energy_repset.representation import UniformRepresentationModel\n&gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n&gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n&gt;&gt;&gt; from energy_repset.selection_policies import ParetoMaxMinStrategy\n&gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create components\n&gt;&gt;&gt; feature_eng = StandardStatsFeatureEngineer()\n&gt;&gt;&gt; objective_set = ObjectiveSet({'wass': (1.0, WassersteinFidelity())})\n&gt;&gt;&gt; policy = ParetoMaxMinStrategy()\n&gt;&gt;&gt; combi_gen = ExhaustiveCombiGen(k=3)\n&gt;&gt;&gt; search_algo = ObjectiveDrivenCombinatorialSearchAlgorithm(\n...     objective_set, policy, combi_gen\n... )\n&gt;&gt;&gt; repr_model = UniformRepresentationModel()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create workflow\n&gt;&gt;&gt; workflow = Workflow(\n...     feature_engineer=feature_eng,\n...     search_algorithm=search_algo,\n...     representation_model=repr_model,\n... )\n</code></pre> Source code in <code>energy_repset/workflow.py</code> <pre><code>@dataclass\nclass Workflow:\n    \"\"\"A serializable object that defines a complete selection problem.\n\n    This dataclass encapsulates all components needed to execute a representative\n    subset selection workflow: feature engineering, search algorithm, and\n    optionally a representation model.\n\n    Attributes:\n        feature_engineer: Component that transforms raw time-series into features.\n        search_algorithm: Algorithm that finds the optimal subset of k periods.\n        representation_model: Model that calculates responsibility weights for\n            selected periods. ``None`` when the search algorithm pre-computes\n            its own weights (e.g. constructive algorithms like\n            ``KMedoidsSearch``).\n        k: Number of representative periods to select.\n\n    Examples:\n        Define a complete workflow:\n\n        &gt;&gt;&gt; from energy_repset.workflow import Workflow\n        &gt;&gt;&gt; from energy_repset.feature_engineering import StandardStatsFeatureEngineer\n        &gt;&gt;&gt; from energy_repset.search_algorithms import ObjectiveDrivenCombinatorialSearchAlgorithm\n        &gt;&gt;&gt; from energy_repset.representation import UniformRepresentationModel\n        &gt;&gt;&gt; from energy_repset.objectives import ObjectiveSet\n        &gt;&gt;&gt; from energy_repset.score_components import WassersteinFidelity\n        &gt;&gt;&gt; from energy_repset.selection_policies import ParetoMaxMinStrategy\n        &gt;&gt;&gt; from energy_repset.combi_gens import ExhaustiveCombiGen\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create components\n        &gt;&gt;&gt; feature_eng = StandardStatsFeatureEngineer()\n        &gt;&gt;&gt; objective_set = ObjectiveSet({'wass': (1.0, WassersteinFidelity())})\n        &gt;&gt;&gt; policy = ParetoMaxMinStrategy()\n        &gt;&gt;&gt; combi_gen = ExhaustiveCombiGen(k=3)\n        &gt;&gt;&gt; search_algo = ObjectiveDrivenCombinatorialSearchAlgorithm(\n        ...     objective_set, policy, combi_gen\n        ... )\n        &gt;&gt;&gt; repr_model = UniformRepresentationModel()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create workflow\n        &gt;&gt;&gt; workflow = Workflow(\n        ...     feature_engineer=feature_eng,\n        ...     search_algorithm=search_algo,\n        ...     representation_model=repr_model,\n        ... )\n    \"\"\"\n    feature_engineer: FeatureEngineer\n    search_algorithm: SearchAlgorithm\n    representation_model: Optional[RepresentationModel] = None\n\n    def save(self, filepath: str | Path):\n        \"\"\"Save workflow configuration to file.\n\n        Args:\n            filepath: Path where workflow configuration will be saved.\n\n        Raises:\n            NotImplementedError: Workflow serialization is not yet implemented.\n        \"\"\"\n        raise NotImplementedError(\"Workflow serialization not yet implemented.\")\n\n    @classmethod\n    def load(cls, filepath: str | Path) -&gt; \"Workflow\":\n        \"\"\"Load workflow configuration from file.\n\n        Args:\n            filepath: Path to workflow configuration file.\n\n        Returns:\n            Workflow: Reconstructed Workflow instance.\n\n        Raises:\n            NotImplementedError: Workflow deserialization is not yet implemented.\n        \"\"\"\n        raise NotImplementedError(\"Workflow deserialization not yet implemented.\")\n</code></pre>"},{"location":"api/workflow/#energy_repset.workflow.Workflow.save","title":"save","text":"<pre><code>save(filepath: str | Path)\n</code></pre> <p>Save workflow configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path where workflow configuration will be saved.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Workflow serialization is not yet implemented.</p> Source code in <code>energy_repset/workflow.py</code> <pre><code>def save(self, filepath: str | Path):\n    \"\"\"Save workflow configuration to file.\n\n    Args:\n        filepath: Path where workflow configuration will be saved.\n\n    Raises:\n        NotImplementedError: Workflow serialization is not yet implemented.\n    \"\"\"\n    raise NotImplementedError(\"Workflow serialization not yet implemented.\")\n</code></pre>"},{"location":"api/workflow/#energy_repset.workflow.Workflow.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: str | Path) -&gt; 'Workflow'\n</code></pre> <p>Load workflow configuration from file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to workflow configuration file.</p> required <p>Returns:</p> Name Type Description <code>Workflow</code> <code>'Workflow'</code> <p>Reconstructed Workflow instance.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Workflow deserialization is not yet implemented.</p> Source code in <code>energy_repset/workflow.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str | Path) -&gt; \"Workflow\":\n    \"\"\"Load workflow configuration from file.\n\n    Args:\n        filepath: Path to workflow configuration file.\n\n    Returns:\n        Workflow: Reconstructed Workflow instance.\n\n    Raises:\n        NotImplementedError: Workflow deserialization is not yet implemented.\n    \"\"\"\n    raise NotImplementedError(\"Workflow deserialization not yet implemented.\")\n</code></pre>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment","title":"RepSetExperiment","text":"<p>Orchestrate a complete and self-contained representative subset experiment.</p> <p>This class manages the execution of a full workflow from raw data to final selection results. It handles feature engineering, search execution, and weight calculation while maintaining references to intermediate states.</p> <p>Attributes:</p> Name Type Description <code>raw_context</code> <p>Initial ProblemContext containing raw time-series data.</p> <code>workflow</code> <p>Workflow definition containing all algorithm components.</p> <code>result</code> <code>RepSetResult</code> <p>Final RepSetResult after run() completes (None before execution).</p> <p>Examples:</p> <p>Run a complete experiment:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from energy_repset.problem import RepSetExperiment\n&gt;&gt;&gt; from energy_repset.context import ProblemContext\n&gt;&gt;&gt; from energy_repset.workflow import Workflow\n&gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n&gt;&gt;&gt; # ... (imports for feature engineer, search algo, etc.)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data and context\n&gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n&gt;&gt;&gt; df = pd.DataFrame({'demand': np.random.rand(8760)}, index=dates)\n&gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n&gt;&gt;&gt; context = ProblemContext(df_raw=df, slicer=slicer)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create workflow (see Workflow docs for details)\n&gt;&gt;&gt; workflow = Workflow(\n...     feature_engineer=feature_eng,\n...     search_algorithm=search_algo,\n...     representation_model=repr_model,\n...     k=3\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run experiment\n&gt;&gt;&gt; experiment = RepSetExperiment(context, workflow)\n&gt;&gt;&gt; result = experiment.run()\n&gt;&gt;&gt; print(result.selection)  # Selected periods\n&gt;&gt;&gt; print(result.weights)    # Responsibility weights\n</code></pre> Source code in <code>energy_repset/problem.py</code> <pre><code>class RepSetExperiment:\n    \"\"\"Orchestrate a complete and self-contained representative subset experiment.\n\n    This class manages the execution of a full workflow from raw data to final\n    selection results. It handles feature engineering, search execution, and\n    weight calculation while maintaining references to intermediate states.\n\n    Attributes:\n        raw_context: Initial ProblemContext containing raw time-series data.\n        workflow: Workflow definition containing all algorithm components.\n        result: Final RepSetResult after run() completes (None before execution).\n\n    Examples:\n        Run a complete experiment:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from energy_repset.problem import RepSetExperiment\n        &gt;&gt;&gt; from energy_repset.context import ProblemContext\n        &gt;&gt;&gt; from energy_repset.workflow import Workflow\n        &gt;&gt;&gt; from energy_repset.time_slicer import TimeSlicer\n        &gt;&gt;&gt; # ... (imports for feature engineer, search algo, etc.)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data and context\n        &gt;&gt;&gt; dates = pd.date_range('2024-01-01', periods=8760, freq='h')\n        &gt;&gt;&gt; df = pd.DataFrame({'demand': np.random.rand(8760)}, index=dates)\n        &gt;&gt;&gt; slicer = TimeSlicer(unit='month')\n        &gt;&gt;&gt; context = ProblemContext(df_raw=df, slicer=slicer)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create workflow (see Workflow docs for details)\n        &gt;&gt;&gt; workflow = Workflow(\n        ...     feature_engineer=feature_eng,\n        ...     search_algorithm=search_algo,\n        ...     representation_model=repr_model,\n        ...     k=3\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run experiment\n        &gt;&gt;&gt; experiment = RepSetExperiment(context, workflow)\n        &gt;&gt;&gt; result = experiment.run()\n        &gt;&gt;&gt; print(result.selection)  # Selected periods\n        &gt;&gt;&gt; print(result.weights)    # Responsibility weights\n    \"\"\"\n\n    def __init__(self, context: ProblemContext, workflow: Workflow):\n        \"\"\"Initialize experiment with raw data context and workflow.\n\n        Args:\n            context: ProblemContext containing raw time-series data and metadata.\n            workflow: Workflow defining feature engineering, search, and representation.\n        \"\"\"\n        self.raw_context = context\n        self.workflow = workflow\n\n        # These will be populated after the run\n        self._feature_context: ProblemContext = None\n        self.result: RepSetResult = None\n\n    @property\n    def feature_context(self) -&gt; ProblemContext:\n        \"\"\"Get the context with computed features.\n\n        Returns:\n            ProblemContext with df_features populated.\n\n        Raises:\n            ValueError: If run() or run_feature_engineer() has not been called yet.\n        \"\"\"\n        if self._feature_context is None:\n            if self.raw_context._df_features is not None:\n                self._feature_context = self.raw_context.copy()\n            else:\n                raise ValueError('Please call run() or run_feature_engineer() first.')\n        return self._feature_context\n\n    def run_feature_engineer(self) -&gt; ProblemContext:\n        \"\"\"Run only the feature engineering step.\n\n        This method allows you to inspect features before running the full workflow.\n\n        Returns:\n            ProblemContext with df_features populated.\n        \"\"\"\n        self._feature_context = self.workflow.feature_engineer.run(self.raw_context)\n        return self._feature_context\n\n    def run(self) -&gt; RepSetResult:\n        \"\"\"Execute the entire workflow from feature engineering to final result.\n\n        This method orchestrates the complete selection process:\n        1. Runs the feature engineer to create a new, feature-rich context\n        2. Stores this feature_context for user inspection\n        3. Runs the search algorithm on the feature_context\n        4. Fits the representation model\n        5. Calculates the final weights\n        6. Stores and returns the final result\n\n        Returns:\n            RepSetResult: The selected periods, weights, scores, and diagnostics.\n        \"\"\"\n        if (self._feature_context is None) and (self.raw_context._df_features is None):\n            self.run_feature_engineer()\n\n        feature_context = self.feature_context\n        search_algorithm = self.workflow.search_algorithm\n        representation_model = self.workflow.representation_model\n\n        result = search_algorithm.find_selection(feature_context)\n        if result.weights is None:\n            if representation_model is None:\n                raise ValueError(\n                    \"Search algorithm returned weights=None but no \"\n                    \"RepresentationModel was provided in the Workflow.\"\n                )\n            representation_model.fit(feature_context)\n            result.weights = representation_model.weigh(result.selection)\n        elif representation_model is not None:\n            raise ValueError(\n                \"Search algorithms already set weights, but you still have a RepresentationModel defined. \\n\"\n                \"Make sure that either your SearchAlgorithm sets the weights OR you have a \"\n                \"RepresentationModel for post-hoc weighting. \\n\"\n                \"You cannot have both.\"\n            )\n\n        self.result = result\n        return self.result\n</code></pre>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.feature_context","title":"feature_context  <code>property</code>","text":"<pre><code>feature_context: ProblemContext\n</code></pre> <p>Get the context with computed features.</p> <p>Returns:</p> Type Description <code>ProblemContext</code> <p>ProblemContext with df_features populated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run() or run_feature_engineer() has not been called yet.</p>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.__init__","title":"__init__","text":"<pre><code>__init__(context: ProblemContext, workflow: Workflow)\n</code></pre> <p>Initialize experiment with raw data context and workflow.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ProblemContext</code> <p>ProblemContext containing raw time-series data and metadata.</p> required <code>workflow</code> <code>Workflow</code> <p>Workflow defining feature engineering, search, and representation.</p> required Source code in <code>energy_repset/problem.py</code> <pre><code>def __init__(self, context: ProblemContext, workflow: Workflow):\n    \"\"\"Initialize experiment with raw data context and workflow.\n\n    Args:\n        context: ProblemContext containing raw time-series data and metadata.\n        workflow: Workflow defining feature engineering, search, and representation.\n    \"\"\"\n    self.raw_context = context\n    self.workflow = workflow\n\n    # These will be populated after the run\n    self._feature_context: ProblemContext = None\n    self.result: RepSetResult = None\n</code></pre>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.run_feature_engineer","title":"run_feature_engineer","text":"<pre><code>run_feature_engineer() -&gt; ProblemContext\n</code></pre> <p>Run only the feature engineering step.</p> <p>This method allows you to inspect features before running the full workflow.</p> <p>Returns:</p> Type Description <code>ProblemContext</code> <p>ProblemContext with df_features populated.</p> Source code in <code>energy_repset/problem.py</code> <pre><code>def run_feature_engineer(self) -&gt; ProblemContext:\n    \"\"\"Run only the feature engineering step.\n\n    This method allows you to inspect features before running the full workflow.\n\n    Returns:\n        ProblemContext with df_features populated.\n    \"\"\"\n    self._feature_context = self.workflow.feature_engineer.run(self.raw_context)\n    return self._feature_context\n</code></pre>"},{"location":"api/workflow/#energy_repset.problem.RepSetExperiment.run","title":"run","text":"<pre><code>run() -&gt; RepSetResult\n</code></pre> <p>Execute the entire workflow from feature engineering to final result.</p> <p>This method orchestrates the complete selection process: 1. Runs the feature engineer to create a new, feature-rich context 2. Stores this feature_context for user inspection 3. Runs the search algorithm on the feature_context 4. Fits the representation model 5. Calculates the final weights 6. Stores and returns the final result</p> <p>Returns:</p> Name Type Description <code>RepSetResult</code> <code>RepSetResult</code> <p>The selected periods, weights, scores, and diagnostics.</p> Source code in <code>energy_repset/problem.py</code> <pre><code>def run(self) -&gt; RepSetResult:\n    \"\"\"Execute the entire workflow from feature engineering to final result.\n\n    This method orchestrates the complete selection process:\n    1. Runs the feature engineer to create a new, feature-rich context\n    2. Stores this feature_context for user inspection\n    3. Runs the search algorithm on the feature_context\n    4. Fits the representation model\n    5. Calculates the final weights\n    6. Stores and returns the final result\n\n    Returns:\n        RepSetResult: The selected periods, weights, scores, and diagnostics.\n    \"\"\"\n    if (self._feature_context is None) and (self.raw_context._df_features is None):\n        self.run_feature_engineer()\n\n    feature_context = self.feature_context\n    search_algorithm = self.workflow.search_algorithm\n    representation_model = self.workflow.representation_model\n\n    result = search_algorithm.find_selection(feature_context)\n    if result.weights is None:\n        if representation_model is None:\n            raise ValueError(\n                \"Search algorithm returned weights=None but no \"\n                \"RepresentationModel was provided in the Workflow.\"\n            )\n        representation_model.fit(feature_context)\n        result.weights = representation_model.weigh(result.selection)\n    elif representation_model is not None:\n        raise ValueError(\n            \"Search algorithms already set weights, but you still have a RepresentationModel defined. \\n\"\n            \"Make sure that either your SearchAlgorithm sets the weights OR you have a \"\n            \"RepresentationModel for post-hoc weighting. \\n\"\n            \"You cannot have both.\"\n        )\n\n    self.result = result\n    return self.result\n</code></pre>"},{"location":"api/workflow/#energy_repset.results.RepSetResult","title":"RepSetResult  <code>dataclass</code>","text":"<p>The standardized output object.</p> Source code in <code>energy_repset/results.py</code> <pre><code>@dataclass\nclass RepSetResult:\n    \"\"\"The standardized output object.\"\"\"\n    context: ProblemContext\n    selection_space: Literal['subset', 'synthetic', 'chronological']\n    selection: SliceCombination\n    scores: Dict[str, float]\n    representatives: Dict[Hashable, pd.DataFrame]  # The actual data of the representatives\n    weights: Union[Dict[Hashable, float], pd.DataFrame] = None  # Populated by RepresentationModel\n    diagnostics: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Interactive Jupyter notebooks demonstrating the <code>energy-repset</code> framework, from basic usage to advanced multi-objective and constructive algorithm workflows.</p>"},{"location":"examples/#example-1-getting-started","title":"Example 1: Getting Started","text":"<p>The simplest end-to-end workflow. Selects 4 representative months using a single objective (Wasserstein fidelity) and uniform weights.</p>"},{"location":"examples/#example-2-feature-space-exploration","title":"Example 2: Feature Space Exploration","text":"<p>Chains statistical summaries with PCA dimensionality reduction, then uses multi-objective selection (Wasserstein + correlation + centroid balance) with KMedoids cluster-size weights.</p>"},{"location":"examples/#example-3-hierarchical-seasonal-selection","title":"Example 3: Hierarchical Seasonal Selection","text":"<p>Daily-resolution features with monthly-level selection under seasonal constraints. Uses <code>GroupQuotaHierarchicalCombiGen</code> to enforce one month per season.</p>"},{"location":"examples/#example-4-comparing-representation-models","title":"Example 4: Comparing Representation Models","text":"<p>Same selection, three different representation models: Uniform, KMedoids cluster-size, and Blended (soft assignment). Compares how each distributes responsibility weights.</p>"},{"location":"examples/#example-5-multi-objective-exploration","title":"Example 5: Multi-Objective Exploration","text":"<p>Four-component objective with <code>ParetoMaxMinStrategy</code> vs <code>WeightedSumPolicy</code>. Includes Pareto front visualization and score contribution analysis.</p>"},{"location":"examples/#example-6-k-medoids-clustering","title":"Example 6: K-Medoids Clustering","text":"<p>Standalone k-medoids clustering demo. Selects representative months as cluster medoids with cluster-size-proportional weights, explores the effect of varying k.</p>"},{"location":"examples/#example-7-constructive-algorithms","title":"Example 7: Constructive Algorithms","text":"<p>Three constructive algorithms -- Hull Clustering, CTPC, and Snippet -- that build solutions using their own internal objectives, bypassing the Generate-and-Test workflow.</p>"},{"location":"examples/#running-the-notebooks","title":"Running the Notebooks","text":"<pre><code># Install the package in editable mode\npip install -e .\n\n# Launch Jupyter\njupyter notebook docs/examples/\n</code></pre>"},{"location":"examples/ex1_getting_started/","title":"Example 1: Getting Started","text":"In\u00a0[11]: Copied! <pre>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[12]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\ndf_raw\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1) df_raw Out[12]: variable load onwind offwind solar 2015-01-01 00:00:00 41.151 0.1566 0.7030 0.0 2015-01-01 01:00:00 40.135 0.1659 0.6875 0.0 2015-01-01 02:00:00 39.106 0.1746 0.6535 0.0 2015-01-01 03:00:00 38.765 0.1745 0.6803 0.0 2015-01-01 04:00:00 38.941 0.1826 0.7272 0.0 ... ... ... ... ... 2015-12-31 19:00:00 47.719 0.1388 0.4434 0.0 2015-12-31 20:00:00 45.911 0.1211 0.4023 0.0 2015-12-31 21:00:00 45.611 0.1082 0.4171 0.0 2015-12-31 22:00:00 43.762 0.1026 0.4716 0.0 2015-12-31 23:00:00 41.905 0.0975 0.5239 0.0 <p>8760 rows \u00d7 4 columns</p> In\u00a0[13]: Copied! <pre>slicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\nprint(f\"Candidate slices: {context.get_unique_slices()}\")\n</pre> slicer = rep.TimeSlicer(unit=\"month\") context = rep.ProblemContext(df_raw=df_raw, slicer=slicer) print(f\"Candidate slices: {context.get_unique_slices()}\") <pre>Candidate slices: [Period('2015-01', 'M'), Period('2015-02', 'M'), Period('2015-03', 'M'), Period('2015-04', 'M'), Period('2015-05', 'M'), Period('2015-06', 'M'), Period('2015-07', 'M'), Period('2015-08', 'M'), Period('2015-09', 'M'), Period('2015-10', 'M'), Period('2015-11', 'M'), Period('2015-12', 'M')]\n</pre> In\u00a0[14]: Copied! <pre>feature_engineer = rep.StandardStatsFeatureEngineer()\n</pre> feature_engineer = rep.StandardStatsFeatureEngineer() In\u00a0[15]: Copied! <pre>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n})\n</pre> objective_set = rep.ObjectiveSet({     'wasserstein': (1.0, rep.WassersteinFidelity()), }) In\u00a0[16]: Copied! <pre>k = 4\ncombi_gen = rep.ExhaustiveCombiGen(k=k)\npolicy = rep.WeightedSumPolicy()\nsearch_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    objective_set, policy, combi_gen\n)\n</pre> k = 4 combi_gen = rep.ExhaustiveCombiGen(k=k) policy = rep.WeightedSumPolicy() search_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(     objective_set, policy, combi_gen ) In\u00a0[17]: Copied! <pre>representation_model = rep.UniformRepresentationModel()\n</pre> representation_model = rep.UniformRepresentationModel() In\u00a0[18]: Copied! <pre>workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n</pre> workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model) experiment = rep.RepSetExperiment(context, workflow) result = experiment.run() <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 495/495 [00:02&lt;00:00, 215.34it/s]\n</pre> In\u00a0[19]: Copied! <pre>print(f\"Selected months: {result.selection}\")\nprint(f\"Weights: {result.weights}\")\nprint(f\"Wasserstein score: {result.scores['wasserstein']:.4f}\")\n</pre> print(f\"Selected months: {result.selection}\") print(f\"Weights: {result.weights}\") print(f\"Wasserstein score: {result.scores['wasserstein']:.4f}\") <pre>Selected months: (Period('2015-01', 'M'), Period('2015-02', 'M'), Period('2015-05', 'M'), Period('2015-06', 'M'))\nWeights: {Period('2015-01', 'M'): 0.25, Period('2015-02', 'M'): 0.25, Period('2015-05', 'M'): 0.25, Period('2015-06', 'M'): 0.25}\nWasserstein score: 0.0684\n</pre> In\u00a0[20]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True)\nfig.update_layout(title='Responsibility Weights (Uniform)')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True) fig.update_layout(title='Responsibility Weights (Uniform)') fig.show()"},{"location":"examples/ex1_getting_started/#example-1-getting-started","title":"Example 1: Getting Started\u00b6","text":"<p>A minimal end-to-end workflow that selects 4 representative months from a year of hourly time-series data.</p> <p>This example walks through the five pillars of the <code>energy-repset</code> framework:</p> Pillar Component Choice in this example F \u2014 Feature Space How periods are compared Statistical summaries (mean, std, min, max, quantiles, ramps) O \u2014 Objective What \"representative\" means Wasserstein distance (marginal distribution fidelity) S \u2014 Selection Space What we pick from All 4-of-12 monthly combinations (495 candidates) R \u2014 Representation How selected periods stand in for the year Uniform weights (each month = 1/4 of the year) A \u2014 Search Algorithm How we find the best selection Exhaustive generate-and-test with weighted-sum policy"},{"location":"examples/ex1_getting_started/#load-data","title":"Load data\u00b6","text":"<p>One year of hourly time series with four variables: electricity demand (<code>load</code>), onshore wind (<code>onwind</code>), offshore wind (<code>offwind</code>), and solar capacity factors (<code>solar</code>).</p>"},{"location":"examples/ex1_getting_started/#define-the-problem-context","title":"Define the problem context\u00b6","text":"<p>The <code>TimeSlicer</code> divides the year into candidate periods \u2014 here, 12 calendar months. The <code>ProblemContext</code> bundles the raw data and slicing logic into a single object that flows through the entire pipeline.</p>"},{"location":"examples/ex1_getting_started/#pillar-f-feature-engineering","title":"Pillar F: Feature engineering\u00b6","text":"<p>Before we can compare months, we need a numerical representation. <code>StandardStatsFeatureEngineer</code> computes a set of statistical summaries (mean, std, min, max, quantiles, ramp rates) per variable per month. This transforms each month into a fixed-length feature vector.</p>"},{"location":"examples/ex1_getting_started/#pillar-o-objective","title":"Pillar O: Objective\u00b6","text":"<p>We use a single score component: Wasserstein fidelity. It measures how well the marginal distribution of the selected months matches the full year. Lower distance = better match.</p> <p>With only one objective, the selection policy is straightforward \u2014 just pick the combination with the best score.</p>"},{"location":"examples/ex1_getting_started/#pillars-s-a-selection-space-and-search","title":"Pillars S + A: Selection space and search\u00b6","text":"<p><code>ExhaustiveCombiGen</code> enumerates all $\\binom{12}{4} = 495$ ways to pick 4 months from 12. Each candidate is scored by the objective, and <code>WeightedSumPolicy</code> (trivial here with one component) picks the winner.</p>"},{"location":"examples/ex1_getting_started/#pillar-r-representation-model","title":"Pillar R: Representation model\u00b6","text":"<p>With uniform weights, each selected month represents exactly 1/4 of the year. This is the simplest model \u2014 no cluster assignment, no optimization of weights. It places the full burden on the selection itself being intrinsically representative.</p>"},{"location":"examples/ex1_getting_started/#run-the-workflow","title":"Run the workflow\u00b6","text":""},{"location":"examples/ex1_getting_started/#inspect-results","title":"Inspect results\u00b6","text":""},{"location":"examples/ex1_getting_started/#diagnostic-responsibility-weights","title":"Diagnostic: responsibility weights\u00b6","text":"<p>The bar chart below shows the weight assigned to each selected month. With uniform representation, all bars are equal at 0.25. The dashed line indicates the \"ideal\" uniform reference.</p>"},{"location":"examples/ex2_feature_space/","title":"Example 2: Feature Space Exploration","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[2]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n\nslicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\nprint(f\"{len(context.get_unique_slices())} candidate monthly slices\")\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1)  slicer = rep.TimeSlicer(unit=\"month\") context = rep.ProblemContext(df_raw=df_raw, slicer=slicer) print(f\"{len(context.get_unique_slices())} candidate monthly slices\") <pre>12 candidate monthly slices\n</pre> In\u00a0[3]: Copied! <pre>stats_eng = rep.StandardStatsFeatureEngineer()\ncontext_stats = stats_eng.run(context)\nprint(f\"{context_stats.df_features.shape[1]} statistical features for {context_stats.df_features.shape[0]} slices\")\ncontext_stats.df_features.head(3)\n</pre> stats_eng = rep.StandardStatsFeatureEngineer() context_stats = stats_eng.run(context) print(f\"{context_stats.df_features.shape[1]} statistical features for {context_stats.df_features.shape[0]} slices\") context_stats.df_features.head(3) <pre>38 statistical features for 12 slices\n</pre> Out[3]: mean__load mean__onwind mean__offwind mean__solar std__load std__onwind std__offwind std__solar q10__load q10__onwind ... ramp_std__load ramp_std__onwind ramp_std__offwind ramp_std__solar corr__load__onwind corr__load__offwind corr__load__solar corr__onwind__offwind corr__onwind__solar corr__offwind__solar 2015-01 0.790427 1.623676 1.227180 -1.400283 0.552545 1.875624 1.933594 -1.648768 0.623443 0.404962 ... 0.484976 -0.042419 0.637891 -1.853069 -0.161411 -1.234681 -0.919195 0.790412 0.522001 0.458110 2015-02 1.454759 -0.178153 0.032890 -0.779126 -1.266598 -0.386469 0.387692 -0.710218 1.967376 0.269301 ... -0.110690 -0.511887 0.024703 -0.629509 -0.948116 -1.024592 -0.810737 -1.246019 0.262220 1.687519 2015-03 0.845602 0.381202 0.172626 -0.018479 -1.705158 0.914114 0.375525 0.222239 1.098170 -0.158519 ... -0.465317 0.921492 0.768165 0.394653 0.160418 -0.492397 0.132615 0.368639 -0.497747 -1.376189 <p>3 rows \u00d7 38 columns</p> In\u00a0[4]: Copied! <pre>mean_cols = [c for c in context_stats.df_features.columns if c.startswith('mean__')]\nfig = diag.FeatureSpaceScatterMatrix().plot(context_stats.df_features, dimensions=mean_cols)\nfig.update_layout(title='Scatter Matrix: Monthly Means')\nfig.show()\n</pre> mean_cols = [c for c in context_stats.df_features.columns if c.startswith('mean__')] fig = diag.FeatureSpaceScatterMatrix().plot(context_stats.df_features, dimensions=mean_cols) fig.update_layout(title='Scatter Matrix: Monthly Means') fig.show() In\u00a0[5]: Copied! <pre>fig = diag.FeatureCorrelationHeatmap().plot(context_stats.df_features, method='pearson')\nfig.update_layout(title='Feature Correlation Matrix (Statistical Features)')\nfig.show()\n</pre> fig = diag.FeatureCorrelationHeatmap().plot(context_stats.df_features, method='pearson') fig.update_layout(title='Feature Correlation Matrix (Statistical Features)') fig.show() In\u00a0[6]: Copied! <pre>pca_full = rep.PCAFeatureEngineer()\ncontext_full_pca = pca_full.run(context_stats)\n\nfig = diag.PCAVarianceExplained(pca_full).plot(show_cumulative=True)\nfig.update_layout(title='PCA Variance Explained')\nfig.show()\n</pre> pca_full = rep.PCAFeatureEngineer() context_full_pca = pca_full.run(context_stats)  fig = diag.PCAVarianceExplained(pca_full).plot(show_cumulative=True) fig.update_layout(title='PCA Variance Explained') fig.show() <p>The cumulative curve shows a clear bend around 4 components \u2014 beyond that, each additional PC adds very little. Four PCs capture the essential structure while keeping the feature space compact.</p> In\u00a0[7]: Copied! <pre>fig = diag.FeatureSpaceScatterMatrix().plot(\n    context_full_pca.df_features, dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3']\n)\nfig.update_layout(title='Scatter Matrix: First 4 Principal Components')\nfig.show()\n</pre> fig = diag.FeatureSpaceScatterMatrix().plot(     context_full_pca.df_features, dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3'] ) fig.update_layout(title='Scatter Matrix: First 4 Principal Components') fig.show() In\u00a0[8]: Copied! <pre>pca_4 = rep.PCAFeatureEngineer(n_components=4)\ncontext_4pc = pca_4.run(context_stats)\ncontext_4pc.df_features\n</pre> pca_4 = rep.PCAFeatureEngineer(n_components=4) context_4pc = pca_4.run(context_stats) context_4pc.df_features Out[8]: pc_0 pc_1 pc_2 pc_3 2015-01 6.184021 0.964440 -0.653615 0.223279 2015-02 1.934236 -3.160402 -1.649667 2.709401 2015-03 1.081686 0.569398 -3.265472 0.251567 2015-04 -4.156230 1.690578 -0.116559 -0.684499 2015-05 -3.367315 3.007706 1.534711 0.538564 2015-06 -4.609243 0.098181 0.590968 -0.152467 2015-07 -2.183370 2.267731 -1.127886 0.872675 2015-08 -5.217145 -1.117851 -0.008774 -0.607658 2015-09 -0.651842 -0.811346 0.338338 -0.044285 2015-10 -1.720184 -4.710915 1.411115 -1.246839 2015-11 6.424006 0.514382 -1.390549 -2.695662 2015-12 6.281379 0.688098 4.337392 0.835923 In\u00a0[9]: Copied! <pre>obj_balanced = rep.ObjectiveSet({\n    'wasserstein': (0.5, rep.WassersteinFidelity()),\n    'correlation': (0.5, rep.CorrelationFidelity()),\n    'centroid_balance': (0.5, rep.CentroidBalance()),\n})\n\nk = 3\nsearch_a = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    obj_balanced,\n    rep.ParetoMaxMinStrategy(),\n    rep.ExhaustiveCombiGen(k=k),\n)\nrepr_model = rep.KMedoidsClustersizeRepresentation()\n\nworkflow_a = rep.Workflow(pca_4, search_a, repr_model)\nexp_a = rep.RepSetExperiment(context_4pc, workflow_a)\nresult_a = exp_a.run()\n\nprint(f\"Selected months (A): {result_a.selection}\")\nprint(f\"Weights (A):         {result_a.weights}\")\nprint(f\"Scores (A):          {result_a.scores}\")\n</pre> obj_balanced = rep.ObjectiveSet({     'wasserstein': (0.5, rep.WassersteinFidelity()),     'correlation': (0.5, rep.CorrelationFidelity()),     'centroid_balance': (0.5, rep.CentroidBalance()), })  k = 3 search_a = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(     obj_balanced,     rep.ParetoMaxMinStrategy(),     rep.ExhaustiveCombiGen(k=k), ) repr_model = rep.KMedoidsClustersizeRepresentation()  workflow_a = rep.Workflow(pca_4, search_a, repr_model) exp_a = rep.RepSetExperiment(context_4pc, workflow_a) result_a = exp_a.run()  print(f\"Selected months (A): {result_a.selection}\") print(f\"Weights (A):         {result_a.weights}\") print(f\"Scores (A):          {result_a.scores}\") <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:01&lt;00:00, 201.66it/s]</pre> <pre>Selected months (A): (Period('2015-01', 'M'), Period('2015-04', 'M'), Period('2015-09', 'M'))\nWeights (A):         {Period('2015-01', 'M'): 0.25, Period('2015-04', 'M'): 0.4166666666666667, Period('2015-09', 'M'): 0.3333333333333333}\nScores (A):          {'wasserstein': 0.20025433806827211, 'correlation': 0.0231217738568774, 'centroid_balance': 0.7982187924943351}\n</pre> <pre>\n</pre> In\u00a0[10]: Copied! <pre>obj_diverse = rep.ObjectiveSet({\n    'wasserstein': (0.5, rep.WassersteinFidelity()),\n    'correlation': (0.5, rep.CorrelationFidelity()),\n    'diversity': (0.5, rep.DiversityReward()),\n})\n\nsearch_b = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    obj_diverse,\n    rep.ParetoMaxMinStrategy(),\n    rep.ExhaustiveCombiGen(k=k),\n)\n\nworkflow_b = rep.Workflow(pca_4, search_b, rep.KMedoidsClustersizeRepresentation())\nexp_b = rep.RepSetExperiment(context_4pc, workflow_b)\nresult_b = exp_b.run()\n\nprint(f\"Selected months (B): {result_b.selection}\")\nprint(f\"Weights (B):         {result_b.weights}\")\nprint(f\"Scores (B):          {result_b.scores}\")\n</pre> obj_diverse = rep.ObjectiveSet({     'wasserstein': (0.5, rep.WassersteinFidelity()),     'correlation': (0.5, rep.CorrelationFidelity()),     'diversity': (0.5, rep.DiversityReward()), })  search_b = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(     obj_diverse,     rep.ParetoMaxMinStrategy(),     rep.ExhaustiveCombiGen(k=k), )  workflow_b = rep.Workflow(pca_4, search_b, rep.KMedoidsClustersizeRepresentation()) exp_b = rep.RepSetExperiment(context_4pc, workflow_b) result_b = exp_b.run()  print(f\"Selected months (B): {result_b.selection}\") print(f\"Weights (B):         {result_b.weights}\") print(f\"Scores (B):          {result_b.scores}\") <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:01&lt;00:00, 216.62it/s]</pre> <pre>Selected months (B): (Period('2015-07', 'M'), Period('2015-10', 'M'), Period('2015-11', 'M'))\nWeights (B):         {Period('2015-07', 'M'): 0.5833333333333334, Period('2015-10', 'M'): 0.16666666666666666, Period('2015-11', 'M'): 0.25}\nScores (B):          {'wasserstein': 0.17951351867468845, 'correlation': 0.04652147452533339, 'diversity': 9.132970566687343}\n</pre> <pre>\n</pre> In\u00a0[11]: Copied! <pre>set_a = set(result_a.selection)\nset_b = set(result_b.selection)\n\nprint(f\"Experiment A (Balanced):  {result_a.selection}\")\nprint(f\"Experiment B (Diverse):   {result_b.selection}\")\nprint(f\"Overlap:                  {set_a &amp; set_b or 'none'}\")\nprint(f\"Only in A:                {set_a - set_b or 'none'}\")\nprint(f\"Only in B:                {set_b - set_a or 'none'}\")\n</pre> set_a = set(result_a.selection) set_b = set(result_b.selection)  print(f\"Experiment A (Balanced):  {result_a.selection}\") print(f\"Experiment B (Diverse):   {result_b.selection}\") print(f\"Overlap:                  {set_a &amp; set_b or 'none'}\") print(f\"Only in A:                {set_a - set_b or 'none'}\") print(f\"Only in B:                {set_b - set_a or 'none'}\") <pre>Experiment A (Balanced):  (Period('2015-01', 'M'), Period('2015-04', 'M'), Period('2015-09', 'M'))\nExperiment B (Diverse):   (Period('2015-07', 'M'), Period('2015-10', 'M'), Period('2015-11', 'M'))\nOverlap:                  none\nOnly in A:                {Period('2015-09', 'M'), Period('2015-01', 'M'), Period('2015-04', 'M')}\nOnly in B:                {Period('2015-11', 'M'), Period('2015-07', 'M'), Period('2015-10', 'M')}\n</pre> In\u00a0[12]: Copied! <pre>fig = diag.SelectionComparisonScatterMatrix().plot(\n    context_4pc.df_features,\n    selections={\n        'A: Balanced': result_a.selection,\n        'B: Diverse': result_b.selection,\n    },\n    dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3'],\n)\nfig.update_layout(title='Selection Comparison in PCA Feature Space')\nfig.show()\n</pre> fig = diag.SelectionComparisonScatterMatrix().plot(     context_4pc.df_features,     selections={         'A: Balanced': result_a.selection,         'B: Diverse': result_b.selection,     },     dimensions=['pc_0', 'pc_1', 'pc_2', 'pc_3'], ) fig.update_layout(title='Selection Comparison in PCA Feature Space') fig.show() In\u00a0[13]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result_a.weights, show_uniform_reference=True)\nfig.update_layout(title='Responsibility Weights \u2014 Experiment A (Balanced)')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result_a.weights, show_uniform_reference=True) fig.update_layout(title='Responsibility Weights \u2014 Experiment A (Balanced)') fig.show() In\u00a0[14]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result_b.weights, show_uniform_reference=True)\nfig.update_layout(title='Responsibility Weights \u2014 Experiment B (Diverse)')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result_b.weights, show_uniform_reference=True) fig.update_layout(title='Responsibility Weights \u2014 Experiment B (Diverse)') fig.show() In\u00a0[15]: Copied! <pre>selected_idx_a = context.slicer.get_indices_for_slice_combi(context.df_raw.index, result_a.selection)\ndf_sel_a = context.df_raw.loc[selected_idx_a]\n\nfig = diag.DistributionOverlayECDFGrid().plot(context.df_raw, df_sel_a)\nfig.update_layout(title='Distribution Fidelity \u2014 Experiment A (Balanced)')\nfig.update_xaxes(matches=None)\nfig.update_yaxes(matches=None)\nfig.show()\n</pre> selected_idx_a = context.slicer.get_indices_for_slice_combi(context.df_raw.index, result_a.selection) df_sel_a = context.df_raw.loc[selected_idx_a]  fig = diag.DistributionOverlayECDFGrid().plot(context.df_raw, df_sel_a) fig.update_layout(title='Distribution Fidelity \u2014 Experiment A (Balanced)') fig.update_xaxes(matches=None) fig.update_yaxes(matches=None) fig.show() In\u00a0[16]: Copied! <pre>fig = diag.CorrelationDifferenceHeatmap().plot(\n    context.df_raw, df_sel_a, method='pearson', show_lower_only=True\n)\nfig.update_layout(title='Correlation Difference \u2014 Experiment A (Balanced)')\nfig.show()\n</pre> fig = diag.CorrelationDifferenceHeatmap().plot(     context.df_raw, df_sel_a, method='pearson', show_lower_only=True ) fig.update_layout(title='Correlation Difference \u2014 Experiment A (Balanced)') fig.show() In\u00a0[17]: Copied! <pre>fig = diag.DiurnalProfileOverlay().plot(\n    context.df_raw, df_sel_a, variables=['load', 'onwind', 'offwind', 'solar']\n)\nfig.update_layout(title='Diurnal Profiles \u2014 Experiment A (Balanced)')\nfig.show()\n</pre> fig = diag.DiurnalProfileOverlay().plot(     context.df_raw, df_sel_a, variables=['load', 'onwind', 'offwind', 'solar'] ) fig.update_layout(title='Diurnal Profiles \u2014 Experiment A (Balanced)') fig.show()"},{"location":"examples/ex2_feature_space/#example-2-feature-space-exploration","title":"Example 2: Feature Space Exploration\u00b6","text":"<p>This notebook explores how feature engineering and objective choice shape the selection process. We:</p> <ol> <li>Start with statistical features and examine why dimensionality reduction is needed</li> <li>Use PCA to compress the feature space, guided by variance analysis</li> <li>Run two experiments with different objectives to see how they influence the result</li> <li>Compare the two selections side-by-side in feature space</li> </ol> <p>Key concepts introduced:</p> <ul> <li>Manual feature engineering chain (stats then PCA)</li> <li>PCA variance analysis to choose the number of components</li> <li>Multi-objective selection with <code>CentroidBalance</code> vs <code>DiversityReward</code></li> <li><code>SelectionComparisonScatterMatrix</code> for comparing multiple selections</li> </ul>"},{"location":"examples/ex2_feature_space/#statistical-features-a-first-look","title":"Statistical features: a first look\u00b6","text":"<p>We begin with <code>StandardStatsFeatureEngineer</code>, which computes summary statistics (mean, std, quantiles, ramp rates, etc.) for each variable and slice. This gives us a multi-dimensional profile for every candidate month.</p>"},{"location":"examples/ex2_feature_space/#how-do-the-months-compare-on-average-values","title":"How do the months compare on average values?\u00b6","text":"<p>A scatter matrix of just the mean features shows how months relate in terms of their average load, onshore wind, offshore wind, and solar generation. This is a useful starting point, but we want fidelity across all statistical dimensions \u2014 not just means.</p>"},{"location":"examples/ex2_feature_space/#the-curse-of-dimensionality","title":"The curse of dimensionality\u00b6","text":"<p>With 12 data points and dozens of features, many of which are highly correlated, distance-based comparisons become unreliable. The feature correlation heatmap reveals substantial redundancy:</p>"},{"location":"examples/ex2_feature_space/#dimensionality-reduction-with-pca","title":"Dimensionality reduction with PCA\u00b6","text":"<p>PCA projects the correlated features onto orthogonal axes ordered by variance explained. This addresses the curse of dimensionality while retaining the essential structure.</p>"},{"location":"examples/ex2_feature_space/#narrowing-the-feature-space","title":"Narrowing the feature space\u00b6","text":"<p>Based on the variance analysis, 4 PCs capture the essential structure. We create a dedicated 4-PC feature context for both experiments below.</p>"},{"location":"examples/ex2_feature_space/#experiment-a-balanced-selection","title":"Experiment A: Balanced selection\u00b6","text":"<p>Our first objective set combines:</p> <ul> <li>Wasserstein fidelity: marginal distribution similarity</li> <li>Correlation fidelity: preservation of cross-variable dependencies</li> <li>Centroid balance: penalises selections whose feature centroid deviates from the data centroid</li> </ul> <p>The <code>ParetoMaxMinStrategy</code> picks the Pareto-optimal combination that maximises the worst objective \u2014 a balanced, conservative choice.</p>"},{"location":"examples/ex2_feature_space/#experiment-b-diversity-focused-selection","title":"Experiment B: Diversity-focused selection\u00b6","text":"<p>We replace <code>CentroidBalance</code> with <code>DiversityReward</code>, which favours selections that are maximally spread out in feature space (large pairwise distances). This can pull the selection towards extreme months rather than central ones.</p>"},{"location":"examples/ex2_feature_space/#comparing-the-two-selections","title":"Comparing the two selections\u00b6","text":"<p>How does the objective shape the result? Let's compare which months each experiment chose.</p>"},{"location":"examples/ex2_feature_space/#side-by-side-in-feature-space","title":"Side-by-side in feature space\u00b6","text":"<p>The <code>SelectionComparisonScatterMatrix</code> plots both selections on top of the full feature space. Distinct markers and colours make it easy to spot where the objectives push the selection.</p>"},{"location":"examples/ex2_feature_space/#per-experiment-diagnostics","title":"Per-experiment diagnostics\u00b6","text":""},{"location":"examples/ex2_feature_space/#responsibility-weights","title":"Responsibility weights\u00b6","text":"<p>The KMedoids representation assigns weights proportional to how many months each representative \"covers\". The dashed line shows uniform (1/k) for reference.</p>"},{"location":"examples/ex2_feature_space/#distribution-fidelity-ecdf-grid","title":"Distribution fidelity (ECDF grid)\u00b6","text":"<p>The ECDF grid shows \u2014 for every variable at once \u2014 how well the selection's marginal distribution tracks the full year. Gaps indicate value ranges that the selection under- or over-represents.</p>"},{"location":"examples/ex2_feature_space/#correlation-preservation","title":"Correlation preservation\u00b6","text":"<p>The heatmap shows the difference between the correlation matrix of the selection and the full year. Values near zero (light) mean the selection preserves that cross-variable relationship well.</p>"},{"location":"examples/ex2_feature_space/#diurnal-profiles","title":"Diurnal profiles\u00b6","text":"<p>Average hourly shape (hour 0-23) of each variable, comparing the full year with the selection. Good matches indicate the selection captures typical within-day patterns.</p>"},{"location":"examples/ex3_hierarchical_selection/","title":"Example 3: Hierarchical Seasonal Selection","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[2]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1) In\u00a0[3]: Copied! <pre>child_slicer = rep.TimeSlicer(unit=\"day\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=child_slicer)\nprint(f\"{len(context.get_unique_slices())} daily slices\")\n</pre> child_slicer = rep.TimeSlicer(unit=\"day\") context = rep.ProblemContext(df_raw=df_raw, slicer=child_slicer) print(f\"{len(context.get_unique_slices())} daily slices\") <pre>365 daily slices\n</pre> In\u00a0[4]: Copied! <pre>feature_engineer = rep.StandardStatsFeatureEngineer()\ncontext = feature_engineer.run(context)\nprint(f\"Features computed for {len(context.df_features)} daily periods\")\n</pre> feature_engineer = rep.StandardStatsFeatureEngineer() context = feature_engineer.run(context) print(f\"Features computed for {len(context.df_features)} daily periods\") <pre>Features computed for 365 daily periods\n</pre> In\u00a0[5]: Copied! <pre>objective_set = rep.ObjectiveSet({\n    'wasserstein': (0.5, rep.WassersteinFidelity()),\n    'correlation': (0.5, rep.CorrelationFidelity()),\n})\npolicy = rep.ParetoMaxMinStrategy()\n</pre> objective_set = rep.ObjectiveSet({     'wasserstein': (0.5, rep.WassersteinFidelity()),     'correlation': (0.5, rep.CorrelationFidelity()), }) policy = rep.ParetoMaxMinStrategy() In\u00a0[6]: Copied! <pre>combi_gen = rep.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(\n    parent_k=4,\n    dt_index=df_raw.index,\n    child_slicer=child_slicer,\n    group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1}\n)\n\ndays = context.get_unique_slices()\nprint(f\"{combi_gen.count(days)} candidate combinations\")\nprint(\"Each = 4 months (1 per season), evaluated on ~120 days total\")\n</pre> combi_gen = rep.GroupQuotaHierarchicalCombiGen.from_slicers_with_seasons(     parent_k=4,     dt_index=df_raw.index,     child_slicer=child_slicer,     group_quota={'winter': 1, 'spring': 1, 'summer': 1, 'fall': 1} )  days = context.get_unique_slices() print(f\"{combi_gen.count(days)} candidate combinations\") print(\"Each = 4 months (1 per season), evaluated on ~120 days total\") <pre>81 candidate combinations\nEach = 4 months (1 per season), evaluated on ~120 days total\n</pre> In\u00a0[7]: Copied! <pre>search_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen)\nrepresentation_model = rep.KMedoidsClustersizeRepresentation()\n\nworkflow = rep.Workflow(feature_engineer, search_algorithm, representation_model)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n</pre> search_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(objective_set, policy, combi_gen) representation_model = rep.KMedoidsClustersizeRepresentation()  workflow = rep.Workflow(feature_engineer, search_algorithm, representation_model) experiment = rep.RepSetExperiment(context, workflow) result = experiment.run() <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81/81 [00:00&lt;00:00, 204.13it/s]\n</pre> In\u00a0[8]: Copied! <pre># Identify which months were selected\nselected_months = sorted({day.asfreq('M') for day in result.selection})\nprint(f\"Selected months: {selected_months}\")\nprint(f\"Total days in selection: {len(result.selection)}\")\nprint(f\"Scores: {result.scores}\")\n</pre> # Identify which months were selected selected_months = sorted({day.asfreq('M') for day in result.selection}) print(f\"Selected months: {selected_months}\") print(f\"Total days in selection: {len(result.selection)}\") print(f\"Scores: {result.scores}\") <pre>Selected months: [Period('2015-01', 'M'), Period('2015-03', 'M'), Period('2015-08', 'M'), Period('2015-09', 'M')]\nTotal days in selection: 123\nScores: {'wasserstein': 0.14336076099229295, 'correlation': 0.04274265308227279}\n</pre> In\u00a0[9]: Copied! <pre>fig = diag.ParetoScatter2D(\n    objective_x='wasserstein', objective_y='correlation'\n).plot(search_algorithm=search_algorithm, selected_combination=result.selection)\nfig.update_layout(title='Pareto Front: Wasserstein vs Correlation')\nfig.show()\n</pre> fig = diag.ParetoScatter2D(     objective_x='wasserstein', objective_y='correlation' ).plot(search_algorithm=search_algorithm, selected_combination=result.selection) fig.update_layout(title='Pareto Front: Wasserstein vs Correlation') fig.show() In\u00a0[10]: Copied! <pre>fig = diag.ParetoParallelCoordinates().plot(search_algorithm=search_algorithm)\nfig.update_layout(title='Pareto Front: Parallel Coordinates')\nfig.show()\n</pre> fig = diag.ParetoParallelCoordinates().plot(search_algorithm=search_algorithm) fig.update_layout(title='Pareto Front: Parallel Coordinates') fig.show() In\u00a0[11]: Copied! <pre>fig = diag.ScoreContributionBars().plot(result.scores, normalize=True)\nfig.update_layout(title='Score Component Contributions (Normalized)')\nfig.show()\n</pre> fig = diag.ScoreContributionBars().plot(result.scores, normalize=True) fig.update_layout(title='Score Component Contributions (Normalized)') fig.show() In\u00a0[12]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True)\nfig.update_layout(title='Responsibility Weights')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True) fig.update_layout(title='Responsibility Weights') fig.show() In\u00a0[13]: Copied! <pre>selected_indices = child_slicer.get_indices_for_slice_combi(df_raw.index, result.selection)\ndf_selection = df_raw.loc[selected_indices]\n\nfor var in df_raw.columns:\n    fig = diag.DistributionOverlayECDF().plot(df_raw[var], df_selection[var])\n    fig.update_layout(title=f'ECDF Overlay: {var}')\n    fig.show()\n</pre> selected_indices = child_slicer.get_indices_for_slice_combi(df_raw.index, result.selection) df_selection = df_raw.loc[selected_indices]  for var in df_raw.columns:     fig = diag.DistributionOverlayECDF().plot(df_raw[var], df_selection[var])     fig.update_layout(title=f'ECDF Overlay: {var}')     fig.show() In\u00a0[14]: Copied! <pre>cols = list(context.df_features.columns[:2])\nfig = diag.FeatureSpaceScatter2D().plot(\n    context.df_features, x=cols[0], y=cols[1], selection=result.selection\n)\nfig.update_layout(title='Feature Space with Selection')\nfig.show()\n</pre> cols = list(context.df_features.columns[:2]) fig = diag.FeatureSpaceScatter2D().plot(     context.df_features, x=cols[0], y=cols[1], selection=result.selection ) fig.update_layout(title='Feature Space with Selection') fig.show()"},{"location":"examples/ex3_hierarchical_selection/#example-3-hierarchical-seasonal-selection","title":"Example 3: Hierarchical Seasonal Selection\u00b6","text":"<p>This notebook demonstrates hierarchical candidate generation \u2014 features are computed at daily resolution, but the selection operates at the monthly level with seasonal constraints.</p> <p>Why hierarchical? Evaluating months by their daily composition gives a finer-grained quality signal than month-level statistics alone. And enforcing \"one month per season\" guarantees seasonal coverage, which pure optimization might sacrifice for aggregate fidelity.</p> <p>Key concepts:</p> <ul> <li><code>GroupQuotaHierarchicalCombiGen</code>: constrained candidate generation with seasonal quotas</li> <li>Pareto front visualization: understanding trade-offs between objectives</li> </ul>"},{"location":"examples/ex3_hierarchical_selection/#problem-context-with-daily-slicing","title":"Problem context with daily slicing\u00b6","text":"<p>We slice at the day level (365 candidate periods). Features are computed per day, which gives the objective functions much more granular data to work with compared to month-level features.</p>"},{"location":"examples/ex3_hierarchical_selection/#objectives-wasserstein-correlation","title":"Objectives: Wasserstein + Correlation\u00b6","text":"<p>Two complementary fidelity metrics:</p> <ul> <li>Wasserstein: are the value distributions of each variable preserved?</li> <li>Correlation: are the dependencies between variables preserved?</li> </ul> <p>The <code>ParetoMaxMinStrategy</code> picks the combination that is Pareto-optimal and maximizes the worst-performing objective \u2014 a robust, balanced choice.</p>"},{"location":"examples/ex3_hierarchical_selection/#hierarchical-combination-generator","title":"Hierarchical combination generator\u00b6","text":"<p>This is where the magic happens. <code>GroupQuotaHierarchicalCombiGen</code> does two things:</p> <ol> <li>Seasonal quotas: enforces exactly 1 month per season (winter, spring, summer, fall) \u2014 so the 4 selected months are structurally diverse</li> <li>Hierarchical evaluation: each candidate \"month\" is expanded to its constituent days for scoring</li> </ol> <p>With 3 months per season and 1 pick each, we get $3^4 = 81$ candidate combinations \u2014 far fewer than the unconstrained $\\binom{12}{4} = 495$.</p>"},{"location":"examples/ex3_hierarchical_selection/#run-the-workflow","title":"Run the workflow\u00b6","text":""},{"location":"examples/ex3_hierarchical_selection/#pareto-front-analysis","title":"Pareto front analysis\u00b6","text":"<p>The scatter plot shows all 81 evaluated combinations in objective space. The Pareto front (highlighted) contains the non-dominated solutions \u2014 no other combination is better on both objectives simultaneously. The selected combination is marked.</p>"},{"location":"examples/ex3_hierarchical_selection/#score-contributions-and-weights","title":"Score contributions and weights\u00b6","text":""},{"location":"examples/ex3_hierarchical_selection/#distribution-fidelity-per-variable","title":"Distribution fidelity per variable\u00b6","text":"<p>ECDF overlays for each variable show how well the selection reproduces the full-year distributions.</p>"},{"location":"examples/ex3_hierarchical_selection/#feature-space-with-selection","title":"Feature space with selection\u00b6","text":""},{"location":"examples/ex4_representation_models/","title":"Example 4: Comparing Representation Models","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport plotly.express as px\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import plotly.express as px import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[2]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n\nslicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1)  slicer = rep.TimeSlicer(unit=\"month\") context = rep.ProblemContext(df_raw=df_raw, slicer=slicer) In\u00a0[3]: Copied! <pre>feature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n\nk = 3\nobjective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n})\npolicy = rep.WeightedSumPolicy(normalization='robust_minmax')\nsearch_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    objective_set, policy, rep.ExhaustiveCombiGen(k=k)\n)\n\n# Run with uniform weights to get the selection\nworkflow = rep.Workflow(feature_pipeline, search_algorithm, rep.UniformRepresentationModel())\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n\nselection = result.selection\nprint(f\"Selected months: {selection}\")\nprint(f\"Scores: {result.scores}\")\n</pre> feature_pipeline = rep.FeaturePipeline(engineers={     'stats': rep.StandardStatsFeatureEngineer(),     'pca': rep.PCAFeatureEngineer(), })  k = 3 objective_set = rep.ObjectiveSet({     'wasserstein': (1.0, rep.WassersteinFidelity()),     'correlation': (1.0, rep.CorrelationFidelity()), }) policy = rep.WeightedSumPolicy(normalization='robust_minmax') search_algorithm = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(     objective_set, policy, rep.ExhaustiveCombiGen(k=k) )  # Run with uniform weights to get the selection workflow = rep.Workflow(feature_pipeline, search_algorithm, rep.UniformRepresentationModel()) experiment = rep.RepSetExperiment(context, workflow) result = experiment.run()  selection = result.selection print(f\"Selected months: {selection}\") print(f\"Scores: {result.scores}\") <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:00&lt;00:00, 227.39it/s]</pre> <pre>Selected months: (Period('2015-01', 'M'), Period('2015-04', 'M'), Period('2015-09', 'M'))\nScores: {'wasserstein': 0.20025433806827211, 'correlation': 0.0231217738568774}\n</pre> <pre>\n</pre> In\u00a0[4]: Copied! <pre>feature_context = experiment.feature_context\n\n# Model A: Uniform \u2014 1/k each\nuniform_model = rep.UniformRepresentationModel()\nuniform_model.fit(feature_context)\nweights_uniform = uniform_model.weigh(selection)\n\n# Model B: KMedoids cluster-size \u2014 proportional to cluster membership\nkmedoids_model = rep.KMedoidsClustersizeRepresentation()\nkmedoids_model.fit(feature_context)\nweights_kmedoids = kmedoids_model.weigh(selection)\n\n# Model C: Blended (soft assignment) \u2014 weight matrix\nblended_model = rep.BlendedRepresentationModel(blend_type='convex')\nblended_model.fit(feature_context)\nweights_blended_df = blended_model.weigh(selection)\n</pre> feature_context = experiment.feature_context  # Model A: Uniform \u2014 1/k each uniform_model = rep.UniformRepresentationModel() uniform_model.fit(feature_context) weights_uniform = uniform_model.weigh(selection)  # Model B: KMedoids cluster-size \u2014 proportional to cluster membership kmedoids_model = rep.KMedoidsClustersizeRepresentation() kmedoids_model.fit(feature_context) weights_kmedoids = kmedoids_model.weigh(selection)  # Model C: Blended (soft assignment) \u2014 weight matrix blended_model = rep.BlendedRepresentationModel(blend_type='convex') blended_model.fit(feature_context) weights_blended_df = blended_model.weigh(selection) In\u00a0[5]: Copied! <pre>print(f\"{'Month':&lt;12} {'Uniform':&gt;10} {'KMedoids':&gt;10}\")\nprint(\"-\" * 34)\nfor s in selection:\n    print(f\"{str(s):&lt;12} {weights_uniform[s]:&gt;10.3f} {weights_kmedoids[s]:&gt;10.3f}\")\n\n# Aggregate blended weights to one value per representative\nblended_col_sums = weights_blended_df.sum(axis=0)\nweights_blended_agg = (blended_col_sums / blended_col_sums.sum()).to_dict()\nprint(f\"\\nBlended (aggregated): {weights_blended_agg}\")\n</pre> print(f\"{'Month':&lt;12} {'Uniform':&gt;10} {'KMedoids':&gt;10}\") print(\"-\" * 34) for s in selection:     print(f\"{str(s):&lt;12} {weights_uniform[s]:&gt;10.3f} {weights_kmedoids[s]:&gt;10.3f}\")  # Aggregate blended weights to one value per representative blended_col_sums = weights_blended_df.sum(axis=0) weights_blended_agg = (blended_col_sums / blended_col_sums.sum()).to_dict() print(f\"\\nBlended (aggregated): {weights_blended_agg}\") <pre>Month           Uniform   KMedoids\n----------------------------------\n2015-01           0.333      0.250\n2015-04           0.333      0.333\n2015-09           0.333      0.417\n\nBlended (aggregated): {Period('2015-01', 'M'): 0.28509640040626255, Period('2015-04', 'M'): 0.3254398531847014, Period('2015-09', 'M'): 0.389463746409036}\n</pre> In\u00a0[6]: Copied! <pre>models = {\n    'Uniform': weights_uniform,\n    'KMedoids': weights_kmedoids,\n    'Blended (aggregated)': weights_blended_agg,\n}\n\nfor label, weights in models.items():\n    fig = diag.ResponsibilityBars().plot(weights, show_uniform_reference=True)\n    fig.update_layout(title=f'Responsibility Weights: {label}')\n    fig.show()\n</pre> models = {     'Uniform': weights_uniform,     'KMedoids': weights_kmedoids,     'Blended (aggregated)': weights_blended_agg, }  for label, weights in models.items():     fig = diag.ResponsibilityBars().plot(weights, show_uniform_reference=True)     fig.update_layout(title=f'Responsibility Weights: {label}')     fig.show() In\u00a0[7]: Copied! <pre>heatmap_df = weights_blended_df.copy()\nheatmap_df.index = heatmap_df.index.astype(str)\nheatmap_df.columns = heatmap_df.columns.astype(str)\n\nfig = px.imshow(\n    heatmap_df.T,\n    labels=dict(x='Original Month', y='Representative', color='Weight'),\n    color_continuous_scale='Blues',\n    aspect='auto',\n    title='Blended Weight Matrix',\n)\nfig.show()\n</pre> heatmap_df = weights_blended_df.copy() heatmap_df.index = heatmap_df.index.astype(str) heatmap_df.columns = heatmap_df.columns.astype(str)  fig = px.imshow(     heatmap_df.T,     labels=dict(x='Original Month', y='Representative', color='Weight'),     color_continuous_scale='Blues',     aspect='auto',     title='Blended Weight Matrix', ) fig.show() In\u00a0[8]: Copied! <pre>fig = diag.FeatureSpaceScatter2D().plot(\n    feature_context.df_features, x='pc_0', y='pc_1', selection=selection\n)\nfig.update_layout(title='Feature Space with Selection')\nfig.show()\n</pre> fig = diag.FeatureSpaceScatter2D().plot(     feature_context.df_features, x='pc_0', y='pc_1', selection=selection ) fig.update_layout(title='Feature Space with Selection') fig.show() In\u00a0[9]: Copied! <pre>selected_indices = slicer.get_indices_for_slice_combi(df_raw.index, selection)\ndf_selection = df_raw.loc[selected_indices]\n\nfor var in df_raw.columns:\n    fig = diag.DistributionOverlayECDF().plot(df_raw[var], df_selection[var])\n    fig.update_layout(title=f'ECDF Overlay: {var}')\n    fig.show()\n</pre> selected_indices = slicer.get_indices_for_slice_combi(df_raw.index, selection) df_selection = df_raw.loc[selected_indices]  for var in df_raw.columns:     fig = diag.DistributionOverlayECDF().plot(df_raw[var], df_selection[var])     fig.update_layout(title=f'ECDF Overlay: {var}')     fig.show()"},{"location":"examples/ex4_representation_models/#example-4-comparing-representation-models","title":"Example 4: Comparing Representation Models\u00b6","text":"<p>The Representation Model (pillar R) determines how selected periods stand in for the full year. This notebook runs a single search to find the best 3-month selection, then applies three different representation models to the same selection:</p> Model How it works Weight distribution Uniform Each period = 1/k Equal bars KMedoids cluster-size Weight = fraction of months closest to this representative Unequal \u2014 popular representatives get higher weight Blended (soft assignment) Each original month is a weighted combination of all representatives Full weight matrix, not just one weight per representative <p>The choice of R does not change which months are selected \u2014 only how they are weighted in the downstream model.</p>"},{"location":"examples/ex4_representation_models/#find-the-best-3-month-selection","title":"Find the best 3-month selection\u00b6","text":"<p>We use PCA features and a weighted-sum policy with robust min-max normalization (so different score components are on comparable scales).</p>"},{"location":"examples/ex4_representation_models/#apply-three-representation-models-to-the-same-selection","title":"Apply three representation models to the same selection\u00b6","text":""},{"location":"examples/ex4_representation_models/#weight-comparison-table","title":"Weight comparison table\u00b6","text":""},{"location":"examples/ex4_representation_models/#responsibility-bars-side-by-side","title":"Responsibility bars: side by side\u00b6","text":"<p>The uniform model produces equal bars. KMedoids assigns more weight to representatives that are \"closest\" to more months. The blended model distributes responsibility more smoothly.</p>"},{"location":"examples/ex4_representation_models/#blended-weight-matrix","title":"Blended weight matrix\u00b6","text":"<p>The heatmap shows the full weight matrix: how much each original month (columns) relies on each representative (rows). In the blended model, every month is a weighted mix of all three representatives \u2014 not assigned to just one.</p>"},{"location":"examples/ex4_representation_models/#feature-space-and-distribution-fidelity","title":"Feature space and distribution fidelity\u00b6","text":""},{"location":"examples/ex5_multi_objective/","title":"Example 5: Multi-Objective Exploration","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[2]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n\nslicer = rep.TimeSlicer(unit=\"month\")\ncontext = rep.ProblemContext(df_raw=df_raw, slicer=slicer)\n\nfeature_pipeline = rep.FeaturePipeline(engineers={\n    'stats': rep.StandardStatsFeatureEngineer(),\n    'pca': rep.PCAFeatureEngineer(),\n})\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1)  slicer = rep.TimeSlicer(unit=\"month\") context = rep.ProblemContext(df_raw=df_raw, slicer=slicer)  feature_pipeline = rep.FeaturePipeline(engineers={     'stats': rep.StandardStatsFeatureEngineer(),     'pca': rep.PCAFeatureEngineer(), }) In\u00a0[3]: Copied! <pre>objective_set = rep.ObjectiveSet({\n    'wasserstein': (1.0, rep.WassersteinFidelity()),\n    'correlation': (1.0, rep.CorrelationFidelity()),\n    'duration_curve': (1.0, rep.DurationCurveFidelity()),\n    'diversity': (0.5, rep.DiversityReward()),\n})\n\nk = 3\ncombi_gen = rep.ExhaustiveCombiGen(k=k)\nrepresentation_model = rep.UniformRepresentationModel()\n</pre> objective_set = rep.ObjectiveSet({     'wasserstein': (1.0, rep.WassersteinFidelity()),     'correlation': (1.0, rep.CorrelationFidelity()),     'duration_curve': (1.0, rep.DurationCurveFidelity()),     'diversity': (0.5, rep.DiversityReward()), })  k = 3 combi_gen = rep.ExhaustiveCombiGen(k=k) representation_model = rep.UniformRepresentationModel() In\u00a0[4]: Copied! <pre>search_pareto = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    objective_set, rep.ParetoMaxMinStrategy(), combi_gen\n)\nworkflow_pareto = rep.Workflow(feature_pipeline, search_pareto, representation_model)\nexperiment_pareto = rep.RepSetExperiment(context, workflow_pareto)\nresult_pareto = experiment_pareto.run()\n\nprint(f\"Selection: {result_pareto.selection}\")\nprint(f\"Scores:    {result_pareto.scores}\")\n</pre> search_pareto = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(     objective_set, rep.ParetoMaxMinStrategy(), combi_gen ) workflow_pareto = rep.Workflow(feature_pipeline, search_pareto, representation_model) experiment_pareto = rep.RepSetExperiment(context, workflow_pareto) result_pareto = experiment_pareto.run()  print(f\"Selection: {result_pareto.selection}\") print(f\"Scores:    {result_pareto.scores}\") <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:01&lt;00:00, 200.24it/s]</pre> <pre>Selection: (Period('2015-07', 'M'), Period('2015-10', 'M'), Period('2015-11', 'M'))\nScores:    {'wasserstein': 0.17951351867468845, 'correlation': 0.04652147452533339, 'nrmse_duration_curve': 0.24354065585430432, 'diversity': 13.23658581042483}\n</pre> <pre>\n</pre> In\u00a0[5]: Copied! <pre>search_weighted = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(\n    objective_set, rep.WeightedSumPolicy(normalization='robust_minmax'), combi_gen\n)\nworkflow_weighted = rep.Workflow(feature_pipeline, search_weighted, representation_model)\nexperiment_weighted = rep.RepSetExperiment(experiment_pareto.feature_context, workflow_weighted)\nresult_weighted = experiment_weighted.run()\n\nprint(f\"Selection: {result_weighted.selection}\")\nprint(f\"Scores:    {result_weighted.scores}\")\n</pre> search_weighted = rep.ObjectiveDrivenCombinatorialSearchAlgorithm(     objective_set, rep.WeightedSumPolicy(normalization='robust_minmax'), combi_gen ) workflow_weighted = rep.Workflow(feature_pipeline, search_weighted, representation_model) experiment_weighted = rep.RepSetExperiment(experiment_pareto.feature_context, workflow_weighted) result_weighted = experiment_weighted.run()  print(f\"Selection: {result_weighted.selection}\") print(f\"Scores:    {result_weighted.scores}\") <pre>Iterating over combinations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 220/220 [00:01&lt;00:00, 188.30it/s]</pre> <pre>Selection: (Period('2015-07', 'M'), Period('2015-10', 'M'), Period('2015-11', 'M'))\nScores:    {'wasserstein': 0.17951351867468845, 'correlation': 0.04652147452533339, 'nrmse_duration_curve': 0.24354065585430432, 'diversity': 13.23658581042483}\n</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre>same = result_pareto.selection == result_weighted.selection\nprint(f\"Same selection? {same}\")\n</pre> same = result_pareto.selection == result_weighted.selection print(f\"Same selection? {same}\") <pre>Same selection? True\n</pre> In\u00a0[7]: Copied! <pre>fig = diag.ParetoScatter2D(\n    objective_x='wasserstein', objective_y='correlation'\n).plot(search_algorithm=search_pareto, selected_combination=result_pareto.selection)\nfig.update_layout(title='Pareto Front: Wasserstein vs Correlation')\nfig.show()\n</pre> fig = diag.ParetoScatter2D(     objective_x='wasserstein', objective_y='correlation' ).plot(search_algorithm=search_pareto, selected_combination=result_pareto.selection) fig.update_layout(title='Pareto Front: Wasserstein vs Correlation') fig.show() <p>The scatter matrix shows all pairwise objective trade-offs at once.</p> In\u00a0[8]: Copied! <pre>fig = diag.ParetoScatterMatrix().plot(\n    search_algorithm=search_pareto, selected_combination=result_pareto.selection\n)\nfig.update_layout(title='Pareto Scatter Matrix')\nfig.show()\n</pre> fig = diag.ParetoScatterMatrix().plot(     search_algorithm=search_pareto, selected_combination=result_pareto.selection ) fig.update_layout(title='Pareto Scatter Matrix') fig.show() In\u00a0[9]: Copied! <pre>for label, res in [('Pareto', result_pareto), ('Weighted Sum', result_weighted)]:\n    fig = diag.ScoreContributionBars().plot(res.scores, normalize=True)\n    fig.update_layout(title=f'Score Contributions: {label}')\n    fig.show()\n</pre> for label, res in [('Pareto', result_pareto), ('Weighted Sum', result_weighted)]:     fig = diag.ScoreContributionBars().plot(res.scores, normalize=True)     fig.update_layout(title=f'Score Contributions: {label}')     fig.show() In\u00a0[10]: Copied! <pre>for label, res in [('Pareto', result_pareto), ('Weighted Sum', result_weighted)]:\n    fig = diag.ResponsibilityBars().plot(res.weights, show_uniform_reference=True)\n    fig.update_layout(title=f'Weights: {label}')\n    fig.show()\n</pre> for label, res in [('Pareto', result_pareto), ('Weighted Sum', result_weighted)]:     fig = diag.ResponsibilityBars().plot(res.weights, show_uniform_reference=True)     fig.update_layout(title=f'Weights: {label}')     fig.show() In\u00a0[11]: Copied! <pre>selected_indices = slicer.get_indices_for_slice_combi(df_raw.index, result_pareto.selection)\ndf_selection = df_raw.loc[selected_indices]\n\nfor var in df_raw.columns:\n    fig = diag.DistributionOverlayHistogram().plot(df_raw[var], df_selection[var], nbins=40)\n    fig.update_layout(title=f'Distribution Overlay: {var}')\n    fig.show()\n</pre> selected_indices = slicer.get_indices_for_slice_combi(df_raw.index, result_pareto.selection) df_selection = df_raw.loc[selected_indices]  for var in df_raw.columns:     fig = diag.DistributionOverlayHistogram().plot(df_raw[var], df_selection[var], nbins=40)     fig.update_layout(title=f'Distribution Overlay: {var}')     fig.show() In\u00a0[12]: Copied! <pre>fig = diag.DiurnalProfileOverlay().plot(\n    df_raw, df_selection, variables=list(df_raw.columns)\n)\nfig.update_layout(title='Diurnal Profiles: Full Year vs Selection')\nfig.show()\n</pre> fig = diag.DiurnalProfileOverlay().plot(     df_raw, df_selection, variables=list(df_raw.columns) ) fig.update_layout(title='Diurnal Profiles: Full Year vs Selection') fig.show() In\u00a0[13]: Copied! <pre>fig = diag.CorrelationDifferenceHeatmap().plot(\n    df_raw, df_selection, method='pearson', show_lower_only=True\n)\nfig.update_layout(title='Correlation Difference: Selection - Full Year')\nfig.show()\n</pre> fig = diag.CorrelationDifferenceHeatmap().plot(     df_raw, df_selection, method='pearson', show_lower_only=True ) fig.update_layout(title='Correlation Difference: Selection - Full Year') fig.show() In\u00a0[14]: Copied! <pre>feature_context = experiment_pareto.feature_context\nfig = diag.FeatureDistributions().plot(feature_context.df_features, nbins=20, cols=4)\nfig.update_layout(title='Feature Distributions')\nfig.show()\n</pre> feature_context = experiment_pareto.feature_context fig = diag.FeatureDistributions().plot(feature_context.df_features, nbins=20, cols=4) fig.update_layout(title='Feature Distributions') fig.show()"},{"location":"examples/ex5_multi_objective/#example-5-multi-objective-exploration","title":"Example 5: Multi-Objective Exploration\u00b6","text":"<p>When multiple aspects of representativeness matter simultaneously, a single weighted score can hide important trade-offs. This notebook sets up a 4-component objective and compares two selection policies:</p> <ul> <li>ParetoMaxMinStrategy: finds the Pareto-optimal solution that maximizes the worst-performing objective (conservative, balanced)</li> <li>WeightedSumPolicy: collapses all objectives into a single scalar via weighted sum (simpler, but requires choosing weights a priori)</li> </ul> <p>The same search is run twice \u2014 the only difference is the policy that picks the winner from the scored candidates.</p>"},{"location":"examples/ex5_multi_objective/#rich-objective-set-4-components","title":"Rich objective set: 4 components\u00b6","text":"<p>Each component captures a different dimension of representativeness:</p> Component What it measures Direction Wasserstein Marginal distribution similarity minimize Correlation Cross-variable dependency preservation minimize Duration curve Duration curve NRMSE (load-ordered fidelity) minimize Diversity Spread of selection in feature space maximize <p>The first three are fidelity metrics (lower = better match to the full year). Diversity is a coverage metric (higher = more spread). This tension is intentional: pure fidelity optimization tends to pick \"average\" months, while diversity pushes toward distinct ones.</p>"},{"location":"examples/ex5_multi_objective/#run-a-paretomaxminstrategy","title":"Run A: ParetoMaxMinStrategy\u00b6","text":""},{"location":"examples/ex5_multi_objective/#run-b-weightedsumpolicy","title":"Run B: WeightedSumPolicy\u00b6","text":"<p>We reuse the already-computed features to skip redundant work.</p>"},{"location":"examples/ex5_multi_objective/#pareto-front-visualization","title":"Pareto front visualization\u00b6","text":"<p>The 2D scatter shows all 220 candidates in two-objective space. Pareto-optimal solutions (highlighted) form the efficient frontier \u2014 no solution dominates them on both axes.</p>"},{"location":"examples/ex5_multi_objective/#score-contributions-pareto-vs-weighted-sum","title":"Score contributions: Pareto vs Weighted Sum\u00b6","text":"<p>Comparing the normalized score profiles of the two winners reveals where they differ. The Pareto policy tends to produce more balanced profiles, while the weighted sum may sacrifice one objective for gains on others.</p>"},{"location":"examples/ex5_multi_objective/#weights-comparison","title":"Weights comparison\u00b6","text":""},{"location":"examples/ex5_multi_objective/#distribution-and-profile-diagnostics-pareto-selection","title":"Distribution and profile diagnostics (Pareto selection)\u00b6","text":""},{"location":"examples/ex6_clustering/","title":"Example 6: K-Medoids Clustering","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[2]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1) In\u00a0[3]: Copied! <pre>context = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))\n\nworkflow = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.KMedoidsSearch(k=4, random_state=42),\n)\nexperiment = rep.RepSetExperiment(context, workflow)\nresult = experiment.run()\n\nprint(f\"Selection: {result.selection}\")\nprint(f\"WCSS:      {result.scores['wcss']:.4f}\")\nprint(f\"Weights:   { {str(k): round(v, 3) for k, v in result.weights.items()} }\")\n</pre> context = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))  workflow = rep.Workflow(     feature_engineer=rep.StandardStatsFeatureEngineer(),     search_algorithm=rep.KMedoidsSearch(k=4, random_state=42), ) experiment = rep.RepSetExperiment(context, workflow) result = experiment.run()  print(f\"Selection: {result.selection}\") print(f\"WCSS:      {result.scores['wcss']:.4f}\") print(f\"Weights:   { {str(k): round(v, 3) for k, v in result.weights.items()} }\") <pre>Selection: (Period('2015-07', 'M'), Period('2015-01', 'M'), Period('2015-09', 'M'), Period('2015-06', 'M'))\nWCSS:      115.3572\nWeights:   {'2015-07': 0.167, '2015-01': 0.25, '2015-09': 0.333, '2015-06': 0.25}\n</pre> <pre>/Users/helgeesch/Documents/repositories/energy-repset/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:95: FutureWarning:\n\nFunction stable_cumsum is deprecated; `sklearn.utils.extmath.stable_cumsum` is deprecated in version 1.8 and will be removed in 1.10. Use `np.cumulative_sum` with the desired dtype directly instead.\n\n</pre> In\u00a0[4]: Copied! <pre>if 'cluster_info' in result.diagnostics:\n    print(\"Cluster membership:\")\n    for info in result.diagnostics['cluster_info']:\n        print(f\"  Cluster {info['cluster']}: medoid={info['medoid']}, \"\n              f\"size={info['size']}, members={info['members']}\")\n</pre> if 'cluster_info' in result.diagnostics:     print(\"Cluster membership:\")     for info in result.diagnostics['cluster_info']:         print(f\"  Cluster {info['cluster']}: medoid={info['medoid']}, \"               f\"size={info['size']}, members={info['members']}\") <pre>Cluster membership:\n  Cluster 0: medoid=2015-07, size=2, members=[Period('2015-05', 'M'), Period('2015-07', 'M')]\n  Cluster 1: medoid=2015-01, size=3, members=[Period('2015-01', 'M'), Period('2015-11', 'M'), Period('2015-12', 'M')]\n  Cluster 2: medoid=2015-09, size=4, members=[Period('2015-02', 'M'), Period('2015-03', 'M'), Period('2015-09', 'M'), Period('2015-10', 'M')]\n  Cluster 3: medoid=2015-06, size=3, members=[Period('2015-04', 'M'), Period('2015-06', 'M'), Period('2015-08', 'M')]\n</pre> In\u00a0[5]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True)\nfig.update_layout(title='K-Medoids: Responsibility Weights (Cluster Fractions)')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result.weights, show_uniform_reference=True) fig.update_layout(title='K-Medoids: Responsibility Weights (Cluster Fractions)') fig.show() In\u00a0[6]: Copied! <pre>feature_ctx = experiment.feature_context\ncols = list(feature_ctx.df_features.columns[:2])\n\nfig = diag.FeatureSpaceScatter2D().plot(\n    feature_ctx.df_features, x=cols[0], y=cols[1], selection=result.selection\n)\nfig.update_layout(title='K-Medoids: Feature Space (First Two Features)')\nfig.show()\n</pre> feature_ctx = experiment.feature_context cols = list(feature_ctx.df_features.columns[:2])  fig = diag.FeatureSpaceScatter2D().plot(     feature_ctx.df_features, x=cols[0], y=cols[1], selection=result.selection ) fig.update_layout(title='K-Medoids: Feature Space (First Two Features)') fig.show() In\u00a0[7]: Copied! <pre>slicer = rep.TimeSlicer(unit=\"month\")\nselected_idx = slicer.get_indices_for_slice_combi(df_raw.index, result.selection)\ndf_sel = df_raw.loc[selected_idx]\n\nfig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel['load'])\nfig.update_layout(title='K-Medoids: ECDF Overlay (Load)')\nfig.show()\n</pre> slicer = rep.TimeSlicer(unit=\"month\") selected_idx = slicer.get_indices_for_slice_combi(df_raw.index, result.selection) df_sel = df_raw.loc[selected_idx]  fig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel['load']) fig.update_layout(title='K-Medoids: ECDF Overlay (Load)') fig.show() In\u00a0[9]: Copied! <pre>results_by_k = {}\nfor k in [3, 4, 6]:\n    wf = rep.Workflow(\n        feature_engineer=rep.StandardStatsFeatureEngineer(),\n        search_algorithm=rep.KMedoidsSearch(k=k, random_state=42),\n    )\n    res = rep.RepSetExperiment(context, wf).run()\n    results_by_k[k] = res\n\nprint(f\"{'k':&gt;3}  {'WCSS':&gt;10}  {'Selection'}\")\nprint(\"-\" * 50)\nfor k, res in results_by_k.items():\n    print(f\"{k:&gt;3}  {res.scores['wcss']:&gt;10.4f}  {res.selection}\")\n</pre> results_by_k = {} for k in [3, 4, 6]:     wf = rep.Workflow(         feature_engineer=rep.StandardStatsFeatureEngineer(),         search_algorithm=rep.KMedoidsSearch(k=k, random_state=42),     )     res = rep.RepSetExperiment(context, wf).run()     results_by_k[k] = res  print(f\"{'k':&gt;3}  {'WCSS':&gt;10}  {'Selection'}\") print(\"-\" * 50) for k, res in results_by_k.items():     print(f\"{k:&gt;3}  {res.scores['wcss']:&gt;10.4f}  {res.selection}\") <pre>  k        WCSS  Selection\n--------------------------------------------------\n  3    144.0684  (Period('2015-06', 'M'), Period('2015-01', 'M'), Period('2015-09', 'M'))\n  4    115.3572  (Period('2015-07', 'M'), Period('2015-01', 'M'), Period('2015-09', 'M'), Period('2015-06', 'M'))\n  6     62.8189  (Period('2015-07', 'M'), Period('2015-11', 'M'), Period('2015-03', 'M'), Period('2015-06', 'M'), Period('2015-10', 'M'), Period('2015-12', 'M'))\n</pre> <pre>/Users/helgeesch/Documents/repositories/energy-repset/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:95: FutureWarning:\n\nFunction stable_cumsum is deprecated; `sklearn.utils.extmath.stable_cumsum` is deprecated in version 1.8 and will be removed in 1.10. Use `np.cumulative_sum` with the desired dtype directly instead.\n\n/Users/helgeesch/Documents/repositories/energy-repset/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:95: FutureWarning:\n\nFunction stable_cumsum is deprecated; `sklearn.utils.extmath.stable_cumsum` is deprecated in version 1.8 and will be removed in 1.10. Use `np.cumulative_sum` with the desired dtype directly instead.\n\n/Users/helgeesch/Documents/repositories/energy-repset/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:95: FutureWarning:\n\nFunction stable_cumsum is deprecated; `sklearn.utils.extmath.stable_cumsum` is deprecated in version 1.8 and will be removed in 1.10. Use `np.cumulative_sum` with the desired dtype directly instead.\n\n</pre>"},{"location":"examples/ex6_clustering/#example-6-k-medoids-clustering","title":"Example 6: K-Medoids Clustering\u00b6","text":"<p>K-medoids clustering is a constructive (Workflow Type 2) algorithm that partitions the feature space into $k$ clusters and selects the medoid of each cluster as a representative period. Unlike k-means, which produces synthetic centroids, k-medoids always selects actual data points --- making it a natural fit for representative period selection.</p> <p>Key properties:</p> <ul> <li>Internal objective: minimizes within-cluster sum of squares (WCSS)</li> <li>Weights: pre-computed as cluster-size fractions ($w_j = n_j / N$)</li> <li>No external ObjectiveSet needed: the algorithm has its own built-in objective</li> <li>Fast: converges in a few iterations for typical problem sizes</li> </ul>"},{"location":"examples/ex6_clustering/#monthly-k-medoids","title":"Monthly K-Medoids\u00b6","text":"<p>We select 4 representative months from 12 using k-medoids clustering on statistical features. The algorithm partitions the 12 months into 4 clusters and picks the medoid of each.</p>"},{"location":"examples/ex6_clustering/#effect-of-k","title":"Effect of k\u00b6","text":"<p>More clusters mean lower WCSS (tighter clusters), but fewer representatives per cluster means less compression. Let's compare k=3 and k=6.</p>"},{"location":"examples/ex6_clustering/#summary","title":"Summary\u00b6","text":"<p>K-medoids clustering is a good default when you want a standard, fast, well-understood clustering-based selection without additional constraints. It works well for monthly or weekly slicing and produces cluster-size-proportional weights automatically.</p> <p>For contiguous temporal segments, use CTPC instead. For multi-day subsequences, use the Snippet algorithm.</p>"},{"location":"examples/ex7_constructive_algorithms/","title":"Example 7: Constructive Algorithms","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport energy_repset as rep\nimport energy_repset.diagnostics as diag\nimport plotly.io as pio; pio.renderers.default = 'notebook_connected'\n</pre> import pandas as pd import energy_repset as rep import energy_repset.diagnostics as diag import plotly.io as pio; pio.renderers.default = 'notebook_connected' In\u00a0[2]: Copied! <pre>url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\"\ndf_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1)\ndf_raw = df_raw.drop('prices', axis=1)\n</pre> url = \"https://tubcloud.tu-berlin.de/s/pKttFadrbTKSJKF/download/time-series-lecture-2.csv\" df_raw = pd.read_csv(url, index_col=0, parse_dates=True).rename_axis('variable', axis=1) df_raw = df_raw.drop('prices', axis=1) In\u00a0[3]: Copied! <pre>context_monthly = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))\n\nworkflow_hull = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.HullClusteringSearch(k=3, hull_type='convex'),\n    representation_model=rep.BlendedRepresentationModel(blend_type='convex'),\n)\nexperiment_hull = rep.RepSetExperiment(context_monthly, workflow_hull)\nresult_hull = experiment_hull.run()\n\nprint(f\"Selection:        {result_hull.selection}\")\nprint(f\"Projection error: {result_hull.scores['projection_error']:.4f}\")\n</pre> context_monthly = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"month\"))  workflow_hull = rep.Workflow(     feature_engineer=rep.StandardStatsFeatureEngineer(),     search_algorithm=rep.HullClusteringSearch(k=3, hull_type='convex'),     representation_model=rep.BlendedRepresentationModel(blend_type='convex'), ) experiment_hull = rep.RepSetExperiment(context_monthly, workflow_hull) result_hull = experiment_hull.run()  print(f\"Selection:        {result_hull.selection}\") print(f\"Projection error: {result_hull.scores['projection_error']:.4f}\") <pre>Selection:        (Period('2015-12', 'M'), Period('2015-08', 'M'), Period('2015-11', 'M'))\nProjection error: 170.9117\n</pre> In\u00a0[4]: Copied! <pre># Aggregate blended weights for bar chart\nif isinstance(result_hull.weights, pd.DataFrame):\n    agg = result_hull.weights.sum(axis=0)\n    weights_hull_agg = (agg / agg.sum()).to_dict()\nelse:\n    weights_hull_agg = result_hull.weights\n\nfig = diag.ResponsibilityBars().plot(weights_hull_agg, show_uniform_reference=True)\nfig.update_layout(title='Hull Clustering: Responsibility Weights (Blended, Aggregated)')\nfig.show()\n</pre> # Aggregate blended weights for bar chart if isinstance(result_hull.weights, pd.DataFrame):     agg = result_hull.weights.sum(axis=0)     weights_hull_agg = (agg / agg.sum()).to_dict() else:     weights_hull_agg = result_hull.weights  fig = diag.ResponsibilityBars().plot(weights_hull_agg, show_uniform_reference=True) fig.update_layout(title='Hull Clustering: Responsibility Weights (Blended, Aggregated)') fig.show() In\u00a0[5]: Copied! <pre>feature_ctx_hull = experiment_hull.feature_context\ncols = list(feature_ctx_hull.df_features.columns[:2])\n\nfig = diag.FeatureSpaceScatter2D().plot(\n    feature_ctx_hull.df_features, x=cols[0], y=cols[1], selection=result_hull.selection\n)\nfig.update_layout(title='Hull Clustering: Feature Space')\nfig.show()\n</pre> feature_ctx_hull = experiment_hull.feature_context cols = list(feature_ctx_hull.df_features.columns[:2])  fig = diag.FeatureSpaceScatter2D().plot(     feature_ctx_hull.df_features, x=cols[0], y=cols[1], selection=result_hull.selection ) fig.update_layout(title='Hull Clustering: Feature Space') fig.show() In\u00a0[6]: Copied! <pre>slicer_monthly = rep.TimeSlicer(unit=\"month\")\nselected_idx = slicer_monthly.get_indices_for_slice_combi(df_raw.index, result_hull.selection)\ndf_sel = df_raw.loc[selected_idx]\n\nfig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel['load'])\nfig.update_layout(title='Hull Clustering: ECDF Overlay (Load)')\nfig.show()\n</pre> slicer_monthly = rep.TimeSlicer(unit=\"month\") selected_idx = slicer_monthly.get_indices_for_slice_combi(df_raw.index, result_hull.selection) df_sel = df_raw.loc[selected_idx]  fig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel['load']) fig.update_layout(title='Hull Clustering: ECDF Overlay (Load)') fig.show() In\u00a0[\u00a0]: Copied! <pre>workflow_ctpc = rep.Workflow(\n    feature_engineer=rep.StandardStatsFeatureEngineer(),\n    search_algorithm=rep.CTPCSearch(k=4, linkage='ward'),\n)\nexperiment_ctpc = rep.RepSetExperiment(context_monthly, workflow_ctpc)\nresult_ctpc = experiment_ctpc.run()\n\nprint(f\"Selection: {result_ctpc.selection}\")\nprint(f\"WCSS:      {result_ctpc.scores['wcss']:.4f}\")\nprint(f\"Weights:   { {str(k): round(v, 3) for k, v in result_ctpc.weights.items()} }\")\n</pre> workflow_ctpc = rep.Workflow(     feature_engineer=rep.StandardStatsFeatureEngineer(),     search_algorithm=rep.CTPCSearch(k=4, linkage='ward'), ) experiment_ctpc = rep.RepSetExperiment(context_monthly, workflow_ctpc) result_ctpc = experiment_ctpc.run()  print(f\"Selection: {result_ctpc.selection}\") print(f\"WCSS:      {result_ctpc.scores['wcss']:.4f}\") print(f\"Weights:   { {str(k): round(v, 3) for k, v in result_ctpc.weights.items()} }\") In\u00a0[8]: Copied! <pre>if 'segments' in result_ctpc.diagnostics:\n    print(\"Contiguous segments:\")\n    for seg in result_ctpc.diagnostics['segments']:\n        print(f\"  {seg['start']} -- {seg['end']}  \"\n              f\"(size={seg['size']}, representative={seg['representative']})\")\n</pre> if 'segments' in result_ctpc.diagnostics:     print(\"Contiguous segments:\")     for seg in result_ctpc.diagnostics['segments']:         print(f\"  {seg['start']} -- {seg['end']}  \"               f\"(size={seg['size']}, representative={seg['representative']})\") <pre>Contiguous segments:\n  2015-01 -- 2015-03  (size=3, representative=2015-03)\n  2015-04 -- 2015-08  (size=5, representative=2015-06)\n  2015-09 -- 2015-10  (size=2, representative=2015-09)\n  2015-11 -- 2015-12  (size=2, representative=2015-11)\n</pre> In\u00a0[9]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result_ctpc.weights, show_uniform_reference=True)\nfig.update_layout(title='CTPC: Responsibility Weights (Segment Fractions)')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result_ctpc.weights, show_uniform_reference=True) fig.update_layout(title='CTPC: Responsibility Weights (Segment Fractions)') fig.show() In\u00a0[10]: Copied! <pre>feature_ctx_ctpc = experiment_ctpc.feature_context\ncols = list(feature_ctx_ctpc.df_features.columns[:2])\n\nfig = diag.FeatureSpaceScatter2D().plot(\n    feature_ctx_ctpc.df_features, x=cols[0], y=cols[1], selection=result_ctpc.selection\n)\nfig.update_layout(title='CTPC: Feature Space')\nfig.show()\n</pre> feature_ctx_ctpc = experiment_ctpc.feature_context cols = list(feature_ctx_ctpc.df_features.columns[:2])  fig = diag.FeatureSpaceScatter2D().plot(     feature_ctx_ctpc.df_features, x=cols[0], y=cols[1], selection=result_ctpc.selection ) fig.update_layout(title='CTPC: Feature Space') fig.show() In\u00a0[11]: Copied! <pre>selected_idx_ctpc = slicer_monthly.get_indices_for_slice_combi(df_raw.index, result_ctpc.selection)\ndf_sel_ctpc = df_raw.loc[selected_idx_ctpc]\n\nfig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel_ctpc['load'])\nfig.update_layout(title='CTPC: ECDF Overlay (Load)')\nfig.show()\n</pre> selected_idx_ctpc = slicer_monthly.get_indices_for_slice_combi(df_raw.index, result_ctpc.selection) df_sel_ctpc = df_raw.loc[selected_idx_ctpc]  fig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel_ctpc['load']) fig.update_layout(title='CTPC: ECDF Overlay (Load)') fig.show() In\u00a0[\u00a0]: Copied! <pre>context_daily = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"day\"))\n\nworkflow_snippet = rep.Workflow(\n    feature_engineer=rep.DirectProfileFeatureEngineer(),\n    search_algorithm=rep.SnippetSearch(k=4, period_length_days=7, step_days=7),\n)\nexperiment_snippet = rep.RepSetExperiment(context_daily, workflow_snippet)\nresult_snippet = experiment_snippet.run()\n\nprint(f\"Selection (start days): {result_snippet.selection}\")\nprint(f\"Total distance:         {result_snippet.scores['total_distance']:.4f}\")\nprint(f\"Weights:                { {str(k): round(v, 3) for k, v in result_snippet.weights.items()} }\")\n</pre> context_daily = rep.ProblemContext(df_raw=df_raw, slicer=rep.TimeSlicer(unit=\"day\"))  workflow_snippet = rep.Workflow(     feature_engineer=rep.DirectProfileFeatureEngineer(),     search_algorithm=rep.SnippetSearch(k=4, period_length_days=7, step_days=7), ) experiment_snippet = rep.RepSetExperiment(context_daily, workflow_snippet) result_snippet = experiment_snippet.run()  print(f\"Selection (start days): {result_snippet.selection}\") print(f\"Total distance:         {result_snippet.scores['total_distance']:.4f}\") print(f\"Weights:                { {str(k): round(v, 3) for k, v in result_snippet.weights.items()} }\") In\u00a0[13]: Copied! <pre>fig = diag.ResponsibilityBars().plot(result_snippet.weights, show_uniform_reference=True)\nfig.update_layout(title='Snippet: Responsibility Weights (Assignment Fractions)')\nfig.show()\n</pre> fig = diag.ResponsibilityBars().plot(result_snippet.weights, show_uniform_reference=True) fig.update_layout(title='Snippet: Responsibility Weights (Assignment Fractions)') fig.show() In\u00a0[14]: Copied! <pre>slicer_daily = rep.TimeSlicer(unit=\"day\")\nselected_idx_snippet = slicer_daily.get_indices_for_slice_combi(\n    df_raw.index, result_snippet.selection\n)\ndf_sel_snippet = df_raw.loc[selected_idx_snippet]\n\nfig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel_snippet['load'])\nfig.update_layout(title='Snippet: ECDF Overlay (Load)')\nfig.show()\n</pre> slicer_daily = rep.TimeSlicer(unit=\"day\") selected_idx_snippet = slicer_daily.get_indices_for_slice_combi(     df_raw.index, result_snippet.selection ) df_sel_snippet = df_raw.loc[selected_idx_snippet]  fig = diag.DistributionOverlayECDF().plot(df_raw['load'], df_sel_snippet['load']) fig.update_layout(title='Snippet: ECDF Overlay (Load)') fig.show() In\u00a0[15]: Copied! <pre>print(f\"{'Algorithm':&lt;25} {'k':&gt;3} {'Internal Score':&gt;20} {'Value':&gt;10}\")\nprint(\"-\" * 60)\nprint(f\"{'Hull Clustering':&lt;25} {'3':&gt;3} {'projection_error':&gt;20} \"\n      f\"{result_hull.scores['projection_error']:&gt;10.4f}\")\nprint(f\"{'CTPC':&lt;25} {'4':&gt;3} {'wcss':&gt;20} \"\n      f\"{result_ctpc.scores['wcss']:&gt;10.4f}\")\nprint(f\"{'Snippet':&lt;25} {'4':&gt;3} {'total_distance':&gt;20} \"\n      f\"{result_snippet.scores['total_distance']:&gt;10.4f}\")\n</pre> print(f\"{'Algorithm':&lt;25} {'k':&gt;3} {'Internal Score':&gt;20} {'Value':&gt;10}\") print(\"-\" * 60) print(f\"{'Hull Clustering':&lt;25} {'3':&gt;3} {'projection_error':&gt;20} \"       f\"{result_hull.scores['projection_error']:&gt;10.4f}\") print(f\"{'CTPC':&lt;25} {'4':&gt;3} {'wcss':&gt;20} \"       f\"{result_ctpc.scores['wcss']:&gt;10.4f}\") print(f\"{'Snippet':&lt;25} {'4':&gt;3} {'total_distance':&gt;20} \"       f\"{result_snippet.scores['total_distance']:&gt;10.4f}\") <pre>Algorithm                   k       Internal Score      Value\n------------------------------------------------------------\nHull Clustering             3     projection_error   170.9117\nCTPC                        4                 wcss   121.7921\nSnippet                     4       total_distance 21833.6298\n</pre>"},{"location":"examples/ex7_constructive_algorithms/#example-7-constructive-algorithms","title":"Example 7: Constructive Algorithms\u00b6","text":"<p>Examples 1--5 all use the Generate-and-Test workflow: enumerate candidates, score each one, pick the best. Constructive algorithms work differently \u2014 they build solutions iteratively using their own internal objectives.</p> <p>This notebook demonstrates three constructive algorithms:</p> Algorithm Idea Internal objective Output Hull Clustering Select periods that span the convex hull of the feature space Minimize projection error Subset + blended weights CTPC Hierarchical clustering that only merges temporally adjacent periods Minimize within-cluster sum of squares Contiguous segments + segment-size weights Snippet Greedy selection of multi-day subsequences via p-median Minimize total distance to nearest snippet Subsequence starts + assignment-fraction weights <p>Because these algorithms have their own built-in objectives, they do not use the <code>ObjectiveSet</code> during search \u2014 only for optional post-hoc evaluation.</p>"},{"location":"examples/ex7_constructive_algorithms/#1-hull-clustering","title":"1. Hull Clustering\u00b6","text":"<p>Hull Clustering selects periods that form the vertices of a convex hull enclosing the data in feature space. The intuition: if your representatives span the \"boundary\" of the data cloud, every other period can be expressed as a convex combination of them \u2014 which is exactly what the blended representation model does.</p> <p>The algorithm greedily adds the period that most reduces the total projection error (i.e., the error from approximating all periods as convex combinations of the selected ones).</p>"},{"location":"examples/ex7_constructive_algorithms/#2-ctpc-chronological-time-period-clustering","title":"2. CTPC (Chronological Time-Period Clustering)\u00b6","text":"<p>CTPC is hierarchical agglomerative clustering with a contiguity constraint: only temporally adjacent periods can be merged. This guarantees the output is a set of contiguous segments \u2014 e.g., \"Jan-Mar\", \"Apr-Jun\", etc. \u2014 which is useful for models with long-duration storage or seasonal dynamics.</p> <p>Weights reflect the size of each segment relative to the total.</p>"},{"location":"examples/ex7_constructive_algorithms/#3-snippet-algorithm","title":"3. Snippet Algorithm\u00b6","text":"<p>The Snippet algorithm is designed for multi-day representative periods (e.g., weeks). Instead of comparing entire weeks as monolithic objects, it compares day-level \"snippets\" within candidate periods. This avoids the problem where a single anomalous day makes an entire week look unique.</p> <p>It uses a greedy p-median approach: iteratively select the snippet whose addition most reduces the total distance from all days to their nearest selected snippet.</p>"},{"location":"examples/ex7_constructive_algorithms/#summary","title":"Summary\u00b6","text":"<p>All three constructive algorithms find solutions without evaluating an external objective set. Their internal objectives are tightly coupled to the algorithm mechanics, which makes them fast but less modular than Generate-and-Test.</p>"}]}